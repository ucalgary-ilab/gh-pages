<!DOCTYPE html><html><head><link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.css" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/css/lightbox.css" rel="stylesheet"/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link href="/static/css/style.css" rel="stylesheet"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox-plus-jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              $('.ui.sidebar')
                .sidebar('attach events', '.sidebar.icon')

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><meta charSet="utf-8" class="next-head"/><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><link rel="preload" href="/_next/static/CRJibYsfvdPuztTvq0Efo/pages/person.js" as="script"/><link rel="preload" href="/_next/static/CRJibYsfvdPuztTvq0Efo/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.98f1a4e4c94db9943918.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-22d58f57abab872d6e70.js" as="script"/></head><body><div id="__next"><div><title>Anthony Tang - Interactions Lab | University of Calgary HCI Group</title><div><div class="ui right vertical sidebar menu"><a class="item" href="/">Home</a><a class="item" href="/publications">Publications</a><a class="item active" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/news">News</a><a class="item" href="/location">Location</a></div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/"><b>UCalgary iLab</b></a></div><div class="right menu"><a class="item" href="/publications">Publications</a><a class="item active" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/news">News</a><a class="item" href="/location">Location</a><div class="toc item"><a href="/"><b>UCalgary iLab</b></a><i style="float:right" class="sidebar icon"></i></div></div></div></div><div class="ui stackable grid"><div class="one wide column"></div><div class="eleven wide column centered"><div id="person" class="category" style="text-align:center"><img class="ui circular image large-profile" src="/static/images/people/anthony-tang.jpg" style="margin:auto"/><h1>Anthony Tang</h1><p>Adjunct Associate Professor</p><p><a href="https://hcitang.github.io/" target="_blank"><i class="fas fa-link fa-fw"></i>https://hcitang.github.io/</a></p><p><a href="https://scholar.google.com/citations?user=RG1EQowAAAAJ" target="_blank"><i class="fas fa-graduation-cap fa-fw"></i>Google Scholar</a></p><div class="ui horizontal small divided link list"><div class="item"><a href="https://twitter.com/proclubboy" target="_blank" style="font-size:1.2em"><i class="fab fa-twitter fa-fw"></i>proclubboy</a></div><div class="item"><a href="http://github.com/hcitang" target="_blank" style="font-size:1.2em"><i class="fab fa-github-alt fa-fw"></i>hcitang</a></div></div></div><div id="publications" class="category"><h1 class="ui horizontal divider header"><i class="file alternate outline icon"></i>Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="uist-2020-yixian"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-yixian.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2020</span></p><p class="color" style="font-size:1.3em"><b>ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World</b></p><p><span>Yan Yixian</span> , <span>Kazuki Takashima</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>Takayuki Tanno</span> , <span>Kazuyuki Fujita</span> , <span>Yoshifumi Kitamura</span></p><p><div class="ui labels"><span class="ui label">encountered-type haptic devices</span><span class="ui label">immersive experience</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-anjani"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b></p><p><span>Laurensia Anjani</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Terrance Mok</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Wooi Boon Goh</span></p><p><div class="ui labels"><span class="ui label">video streams</span><span class="ui label">mukbang</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-seyed"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-seyed.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span><span class="ui big label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways</b></p><p><a href="/people/teddy-seyed"><img src="/static/images/people/teddy-seyed.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Teddy Seyed</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a></p><p><div class="ui labels"><span class="ui label">fashion</span><span class="ui label">haute couture</span><span class="ui label">e-textiles</span><span class="ui label">maker culture</span><span class="ui label">fashion-tech</span><span class="ui label">wearables</span><span class="ui label">avant-garde</span><span class="ui label">haute-tech couture</span><span class="ui label">modular</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2019-tolley"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-tolley.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2019</span></p><p class="color" style="font-size:1.3em"><b>WindyWall: Exploring Creative Wind Simulations</b></p><p><span>David Tolley</span> , <span>Thi Ngoc Tram Nguyen</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>Nimesha Ranasinghe</span> , <span>Kensaku Kawauchi</span> , <span>Ching-Chiuan Yen</span></p><p><div class="ui labels"><span class="ui label">tactile/haptic interaction</span><span class="ui label">multimodal interaction</span><span class="ui label">novel actuators/displays</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2018-pham"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2018-pham.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2018</span></p><p class="color" style="font-size:1.3em"><b>Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design</b></p><p><span>Tran Pham</span> , <span>Jo Vermeulen</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>Lindsay MacDonald Vermeulen</span></p><p><div class="ui labels"><span class="ui label">augmented reality</span><span class="ui label">gestures</span><span class="ui label">gesture elicitation</span><span class="ui label">hololens</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-dillman"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-dillman.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</b></p><p><span>Kody R. Dillman</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Terrance Mok</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Alex Mitchell</span></p><p><div class="ui labels"><span class="ui label">game design</span><span class="ui label">guidance</span><span class="ui label">interaction cues</span><span class="ui label">augmented reality</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-feick"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span><span class="ui big label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</b></p><p><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Martin Feick</span></a> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p><div class="ui labels"><span class="ui label">cscw</span><span class="ui label">remote collaboration</span><span class="ui label">object-focused collaboration</span><span class="ui label">physical telepresence</span><span class="ui label">collaborative physical tasks</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-wuertz"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-wuertz.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>A Design Framework for Awareness Cues in Distributed Multiplayer Games</b></p><p><span>Jason Wuertz</span> , <span>Sultan A. Alharthi</span> , <span>William A. Hamilton</span> , <span>Scott Bateman</span> , <span>Carl Gutwin</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>Zachary O. Toups</span> , <span>Jessica Hammer</span></p><p><div class="ui labels"><span class="ui label">workspace awareness</span><span class="ui label">situation awareness</span><span class="ui label">game design</span><span class="ui label">distributed multiplayer games</span><span class="ui label">awareness cues</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-heshmat"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-heshmat.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing</b></p><p><span>Yasamin Heshmat</span> , <a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Brennan Jones</span></a> , <span>Xiaoxuan Xiong</span> , <span>Carman Neustaedter</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>Bernhard E. Riecke</span> , <span>Lillian Yang</span></p><p><div class="ui labels"><span class="ui label">video communication</span><span class="ui label">telepresence robots</span><span class="ui label">leisure activities</span><span class="ui label">social presence</span><span class="ui label">geocaching</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="hri-2018-feick"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">HRI 2018</span></p><p class="color" style="font-size:1.3em"><b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b></p><p><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Martin Feick</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>André Miede</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p><div class="ui labels"><span class="ui label">movement trajectory &amp; velocity</span><span class="ui label">remote collaboration</span><span class="ui label">robot surrogate</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2016-jones"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2016-jones.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2016</span></p><p class="color" style="font-size:1.3em"><b>Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</b></p><p><a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Brennan Jones</span></a> , <span>Kody Dillman</span> , <span>Richard Tang</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Carman Neustaedter</span> , <span>Scott Bateman</span></p><p><div class="ui labels"><span class="ui label">cscw</span><span class="ui label">telepresence</span><span class="ui label">video communication</span><span class="ui label">shared experiences</span><span class="ui label">teleoperation</span><span class="ui label">drones</span><span class="ui label">collaboration</span><span class="ui label">hri</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-aseniero"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-aseniero.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Stratos: Using Visualization to Support Decisions in Strategic Software Release Planning</b></p><p><a href="/people/bon-adriel-aseniero"><img src="/static/images/people/bon-adriel-aseniero.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Bon Adriel Aseniero</span></a> , <span>Tiffany Wun</span> , <a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <span>Guenther Ruhe</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a></p><p><div class="ui labels"><span class="ui label">software engineering</span><span class="ui label">information visualization</span><span class="ui label">release planning</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-jones"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-jones.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Mechanics of Camera Work in Mobile Video Collaboration</b></p><p><a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Brennan Jones</span></a> , <span>Anna Witcraft</span> , <span>Scott Bateman</span> , <span>Carman Neustaedter</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a></p><p><div class="ui labels"><span class="ui label">video communication</span><span class="ui label">collaboration</span><span class="ui label">mobile computing</span><span class="ui label">handheld devices</span><span class="ui label">cscw</span></div></p></div></div></div><div id="publications-modal"><div id="uist-2020-yixian" class="ui large modal"><div class="header"><a href="/publications/uist-2020-yixian" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2020-yixian</a>  -  <a href="/publications/uist-2020-yixian.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-yixian.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2020-yixian" target="_blank">ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World</a></h1><p class="meta"><span>Yan Yixian</span> , <span>Kazuki Takashima</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>Takayuki Tanno</span> , <span>Kazuyuki Fujita</span> , <span>Yoshifumi Kitamura</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/j2iSNDkBxAY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/j2iSNDkBxAY?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/j2iSNDkBxAY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls---autonomous robotic encounter-type haptic wall-shaped props---that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user&#x27;s movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of autonomous mobile reusable props, which we view as an important design direction for haptics in VR.</p><div class="ui labels">Keywords:  <span class="ui large label">encountered-type haptic devices</span><span class="ui large label">immersive experience</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Yan Yixian<!-- -->, <!-- -->Kazuki Takashima<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Takayuki Tanno<!-- -->, <!-- -->Kazuyuki Fujita<!-- -->, <!-- -->Yoshifumi Kitamura<!-- -->. <b>ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3379337.3415859" target="_blank">https://doi.org/10.1145/3379337.3415859</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-anjani" class="ui large modal"><div class="header"><a href="/publications/chi-2020-anjani" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-anjani</a>  -  <a href="/publications/chi-2020-anjani.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-anjani" target="_blank">Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</a></h1><p class="meta"><span>Laurensia Anjani</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><strong>Terrance Mok</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Wooi Boon Goh</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present a mixed-methods study of viewers on their practices and motivations around watching mukbang — video streams of people eating large quantities of food. Viewers&#x27; experiences provide insight on future technologies for multisensorial video streams and technology-supported commensality (eating with others). We surveyed 104 viewers and interviewed 15 of them about their attitudes and reflections on their mukbang viewing habits, their physiological aspects of watching someone eat, and their perceived social relationship with mukbangers. Based on our findings, we propose design implications for remote commensality, and for synchronized multisensorial video streaming content.</p><div class="ui labels">Keywords:  <span class="ui large label">video streams</span><span class="ui large label">mukbang</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Laurensia Anjani<!-- -->, <!-- -->Terrance Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Wooi Boon Goh<!-- -->. <b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3313831.3376567" target="_blank">https://doi.org/10.1145/3313831.3376567</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-seyed" class="ui large modal"><div class="header"><a href="/publications/dis-2019-seyed" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2019-seyed</a>  -  <a href="/publications/dis-2019-seyed.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-seyed.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-seyed" target="_blank">Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways</a></h1><p class="meta"><a href="/people/teddy-seyed"><img src="/static/images/people/teddy-seyed.jpg" class="ui circular spaced image mini-profile"/><strong>Teddy Seyed</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Drawing upon multiple disciplines, avant-garde fashion-tech teams push the boundaries between fashion and technology. Many are well trained in envisioning aesthetic qualities of garments, but few have formal training on designing and fabricating technologies themselves. We introduce Mannequette, a prototyping tool for fashion-tech garments that enables teams to experiment with interactive technologies at early stages of their design processes. Mannequette provides an abstraction of light-based outputs and sensor-based inputs for garments through a DJ mixer-like interface that allows for dynamic changes and recording/playback of visual effects. The base of Mannequette can also be incorporated into the final garment, where it is then connected to the final components. We conducted an 8-week deployment study with eight design teams who created new garments for a runway show. Our results revealed Mannequette allowed teams to repeatedly consider new design and technical options early in their creative processes, and to communicate more effectively across disciplinary backgrounds.</p><div class="ui labels">Keywords:  <span class="ui large label">fashion</span><span class="ui large label">haute couture</span><span class="ui large label">e-textiles</span><span class="ui large label">maker culture</span><span class="ui large label">fashion-tech</span><span class="ui large label">wearables</span><span class="ui large label">avant-garde</span><span class="ui large label">haute-tech couture</span><span class="ui large label">modular</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Teddy Seyed<!-- -->, <!-- -->Anthony Tang<!-- -->. <b>Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3322276.3322305" target="_blank">https://doi.org/10.1145/3322276.3322305</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2019-tolley" class="ui large modal"><div class="header"><a href="/publications/tei-2019-tolley" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2019-tolley</a>  -  <a href="/publications/tei-2019-tolley.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-tolley.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2019-tolley" target="_blank">WindyWall: Exploring Creative Wind Simulations</a></h1><p class="meta"><span>David Tolley</span> , <span>Thi Ngoc Tram Nguyen</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>Nimesha Ranasinghe</span> , <span>Kensaku Kawauchi</span> , <span>Ching-Chiuan Yen</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Wind simulations are typically one-off implementations for specific applications. We introduce WindyWall, a platform for creative design and exploration of wind simulations. WindyWall is a three-panel 90-fan array that encapsulates users with 270? of wind coverage. We describe the design and implementation of the array panels, discussing how the panels can be re-arranged, where various wind simulations can be realized as simple effects. To understand how people perceive &quot;wind&quot; generated from WindyWall, we conducted a pilot study of wind magnitude perception using different wind activation patterns from WindyWall. Our findings suggest that: horizontal wind activations are perceived more readily than vertical ones, and that people&#x27;s perceptions of wind are highly variable-most individuals will rate airflow differently in subsequent exposures. Based on our findings, we discuss the importance of developing a method for characterizing wind simulations, and provide design directions for others using fan arrays to simulate wind.</p><div class="ui labels">Keywords:  <span class="ui large label">tactile/haptic interaction</span><span class="ui large label">multimodal interaction</span><span class="ui large label">novel actuators/displays</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Tolley<!-- -->, <!-- -->Thi Ngoc Tram Nguyen<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Nimesha Ranasinghe<!-- -->, <!-- -->Kensaku Kawauchi<!-- -->, <!-- -->Ching-Chiuan Yen<!-- -->. <b>WindyWall: Exploring Creative Wind Simulations</b>. <i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/3294109.3295624" target="_blank">https://doi.org/10.1145/3294109.3295624</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2018-pham" class="ui large modal"><div class="header"><a href="/publications/dis-2018-pham" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2018-pham</a>  -  <a href="/publications/dis-2018-pham.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2018-pham.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2018-pham" target="_blank">Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design</a></h1><p class="meta"><span>Tran Pham</span> , <span>Jo Vermeulen</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>Lindsay MacDonald Vermeulen</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Because gesture design for augmented reality (AR) remains idiosyncratic, people cannot necessarily use gestures learned in one AR application in another. To design discoverable gestures, we need to understand what gestures people expect to use. We explore how the scale of AR affects the gestures people expect to use to interact with 3D holograms. Using an elicitation study, we asked participants to generate gestures in response to holographic task referents, where we varied the scale of holograms from desktop-scale to room-scale objects. We found that the scale of objects and scenes in the AR experience moderates the generated gestures. Most gestures were informed by physical interaction, and when people interacted from a distance, they sought a good perspective on the target object before and during the interaction. These results suggest that gesture designers need to account for scale, and should not simply reuse gestures across different hologram sizes.</p><div class="ui labels">Keywords:  <span class="ui large label">augmented reality</span><span class="ui large label">gestures</span><span class="ui large label">gesture elicitation</span><span class="ui large label">hololens</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Tran Pham<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lindsay MacDonald Vermeulen<!-- -->. <b>Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->14<!-- -->.  DOI: <a href="https://doi.org/10.1145/3196709.3196719" target="_blank">https://doi.org/10.1145/3196709.3196719</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-dillman" class="ui large modal"><div class="header"><a href="/publications/chi-2018-dillman" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-dillman</a>  -  <a href="/publications/chi-2018-dillman.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-dillman.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-dillman" target="_blank">A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</a></h1><p class="meta"><span>Kody R. Dillman</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><strong>Terrance Mok</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Alex Mitchell</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences.</p><div class="ui labels">Keywords:  <span class="ui large label">game design</span><span class="ui large label">guidance</span><span class="ui large label">interaction cues</span><span class="ui large label">augmented reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kody R. Dillman<!-- -->, <!-- -->Terrance Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Alex Mitchell<!-- -->. <b>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173714" target="_blank">https://doi.org/10.1145/3173574.3173714</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-feick" class="ui large modal"><div class="header"><a href="/publications/chi-2018-feick" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-feick</a>  -  <a href="/publications/chi-2018-feick.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-feick.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-feick" target="_blank">Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</a></h1><p class="meta"><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><strong>Martin Feick</strong></a> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/sfxTHsPJWHY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/sfxTHsPJWHY?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/sfxTHsPJWHY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Remote collaborators working together on physical objects have difficulty building a shared understanding of what each person is talking about. Conventional video chat systems are insufficient for many situations because they present a single view of the object in a flattened image. To understand how this limited perspective affects collaboration, we designed the Remote Manipulator (ReMa), which can reproduce orientation manipulations on a proxy object at a remote site. We conducted two studies with ReMa, with two main findings. First, a shared perspective is more effective and preferred compared to the opposing perspective offered by conventional video chat systems. Second, the physical proxy and video chat complement one another in a combined system: people used the physical proxy to understand objects, and used video chat to perform gestures and confirm remote actions.</p><div class="ui labels">Keywords:  <span class="ui large label">cscw</span><span class="ui large label">remote collaboration</span><span class="ui large label">object-focused collaboration</span><span class="ui large label">physical telepresence</span><span class="ui large label">collaborative physical tasks</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Terrance Tin Hoi Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173855" target="_blank">https://doi.org/10.1145/3173574.3173855</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-wuertz" class="ui large modal"><div class="header"><a href="/publications/chi-2018-wuertz" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-wuertz</a>  -  <a href="/publications/chi-2018-wuertz.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-wuertz.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-wuertz" target="_blank">A Design Framework for Awareness Cues in Distributed Multiplayer Games</a></h1><p class="meta"><span>Jason Wuertz</span> , <span>Sultan A. Alharthi</span> , <span>William A. Hamilton</span> , <span>Scott Bateman</span> , <span>Carl Gutwin</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>Zachary O. Toups</span> , <span>Jessica Hammer</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In the physical world, teammates develop situation awareness about each other&#x27;s location, status, and actions through cues such as gaze direction and ambient noise. To support situation awareness, distributed multiplayer games provide awareness cues - information that games automatically make available to players to support cooperative gameplay. The design of awareness cues can be extremely complex, impacting how players experience games and work with teammates. Despite the importance of awareness cues, designers have little beyond experiential knowledge to guide their design. In this work, we describe a design framework for awareness cues, providing insight into what information they provide, how they communicate this information, and how design choices can impact play experience. Our research, based on a grounded theory analysis of current games, is the first to provide a characterization of awareness cues, providing a palette for game designers to improve design practice and a starting point for deeper research into collaborative play.</p><div class="ui labels">Keywords:  <span class="ui large label">workspace awareness</span><span class="ui large label">situation awareness</span><span class="ui large label">game design</span><span class="ui large label">distributed multiplayer games</span><span class="ui large label">awareness cues</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jason Wuertz<!-- -->, <!-- -->Sultan A. Alharthi<!-- -->, <!-- -->William A. Hamilton<!-- -->, <!-- -->Scott Bateman<!-- -->, <!-- -->Carl Gutwin<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Zachary O. Toups<!-- -->, <!-- -->Jessica Hammer<!-- -->. <b>A Design Framework for Awareness Cues in Distributed Multiplayer Games</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->14<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173817" target="_blank">https://doi.org/10.1145/3173574.3173817</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-heshmat" class="ui large modal"><div class="header"><a href="/publications/chi-2018-heshmat" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-heshmat</a>  -  <a href="/publications/chi-2018-heshmat.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-heshmat.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-heshmat" target="_blank">Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing</a></h1><p class="meta"><span>Yasamin Heshmat</span> , <a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><strong>Brennan Jones</strong></a> , <span>Xiaoxuan Xiong</span> , <span>Carman Neustaedter</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>Bernhard E. Riecke</span> , <span>Lillian Yang</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>People often enjoy sharing outdoor activities together such as walking and hiking. However, when family and friends are separated by distance it can be difficult if not impossible to share such activities. We explore this design space by investigating the benefits and challenges of using a telepresence robot to support outdoor leisure activities. In our study, participants participated in the outdoor activity of geocaching where one person geocached with the help of a remote partner via a telepresence robot. We compared a wide field of view (WFOV) camera to a 360° camera. Results show the benefits of having a physical embodiment and a sense of immersion with the 360° view. Yet challenges related to a lack of environmental awareness, safety issues, and privacy concerns resulting from bystander interactions. These findings illustrate the need to design telepresence robots with the environment and public in mind to provide an enhanced sensory experience while balancing safety and privacy issues resulting from being amongst the general public.</p><div class="ui labels">Keywords:  <span class="ui large label">video communication</span><span class="ui large label">telepresence robots</span><span class="ui large label">leisure activities</span><span class="ui large label">social presence</span><span class="ui large label">geocaching</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Yasamin Heshmat<!-- -->, <!-- -->Brennan Jones<!-- -->, <!-- -->Xiaoxuan Xiong<!-- -->, <!-- -->Carman Neustaedter<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Bernhard E. Riecke<!-- -->, <!-- -->Lillian Yang<!-- -->. <b>Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173933" target="_blank">https://doi.org/10.1145/3173574.3173933</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="hri-2018-feick" class="ui large modal"><div class="header"><a href="/publications/hri-2018-feick" target="_blank"><i class="fas fa-link fa-fw"></i>hri-2018-feick</a>  -  <a href="/publications/hri-2018-feick.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">HRI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/hri-2018-feick" target="_blank">The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</a></h1><p class="meta"><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><strong>Martin Feick</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>André Miede</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we discuss the role of the movement trajectory and velocity enabled by our tele-robotic system (ReMa) for remote collaboration on physical tasks. Our system reproduces changes in object orientation and position at a remote location using a humanoid robotic arm. However, even minor kinematics differences between robot and human arm can result in awkward or exaggerated robot movements. As a result, user communication with the robotic system can become less efficient, less fluent and more time intensive.</p><div class="ui labels">Keywords:  <span class="ui large label">movement trajectory &amp; velocity</span><span class="ui large label">remote collaboration</span><span class="ui large label">robot surrogate</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->André Miede<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b>. <i>In Adjunct Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction (HRI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->2<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173386.3176959" target="_blank">https://doi.org/10.1145/3173386.3176959</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2016-jones" class="ui large modal"><div class="header"><a href="/publications/dis-2016-jones" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2016-jones</a>  -  <a href="/publications/dis-2016-jones.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2016</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2016-jones.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2016-jones" target="_blank">Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</a></h1><p class="meta"><a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><strong>Brennan Jones</strong></a> , <span>Kody Dillman</span> , <span>Richard Tang</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Carman Neustaedter</span> , <span>Scott Bateman</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/10hbJHIQVX8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/10hbJHIQVX8?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/10hbJHIQVX8/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>People are increasingly using mobile video to communicate, collaborate, and share experiences while on the go. Yet this presents challenges in adequately sharing camera views with remote users. In this paper, we study the use of semi-autonomous drones for video conferencing, where an outdoor user (using a smartphone) is connected to a desktop user who can explore the environment from the drone&#x27;s perspective. We describe findings from a study where pairs collaborated to complete shared navigation and search tasks. We illustrate the benefits of providing the desktop user with a view that is elevated, manipulable, and decoupled from the outdoor user. In addition, we articulate how participants overcame challenges in communicating environmental information and navigational cues, negotiated control of the view, and used the drone as a tool for sharing experiences. This provides a new way of thinking about mobile video conferencing where cameras that are decoupled from both users play an integral role in communication, collaboration, and sharing experiences.</p><div class="ui labels">Keywords:  <span class="ui large label">cscw</span><span class="ui large label">telepresence</span><span class="ui large label">video communication</span><span class="ui large label">shared experiences</span><span class="ui large label">teleoperation</span><span class="ui large label">drones</span><span class="ui large label">collaboration</span><span class="ui large label">hri</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Brennan Jones<!-- -->, <!-- -->Kody Dillman<!-- -->, <!-- -->Richard Tang<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Carman Neustaedter<!-- -->, <!-- -->Scott Bateman<!-- -->. <b>Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;16)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/2901790.2901847" target="_blank">https://doi.org/10.1145/2901790.2901847</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-aseniero" class="ui large modal"><div class="header"><a href="/publications/chi-2015-aseniero" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2015-aseniero</a>  -  <a href="/publications/chi-2015-aseniero.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-aseniero.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-aseniero" target="_blank">Stratos: Using Visualization to Support Decisions in Strategic Software Release Planning</a></h1><p class="meta"><a href="/people/bon-adriel-aseniero"><img src="/static/images/people/bon-adriel-aseniero.jpg" class="ui circular spaced image mini-profile"/><strong>Bon Adriel Aseniero</strong></a> , <span>Tiffany Wun</span> , <a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <span>Guenther Ruhe</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/qm57aHjTAYc" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/qm57aHjTAYc?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/qm57aHjTAYc/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Software is typically developed incrementally and released in stages. Planning these releases involves deciding which features of the system should be implemented for each release. This is a complex planning process involving numerous trade-offs-constraints and factors that often make decisions difficult. Since the success of a product depends on this plan, it is important to understand the trade-offs between different release plans in order to make an informed choice. We present STRATOS, a tool that simultaneously visualizes several software release plans. The visualization shows several attributes about each plan that are important to planners. Multiple plans are shown in a single layout to help planners find and understand the trade-offs between alternative plans. We evaluated our tool via a qualitative study and found that STRATOS enables a range of decision-making processes, helping participants decide on which plan is most optimal.</p><div class="ui labels">Keywords:  <span class="ui large label">software engineering</span><span class="ui large label">information visualization</span><span class="ui large label">release planning</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Bon Adriel Aseniero<!-- -->, <!-- -->Tiffany Wun<!-- -->, <!-- -->David Ledo<!-- -->, <!-- -->Guenther Ruhe<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->. <b>Stratos: Using Visualization to Support Decisions in Strategic Software Release Planning</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;15)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/2702123.2702426" target="_blank">https://doi.org/10.1145/2702123.2702426</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-jones" class="ui large modal"><div class="header"><a href="/publications/chi-2015-jones" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2015-jones</a>  -  <a href="/publications/chi-2015-jones.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-jones.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-jones" target="_blank">Mechanics of Camera Work in Mobile Video Collaboration</a></h1><p class="meta"><a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><strong>Brennan Jones</strong></a> , <span>Anna Witcraft</span> , <span>Scott Bateman</span> , <span>Carman Neustaedter</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Mobile video conferencing, where one or more participants are moving about in the real world, enables entirely new interaction scenarios (e.g., asking for help to construct or repair an object, or showing a physical location). While we have a good understanding of the challenges of video conferencing in office or home environments, we do not fully understand the mechanics of camera work-how people use mobile devices to communicate with one another-during mobile video calls. To provide an understanding of what people do in mobile video collaboration, we conducted an observational study where pairs of participants completed tasks using a mobile video conferencing system. Our analysis suggests that people use the camera view deliberately to support their interactions-for example, to convey a message or to ask questions-but the limited field of view, and the lack of camera control can make it a frustrating experience.</p><div class="ui labels">Keywords:  <span class="ui large label">video communication</span><span class="ui large label">collaboration</span><span class="ui large label">mobile computing</span><span class="ui large label">handheld devices</span><span class="ui large label">cscw</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Brennan Jones<!-- -->, <!-- -->Anna Witcraft<!-- -->, <!-- -->Scott Bateman<!-- -->, <!-- -->Carman Neustaedter<!-- -->, <!-- -->Anthony Tang<!-- -->. <b>Mechanics of Camera Work in Mobile Video Collaboration</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;15)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/2702123.2702345" target="_blank">https://doi.org/10.1145/2702123.2702345</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div></div><div class="one wide column"></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img style="max-width:180px;margin:30px auto" src="/static/images/logo-6.png"/><div class="content"><img style="max-width:200px;margin:0px auto" src="/static/images/logo-4.png"/><div class="sub header">Department of Computer Science</div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"id":"anthony-tang"}},"page":"/person","query":{"id":"anthony-tang"},"buildId":"CRJibYsfvdPuztTvq0Efo","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/person" src="/_next/static/CRJibYsfvdPuztTvq0Efo/pages/person.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/CRJibYsfvdPuztTvq0Efo/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.98f1a4e4c94db9943918.js" async=""></script><script src="/_next/static/runtime/main-22d58f57abab872d6e70.js" async=""></script></body></html>