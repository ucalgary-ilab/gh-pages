<!DOCTYPE html><html><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="format-detection" content="telephone=no"/><link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.css" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/css/lightbox.css" rel="stylesheet"/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link href="/static/css/style.css" rel="stylesheet"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox-plus-jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              $('.ui.sidebar')
                .sidebar('attach events', '.sidebar.icon')

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><meta charSet="utf-8" class="next-head"/><title class="next-head">Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="Human-Computer Interaction, HCI, Information Visualization, University of Calgary, CHI, UIST" class="next-head"/><meta name="description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta property="og:title" content="Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta property="og:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta property="og:site_name" content="University of Calgary Interactions Lab" class="next-head"/><meta property="og:url" content="https://ilab.ucalgary.ca/" class="next-head"/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/cover.jpg" class="next-head"/><meta property="og:type" content="website" class="next-head"/><meta name="twitter:title" content="Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta name="twitter:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/cover.jpg" class="next-head"/><meta name="twitter:card" content="summary" class="next-head"/><meta name="twitter:site" content="@ucalgary" class="next-head"/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" class="next-head"/><link rel="preload" href="/_next/static/TtqpnG459e06x8_W6ySjW/pages/index.js" as="script"/><link rel="preload" href="/_next/static/TtqpnG459e06x8_W6ySjW/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.c898a08c8bf8d90e85ef.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-fad3ab09a2039edff31b.js" as="script"/></head><body><div id="__next"><div><div><div class="ui right vertical sidebar menu"><a class="item" href="/">Home</a><a class="item" href="/publications">Publications</a><a class="item" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/seminar">Seminar</a><a class="item" href="/location">Location</a></div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/"><b>UCalgary iLab</b></a></div><div class="right menu"><a class="item" href="/publications">Publications</a><a class="item" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/seminar">Seminar</a><a class="item" href="/location">Location</a><div class="toc item"><a href="/"><b>UCalgary iLab</b></a><i style="float:right" class="sidebar icon"></i></div></div></div></div><div id="top-video-container"><video id="top-video" poster="/static/posters/top.png" preload="metadata" autoplay="" loop="" muted="" playsinline="" webkit-playsinline=""><source src="/static/videos/top.mp4" type="video/mp4"/></video></div><div class="ui stackable grid"><div class="eleven wide column centered"><div id="header-logo"><div><img src="/static/images/logo-5.png" style="height:200px"/></div></div><div id="header" class="category"><img src="/static/images/logo-4.png" style="height:100px;margin-top:0px"/><h1 style="font-size:2em">Interactions Lab</h1><p>Human-Computer Interaction and Information Visualization Group</p></div><div><h1 class="ui horizontal divider header">Research Labs</h1><div id="labs" class="ui four cards" style="text-align:center;margin-top:15px"><div class="card" style="padding:15px"><a href="https://utouch.cpsc.ucalgary.ca/" target="_blank" class="ui "><div class="img card-color-0"><img src="/static/images/labs/utouch.png"/></div><h3>Physical Interaction and Human-Robot Interaction</h3><p class="header">Prof. <!-- -->Ehud Sharlin</p></a></div><div class="card" style="padding:15px"><a href="http://pages.cpsc.ucalgary.ca/~lora.oehlberg/" target="_blank" class="ui "><div class="img card-color-1"><img src="/static/images/labs/curiosity.png"/></div><h3>Human-Centered Design for Creativity &amp; Curiosity</h3><p class="header">Prof. <!-- -->Lora Oehlberg</p></a></div><div class="card" style="padding:15px"><a href="https://dataexperience.cpsc.ucalgary.ca/" target="_blank" class="ui "><div class="img card-color-2"><img src="/static/images/labs/dataexperience.png"/></div><h3>Visual Data-driven Tools and Experiences</h3><p class="header">Prof. <!-- -->Wesley Willett</p></a></div><div class="card" style="padding:15px"><a href="https://programmable-reality-lab.github.io/" target="_blank" class="ui "><div class="img card-color-3"><img src="/static/images/labs/suzuki.png"/></div><h3>Programmable Reality Lab - Tangible, AR/VR, and Robotics</h3><p class="header">Prof. <!-- -->Ryo Suzuki</p></a></div><div class="card" style="padding:15px"><a href="https://helenaihe.com/research/" target="_blank" class="ui "><div class="img card-color-4"><img src="/static/images/labs/c3-lab.png"/></div><h3>Tech to Bridge Cultural Barriers, Improve Collaboration, &amp; Build Community</h3><p class="header">Prof. <!-- -->Helen Ai He</p></a></div><div class="card" style="padding:15px"><a href="http://grouplab.cpsc.ucalgary.ca/" target="_blank" class="ui "><div class="img card-color-5"><img src="/static/images/labs/grouplab.png"/></div><h3>Research in HCI, CSCW, and UbiComp</h3><p class="header">Prof. <!-- -->Saul Greenberg (Emeritus)</p></a></div><div class="card" style="padding:15px"><a href="https://ricelab.github.io/" target="_blank" class="ui "><div class="img card-color-6"><img src="/static/images/labs/ricelab.png"/></div><h3>Rethinking Interaction, Collaboration, &amp; Engagement</h3><p class="header">Prof. <!-- -->Anthony Tang (Adjunct - University of Toronto)</p></a></div><div class="card" style="padding:15px"><a href="http://sheelaghcarpendale.ca/" target="_blank" class="ui "><div class="img card-color-7"><img src="/static/images/labs/innovis.png"/></div><h3>Innovations in Visualization Laboratory</h3><p class="header">Prof. <!-- -->Sheelagh Carpendale (Adjunct - Simon Fraser University)</p></a></div></div></div><div id="people" class="category"><h1 class="ui horizontal divider header"><i class="child icon"></i>Faculty</h1><div class="people-category"><h2></h2><div class="ui grid"><a class="five wide column person" href="/people/ehud-sharlin"><img class="ui circular image medium-profile" src="/static/images/people/ehud-sharlin.jpg"/><p><b>Ehud Sharlin</b></p><p>Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">HRI</span><span class="ui large inverted label label-brown-color">Robots</span><span class="ui large inverted label label-brown-color">Drones</span></div></a><a class="five wide column person" href="/people/lora-oehlberg"><img class="ui circular image medium-profile" src="/static/images/people/lora-oehlberg.jpg"/><p><b>Lora Oehlberg</b></p><p>Associate Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Tangible</span><span class="ui large inverted label label-brown-color">Design Tools</span></div></a><a class="five wide column person" href="/people/wesley-willett"><img class="ui circular image medium-profile" src="/static/images/people/wesley-willett.jpg"/><p><b>Wesley Willett</b></p><p>Associate Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Data Viz</span><span class="ui large inverted label label-brown-color">Data Phyz</span><span class="ui large inverted label label-brown-color">AR</span></div></a><a class="five wide column person" href="/people/ryo-suzuki"><img class="ui circular image medium-profile" src="/static/images/people/ryo-suzuki.jpg"/><p><b>Ryo Suzuki</b></p><p>Assistant Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Tangible</span><span class="ui large inverted label label-brown-color">Robots</span><span class="ui large inverted label label-brown-color">AR/VR</span></div></a><a class="five wide column person" href="/people/helen-ai-he"><img class="ui circular image medium-profile" src="/static/images/people/helen-ai-he.jpg"/><p><b>Helen Ai He</b></p><p>Assistant Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Inclusive Design</span><span class="ui large inverted label label-brown-color">CSCW</span></div></a><a class="five wide column person" href="/people/aditya-shekhar-nittala"><img class="ui circular image medium-profile" src="/static/images/people/aditya-shekhar-nittala.jpg"/><p><b>Aditya Shekhar Nittala</b></p><p>Assistant Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Wearable Computing</span><span class="ui large inverted label label-brown-color">Fabrication</span><span class="ui large inverted label label-brown-color">Interaction Techniques</span></div></a><a class="five wide column person" href="/people/anthony-tang"><img class="ui circular image medium-profile" src="/static/images/people/anthony-tang.jpg"/><p><b>Anthony Tang</b></p><p>Adjunct Associate Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Mixed Reality</span><span class="ui large inverted label label-brown-color">CSCW</span></div></a><a class="five wide column person" href="/people/saul-greenberg"><img class="ui circular image medium-profile" src="/static/images/people/saul-greenberg.jpg"/><p><b>Saul Greenberg</b></p><p>Emeritus Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">UbiComp</span><span class="ui large inverted label label-brown-color">CSCW</span></div></a><a class="five wide column person" href="/people/sheelagh-carpendale"><img class="ui circular image medium-profile" src="/static/images/people/sheelagh-carpendale.jpg"/><p><b>Sheelagh Carpendale</b></p><p>Adjunct Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Data Viz</span><span class="ui large inverted label label-brown-color">Data Phyz</span></div></a></div></div></div><div id="publications" class="category"><h1 class="ui horizontal divider header"><i class="file alternate outline icon"></i>Recent Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="chi-2022-bressa"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-bressa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>Data Every Day: Designing and Living with Personal Situated Visualizations</b></p><p><a href="/people/nathalie-bressa"><img src="/static/images/people/nathalie-bressa.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nathalie Bressa</span></a> , <span>Jo Vermeulen</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Self Tracking</span><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Personal Data</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-ivanov"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-ivanov.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>One Week in the Future: Previs Design Futuring for HCI Research</b></p><p><a href="/people/sasha-ivanov"><img src="/static/images/people/sasha-ivanov.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sasha Ivanov</span></a> , <a href="/people/tim-au-yeung"><img src="/static/images/people/tim-au-yeung.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Tim Au Yeung</span></a> , <a href="/people/kathryn-blair"><img src="/static/images/people/kathryn-blair.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kathryn Blair</span></a> , <a href="/people/kurtis-danyluk"><img src="/static/images/people/kurtis-danyluk.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kurtis Danyluk</span></a> , <a href="/people/georgina-freeman"><img src="/static/images/people/georgina-freeman.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Georgina Freeman</span></a> , <a href="/people/marcus-friedel"><img src="/static/images/people/marcus-friedel.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Marcus Friedel</span></a> , <a href="/people/carmen-hull"><img src="/static/images/people/carmen-hull.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Carmen Hull</span></a> , <a href="/people/michael-hung"><img src="/static/images/people/michael-hung.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Michael Hung</span></a> , <a href="/people/sydney-pratte"><img src="/static/images/people/sydney-pratte.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sydney Pratte</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Design Futuring</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Previsualization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-nittala"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-nittala.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices</b></p><p><a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <span>Jürgen Steimle</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Wearable Devices</span><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Survey</span><span class="ui brown basic label">Soft Wearables</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <a href="/people/adnan-karim"><img src="/static/images/people/adnan-karim.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Adnan Karim</span></a> , <a href="/people/tian-xia"><img src="/static/images/people/tian-xia.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Tian Xia</span></a> , <span>Hooman Hedayati</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nicolai Marquardt</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Robotics</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Actuated Tangible UI</span><span class="ui brown basic label">Shape Changing UI</span><span class="ui brown basic label">AR HRI</span><span class="ui brown basic label">VAM HRI</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-ea-2022-blair"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-ea-2022-blair.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI EA 2022</span></p><p class="color" style="font-size:1.3em"><b>Art is Not Research. Research is not Art</b></p><p><a href="/people/kathryn-blair"><img src="/static/images/people/kathryn-blair.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kathryn Blair</span></a> , <span>Miriam Sturdee</span> , <span>Lindsay Macdonald Vermeulen</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Interdisciplinary Research</span><span class="ui brown basic label">Research Ethics</span><span class="ui brown basic label">Arts And Computing</span><span class="ui brown basic label">Research Methods</span><span class="ui brown basic label">Knowledge Creation</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2021-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2021-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2021</span></p><p class="color" style="font-size:1.3em"><b>HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Eyal Ofek</span> , <span>Mike Sinclair</span> , <span>Daniel Leithinger</span> , <span>Mar Gonzalez-Franco</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Encountered Type Haptics</span><span class="ui brown basic label">Tabletop Mobile Robots</span><span class="ui brown basic label">Swarm User Interfaces</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="ieee-2021-willett"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/ieee-2021-willett.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IEEE 2021</span><span class="ui big basic pink label"><b><i class="fas fa-trophy"></i> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>Perception! Immersion! Empowerment!: Superpowers as Inspiration for Visualization</b></p><p><a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <a href="/people/bon-adriel-aseniero"><img src="/static/images/people/bon-adriel-aseniero.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Bon Adriel Aseniero</span></a> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a> , <span>Pierre Dragicevic</span> , <span>Yvonne Jansen</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Petra Isenberg</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Cognition</span><span class="ui brown basic label">Interactive Systems</span><span class="ui brown basic label">Tools</span><span class="ui brown basic label">Pragmatics</span><span class="ui brown basic label">Pattern Recognition</span><span class="ui brown basic label">Superpowers</span><span class="ui brown basic label">Empowerment</span><span class="ui brown basic label">Vision</span><span class="ui brown basic label">Perception</span><span class="ui brown basic label">Fiction</span><span class="ui brown basic label">Situated Visualization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2021-asha"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2021-asha.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2021</span></p><p class="color" style="font-size:1.3em"><b>Co-Designing Interactions between Pedestrians in Wheelchairs and Autonomous Vehicles</b></p><p><a href="/people/ashratuz-zavin-asha"><img src="/static/images/people/ashratuz-zavin-asha.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ashratuz Zavin Asha</span></a> , <a href="/people/christopher-smith"><img src="/static/images/people/christopher-smith.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Christopher Smith</span></a> , <a href="/people/georgina-freeman"><img src="/static/images/people/georgina-freeman.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Georgina Freeman</span></a> , <span>Sean Crump</span> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Pedestrians In Wheelchairs</span><span class="ui brown basic label">Co Design</span><span class="ui brown basic label">Autonomous Vehicles</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2021-blair"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2021-blair.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2021</span></p><p class="color" style="font-size:1.3em"><b>Participatory Art for Public Exploration of Algorithmic Decision-Making</b></p><p><a href="/people/kathryn-blair"><img src="/static/images/people/kathryn-blair.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kathryn Blair</span></a> , <span>Pil Hansen</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Fine Arts</span><span class="ui brown basic label">Physical Artifact</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Social Impact Of Technology</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Participatory Art Installation</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2021-wannamaker"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2021-wannamaker.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2021</span></p><p class="color" style="font-size:1.3em"><b>I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking</b></p><p><a href="/people/kendra-wannamaker"><img src="/static/images/people/kendra-wannamaker.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kendra Wannamaker</span></a> , <span>Sandeep Kollannur</span> , <span>Marian Dörk</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Personal Informatics</span><span class="ui brown basic label">Situated Visualization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2021-danyluk"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2021-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2021</span></p><p class="color" style="font-size:1.3em"><b>A Design Space Exploration of Worlds in Miniature</b></p><p><a href="/people/kurtis-danyluk"><img src="/static/images/people/kurtis-danyluk.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kurtis Danyluk</span></a> , <span>Barrett Ens</span> , <span>Bernhard Jenny</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Virtual Augmented Reality</span><span class="ui brown basic label">Meta Analysis Literature Survey</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2021-ens"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2021-ens.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2021</span></p><p class="color" style="font-size:1.3em"><b>Grand Challenges in Immersive Analytics</b></p><p><span>Barrett Ens</span> , <span>Benjamin Bach</span> , <span>Maxime Cordeil</span> , <span>Ulrich Engelke</span> , <span>Marcos Serrano</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <span>Arnaud Prouzeau</span> , <span>Christoph Anthes</span> , <span>Wolfgang Büschel</span> , <span>Cody Dunne</span> , <span>Tim Dwyer</span> , <span>Jens Grubert</span> , <span>Jason H. Haga</span> , <span>Nurit Kishenbaum</span> , <span>Dylan Kobayashi</span> , <span>Tica Lin</span> , <span>Monsurat Olaosebikan</span> , <span>Fabian Pointecker</span> , <span>David Saffo</span> , <span>Nazmus Saquib</span> , <span>Dieter Schmalsteig</span> , <span>Danielle Albers Szafir</span> , <span>Matthew Whitlock</span> , <span>Yalong Yang</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">Grand Research Challenges</span><span class="ui brown basic label">Data Visualisation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Virtual Reality</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2021-hammad"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2021-hammad.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2021</span></p><p class="color" style="font-size:1.3em"><b>Homecoming: Exploring Returns to Long-Term Single Player Games</b></p><p><span>Noor Hammad</span> , <span>Owen Brierley</span> , <a href="/people/zachary-mckendrick"><img src="/static/images/people/zachary-mckendrick.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Zachary McKendrick</span></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <span>Patrick Finn</span> , <span>Jessica Hammer</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Long Term Single Player Game</span><span class="ui brown basic label">Autobiographical Design</span><span class="ui brown basic label">Pivot Point</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="gi-2021-mactavish"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/gi-2021-mactavish.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">GI 2021</span></p><p class="color" style="font-size:1.3em"><b>Perspective Charts</b></p><p><span>Mia MacTavish</span> , <span>Katayoon Etemad</span> , <span>Faramarz Samavati</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="ijac-2021-hosseini"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/ijac-2021-hosseini.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IJAC 2021</span></p><p class="color" style="font-size:1.3em"><b>Optically illusive architecture (OIA): Introduction and evaluation using virtual reality</b></p><p><span>Seyed Vahab Hosseini</span> , <span>Usman R Alim</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Joshua M Taron</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Architectural Representation</span><span class="ui brown basic label">Optical Illusion</span><span class="ui brown basic label">Design Evaluation</span><span class="ui brown basic label">Virtual Reality</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="cupum-2021-rout"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/cupum-2021-rout.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Urban Informatics and Future Cities</span></p><p class="color" style="font-size:1.3em"><b>(Big) Data in Urban Design Practice: Supporting High-Level Design Tasks Using a Visualization of Human Movement Data from Smartphones</b></p><p><span>Angela Rout</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Smartphone Data</span><span class="ui brown basic label">GPS</span><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Architecture</span><span class="ui brown basic label">Urban Design</span><span class="ui brown basic label">Task Based Framework</span><span class="ui brown basic label">High Level Tasks</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2021-pratte"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2021-pratte.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2021</span></p><p class="color" style="font-size:1.3em"><b>Evoking Empathy: A Framework for Describing Empathy Tools</b></p><p><a href="/people/sydney-pratte"><img src="/static/images/people/sydney-pratte.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sydney Pratte</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Empathy</span><span class="ui brown basic label">Empathy Tools</span><span class="ui brown basic label">Design Strategies</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2020-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2020</span><span class="ui big basic pink label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Rubaiat Habib Kazi</span> , <span>Li-Yi Wei</span> , <span>Stephen DiVerdi</span> , <span>Wilmot Li</span> , <span>Daniel Leithinger</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Real Time Authoring</span><span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Tangible Interaction</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2020-yixian"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-yixian.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2020</span></p><p class="color" style="font-size:1.3em"><b>ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World</b></p><p><span>Yan Yixian</span> , <span>Kazuki Takashima</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>Takayuki Tanno</span> , <span>Kazuyuki Fujita</span> , <span>Yoshifumi Kitamura</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Encountered Type Haptic Devices</span><span class="ui brown basic label">Immersive Experience</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="iros-2020-hedayati"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/iros-2020-hedayati.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IROS 2020</span></p><p class="color" style="font-size:1.3em"><b>PufferBot: Actuated Expandable Structures for Aerial Robots</b></p><p><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><p></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2020-danyluk"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2020-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2020</span></p><p class="color" style="font-size:1.3em"><b>Touch and Beyond: Comparing Physical and Virtual Reality Visualizations</b></p><p><span>Kurtis Thorvald Danyluk</span> , <span>Teoman Tomo Ulusoy</span> , <a href="/people/wei-wei"><img src="/static/images/people/wei-wei.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wei Wei</span></a> , <span>Wesley J. Willett</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Physicalization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="nime-2020-ko"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/nime-2020-ko.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">NIME 2020</span></p><p class="color" style="font-size:1.3em"><b>Touch Responsive Augmented Violin Interface System II: Integrating Sensors into a 3D Printed Fingerboard</b></p><p><span>Chantelle Ko</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Violin</span><span class="ui brown basic label">Touch Sensor</span><span class="ui brown basic label">FSR</span><span class="ui brown basic label">Fingerboard</span><span class="ui brown basic label">Augmented</span><span class="ui brown basic label">3 D Printing</span><span class="ui brown basic label">Conductive Filament</span><span class="ui brown basic label">Interactive</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="imwut-2020-wang"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/imwut-2020-wang.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IMWUT 2020</span></p><p class="color" style="font-size:1.3em"><b>AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</b></p><p><span>Xiyue Wang</span> , <span>Kazuki Takashima</span> , <span>Tomoaki Adachi</span> , <span>Patrick Finn</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <span>Yoshifumi Kitamura</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Well Being</span><span class="ui brown basic label">Toy Blocks</span><span class="ui brown basic label">PTSD</span><span class="ui brown basic label">Tangibles For Health</span><span class="ui brown basic label">Stress Assessment</span><span class="ui brown basic label">Play</span><span class="ui brown basic label">Children</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="imx-2020-mok"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/imx-2020-mok.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IMX 2020</span></p><p class="color" style="font-size:1.3em"><b>Talk Like Somebody is Watching: Understanding and Supporting Novice Live Streamers</b></p><p><a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Terrance Mok</span></a> , <a href="/people/colin-auyeung"><img src="/static/images/people/colin-auyeung.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Colin Au Yeung</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Live Streams</span><span class="ui brown basic label">Streamers</span><span class="ui brown basic label">Chatbot</span><span class="ui brown basic label">Audience</span><span class="ui brown basic label">Virtual Audience</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Hooman Hedayati</span> , <span>Clement Zheng</span> , <span>James Bohn</span> , <span>Daniel Szafir</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D Gross</span> , <span>Daniel Leithinger</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Room Scale Haptics</span><span class="ui brown basic label">Haptic Interfaces</span><span class="ui brown basic label">Swarm Robots</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-anjani"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b></p><p><span>Laurensia Anjani</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Terrance Mok</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Wooi Boon Goh</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Video Streams</span><span class="ui brown basic label">Mukbang</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-asha"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-asha.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020 LBW</span></p><p class="color" style="font-size:1.3em"><b>Views from the Wheelchair: Understanding Interaction between Autonomous Vehicle and Pedestrians with Reduced Mobility</b></p><p><a href="/people/ashratuz-zavin-asha"><img src="/static/images/people/ashratuz-zavin-asha.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ashratuz Zavin Asha</span></a> , <a href="/people/christopher-smith"><img src="/static/images/people/christopher-smith.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Christopher Smith</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicle</span><span class="ui brown basic label">Pedestrian With Reduced Mobility</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-goffin"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-goffin.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</b></p><p><span>Pascal Goffin</span> , <span>Tanja Blascheck</span> , <span>Petra Isenberg</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Glyphs</span><span class="ui brown basic label">Word Scale Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Text Visualization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-hou"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b></p><p><span>Ming Hou</span> , <a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Karthik Mahadevan</span></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Autonomous Vehicle Cyclist Interaction</span><span class="ui brown basic label">Interfaces For Communicating Intent And Awareness</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2020-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2020</span></p><p class="color" style="font-size:1.3em"><b>LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Ryosuke Nakayama</span> , <span>Dan Liu</span> , <span>Yasuaki Kakehi</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Shape Changing Interfaces</span><span class="ui brown basic label">Inflatables</span><span class="ui brown basic label">Large Scale Interactions</span></div></p></div></div></div><div id="publications-modal"><div id="chi-2022-bressa" class="ui large modal"><div class="header"><a href="/publications/chi-2022-bressa" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2022-bressa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-bressa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-bressa" target="_blank">Data Every Day: Designing and Living with Personal Situated Visualizations</a></h1><p class="meta"><a href="/people/nathalie-bressa"><img src="/static/images/people/nathalie-bressa.jpg" class="ui circular spaced image mini-profile"/><strong>Nathalie Bressa</strong></a> , <span>Jo Vermeulen</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/chi-2022-bressa.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2022-bressa.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/B0bKMgDd1xY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/B0bKMgDd1xY?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/B0bKMgDd1xY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the design and utility of situated manual self-tracking visualizations on dedicated displays that integrate data tracking into existing practices and physical environments. Situating self-tracking tools in relevant locations is a promising approach to enable reflection on and awareness of data without needing to rely on sensorized tracking or personal devices. In both a long-term autobiographical design process and a co-design study with six participants, we rapidly prototyped and deployed 30 situated self-tracking applications over a ten month period. Grounded in the experience of designing and living with these trackers, we contribute findings on logging and data entry, the use of situated displays, and the visual design and customization of trackers. Our results demonstrate the potential of customizable dedicated self-tracking visualizations that are situated in relevant physical spaces, and suggest future research opportunities and new potential applications for situated visualizations.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Self Tracking</span><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Personal Data</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nathalie Bressa<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Data Every Day: Designing and Living with Personal Situated Visualizations</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->18<!-- -->.  DOI: <a href="https://doi.org/10.1145/3491102.3517737" target="_blank">https://doi.org/10.1145/3491102.3517737</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-ivanov" class="ui large modal"><div class="header"><a href="/publications/chi-2022-ivanov" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2022-ivanov</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-ivanov.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-ivanov" target="_blank">One Week in the Future: Previs Design Futuring for HCI Research</a></h1><p class="meta"><a href="/people/sasha-ivanov"><img src="/static/images/people/sasha-ivanov.jpg" class="ui circular spaced image mini-profile"/><strong>Sasha Ivanov</strong></a> , <a href="/people/tim-au-yeung"><img src="/static/images/people/tim-au-yeung.jpg" class="ui circular spaced image mini-profile"/><strong>Tim Au Yeung</strong></a> , <a href="/people/kathryn-blair"><img src="/static/images/people/kathryn-blair.jpg" class="ui circular spaced image mini-profile"/><strong>Kathryn Blair</strong></a> , <a href="/people/kurtis-danyluk"><img src="/static/images/people/kurtis-danyluk.jpg" class="ui circular spaced image mini-profile"/><strong>Kurtis Danyluk</strong></a> , <a href="/people/georgina-freeman"><img src="/static/images/people/georgina-freeman.jpg" class="ui circular spaced image mini-profile"/><strong>Georgina Freeman</strong></a> , <a href="/people/marcus-friedel"><img src="/static/images/people/marcus-friedel.jpg" class="ui circular spaced image mini-profile"/><strong>Marcus Friedel</strong></a> , <a href="/people/carmen-hull"><img src="/static/images/people/carmen-hull.jpg" class="ui circular spaced image mini-profile"/><strong>Carmen Hull</strong></a> , <a href="/people/michael-hung"><img src="/static/images/people/michael-hung.jpg" class="ui circular spaced image mini-profile"/><strong>Michael Hung</strong></a> , <a href="/people/sydney-pratte"><img src="/static/images/people/sydney-pratte.jpg" class="ui circular spaced image mini-profile"/><strong>Sydney Pratte</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/chi-2022-ivanov.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2022-ivanov.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/qoIwYW83iSU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/qoIwYW83iSU?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/qoIwYW83iSU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the use of cinematic “pre-visualization” (previs) techniques as a rapid ideation and design futuring method for human computer interaction (HCI) research. Previs approaches, which are widely used in animation and film production, use digital design tools to create medium-fidelity videos that capture richer interaction, motion, and context than sketches or static illustrations. When used as a design futuring method, previs can facilitate rapid, iterative discussions that reveal tensions, challenges, and opportunities for new research. We performed eight one-week design futuring sprints, in which individual HCI researchers collaborated with a lead designer to produce concept sketches, storyboards, and videos that examined future applications of their research. From these experiences, we identify recurring themes and challenges and present a One Week Futuring Workbook that other researchers can use to guide their own futuring sprints. We also highlight how variations of our approach could support other speculative design practices.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Design Futuring</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Previsualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sasha Ivanov<!-- -->, <!-- -->Tim Au Yeung<!-- -->, <!-- -->Kathryn Blair<!-- -->, <!-- -->Kurtis Danyluk<!-- -->, <!-- -->Georgina Freeman<!-- -->, <!-- -->Marcus Friedel<!-- -->, <!-- -->Carmen Hull<!-- -->, <!-- -->Michael Hung<!-- -->, <!-- -->Sydney Pratte<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>One Week in the Future: Previs Design Futuring for HCI Research</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->15<!-- -->.  DOI: <a href="https://doi.org/10.1145/3491102.3517584" target="_blank">https://doi.org/10.1145/3491102.3517584</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-nittala" class="ui large modal"><div class="header"><a href="/publications/chi-2022-nittala" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2022-nittala</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-nittala.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-nittala" target="_blank">Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices</a></h1><p class="meta"><a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Jürgen Steimle</span></p><p><a href="/static/publications/chi-2022-nittala.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2022-nittala.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Lj9Yk5IQsok" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Lj9Yk5IQsok?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/Lj9Yk5IQsok/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Skin is a promising interaction medium and has been widely explored for mobile, and expressive interaction. Recent research in HCI has seen the development of Epidermal Computing Devices: ultra-thin and non-invasive devices which reside on the user’s skin, offering intimate integration with the curved surfaces of the body, while having physical and mechanical properties that are akin to skin, expanding the horizon of on-body interaction. However, with rapid technological advancements in multiple disciplines, we see a need to synthesize the main open research questions and opportunities for the HCI community to advance future research in this area. By systematically analyzing Epidermal Devices contributed in the HCI community, physical sciences research and from our experiences in designing and building Epidermal Devices, we identify opportunities and challenges for advancing research across five themes. This multi-disciplinary synthesis enables multiple research communities to facilitate progression towards more coordinated endeavors for advancing Epidermal Computing.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Wearable Devices</span><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Survey</span><span class="ui brown basic label">Soft Wearables</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Aditya Shekhar Nittala<!-- -->, <!-- -->Jürgen Steimle<!-- -->. <b>Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->22<!-- -->.  DOI: <a href="https://doi.org/10.1145/3491102.3517668" target="_blank">https://doi.org/10.1145/3491102.3517668</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-suzuki" class="ui large modal"><div class="header"><a href="/publications/chi-2022-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2022-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-suzuki" target="_blank">Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <a href="/people/adnan-karim"><img src="/static/images/people/adnan-karim.jpg" class="ui circular spaced image mini-profile"/><strong>Adnan Karim</strong></a> , <a href="/people/tian-xia"><img src="/static/images/people/tian-xia.jpg" class="ui circular spaced image mini-profile"/><strong>Tian Xia</strong></a> , <span>Hooman Hedayati</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><strong>Nicolai Marquardt</strong></a></p><p><a href="/static/publications/chi-2022-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2022-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/MvOWxQC_4uQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/MvOWxQC_4uQ?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/MvOWxQC_4uQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper contributes to a taxonomy of augmented reality and robotics based on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have emerged as a new way to enhance human-robot interaction (HRI) and robotic interfaces (e.g., actuated and shape-changing interfaces). Recently, an increasing number of studies in HCI, HRI, and robotics have demonstrated how AR enables better interactions between people and robots. However, often research remains focused on individual explorations and key design strategies, and research questions are rarely analyzed systematically. In this paper, we synthesize and categorize this research field in the following dimensions: 1) approaches to augmenting reality; 2) characteristics of robots; 3) purposes and benefits; 4) classification of presented information; 5) design components and strategies for visual augmentation; 6) interaction techniques and modalities; 7) application domains; and 8) evaluation strategies. We formulate key challenges and opportunities to guide and inform future research in AR and robotics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Robotics</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Actuated Tangible UI</span><span class="ui brown basic label">Shape Changing UI</span><span class="ui brown basic label">AR HRI</span><span class="ui brown basic label">VAM HRI</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Adnan Karim<!-- -->, <!-- -->Tian Xia<!-- -->, <!-- -->Hooman Hedayati<!-- -->, <!-- -->Nicolai Marquardt<!-- -->. <b>Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->33<!-- -->.  DOI: <a href="https://doi.org/10.1145/3491102.3517719" target="_blank">https://doi.org/10.1145/3491102.3517719</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-ea-2022-blair" class="ui large modal"><div class="header"><a href="/publications/chi-ea-2022-blair" target="_blank"><i class="fas fa-link fa-fw"></i>chi-ea-2022-blair</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI EA 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-ea-2022-blair.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-ea-2022-blair" target="_blank">Art is Not Research. Research is not Art</a></h1><p class="meta"><a href="/people/kathryn-blair"><img src="/static/images/people/kathryn-blair.jpg" class="ui circular spaced image mini-profile"/><strong>Kathryn Blair</strong></a> , <span>Miriam Sturdee</span> , <span>Lindsay Macdonald Vermeulen</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p><p><a href="/static/publications/chi-ea-2022-blair.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-ea-2022-blair.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/G0CSYn_uhIE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/G0CSYn_uhIE?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/G0CSYn_uhIE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Art is not Research. Research is not Art. is a multimedia, multi-site participatory installation by a collective of artists and researchers from Calgary, Toronto, and Lancaster; it is informed by these contexts. It reflects the tensions between how “participants” are treated in participatory art and interaction research. It offers a framework through which we can explore how epistemologies might evolve in a blending between Art and Research. Visitors download the paper to read, critically reflect on the relationship between art and research, and experientially engage with the material through a series of creative prompts. A performance variation of the piece will be performed in-person and online through the ACM SIGCHI Conference on Human Factors in Computing Systems alt.chi track.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Interdisciplinary Research</span><span class="ui brown basic label">Research Ethics</span><span class="ui brown basic label">Arts And Computing</span><span class="ui brown basic label">Research Methods</span><span class="ui brown basic label">Knowledge Creation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kathryn Blair<!-- -->, <!-- -->Miriam Sturdee<!-- -->, <!-- -->Lindsay Macdonald Vermeulen<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Art is Not Research. Research is not Art</b>. <i>In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->9<!-- -->.  DOI: <a href="https://doi.org/10.1145/3491101.3516391" target="_blank">https://doi.org/10.1145/3491101.3516391</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2021-suzuki" class="ui large modal"><div class="header"><a href="/publications/uist-2021-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2021-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2021-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2021-suzuki" target="_blank">HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Eyal Ofek</span> , <span>Mike Sinclair</span> , <span>Daniel Leithinger</span> , <span>Mar Gonzalez-Franco</span></p><p><a href="/static/publications/uist-2021-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2021-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HTiZgOESJyQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HTiZgOESJyQ?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/HTiZgOESJyQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic ap- proaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in- time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various ap- plications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Encountered Type Haptics</span><span class="ui brown basic label">Tabletop Mobile Robots</span><span class="ui brown basic label">Swarm User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Eyal Ofek<!-- -->, <!-- -->Mike Sinclair<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Mar Gonzalez-Franco<!-- -->. <b>HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;21)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3472749.3474821" target="_blank">https://doi.org/10.1145/3472749.3474821</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="ieee-2021-willett" class="ui large modal"><div class="header"><a href="/publications/ieee-2021-willett" target="_blank"><i class="fas fa-link fa-fw"></i>ieee-2021-willett</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IEEE 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/ieee-2021-willett.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/ieee-2021-willett" target="_blank">Perception! Immersion! Empowerment!: Superpowers as Inspiration for Visualization</a></h1><p class="meta"><a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <a href="/people/bon-adriel-aseniero"><img src="/static/images/people/bon-adriel-aseniero.jpg" class="ui circular spaced image mini-profile"/><strong>Bon Adriel Aseniero</strong></a> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a> , <span>Pierre Dragicevic</span> , <span>Yvonne Jansen</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Petra Isenberg</span></p><p><a href="/static/publications/ieee-2021-willett.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>ieee-2021-willett.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We explore how the lens of fictional superpowers can help characterize how visualizations empower people and provide inspiration for new visualization systems. Researchers and practitioners often tout visualizations&#x27; ability to “make the invisible visible” and to “enhance cognitive abilities.” Meanwhile superhero comics and other modern fiction often depict characters with similarly fantastic abilities that allow them to see and interpret the world in ways that transcend traditional human perception. We investigate the intersection of these domains, and show how the language of superpowers can be used to characterize existing visualization systems and suggest opportunities for new and empowering ones. We introduce two frameworks: The first characterizes seven underlying mechanisms that form the basis for a variety of visual superpowers portrayed in fiction. The second identifies seven ways in which visualization tools and interfaces can instill a sense of empowerment in the people who use them. Building on these observations, we illustrate a diverse set of “visualization superpowers” and highlight opportunities for the visualization community to create new systems and interactions that empower new experiences with data. Material and illustrations are available under CC-BY 4.0 at osf.io/8yhfz.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Cognition</span><span class="ui brown basic label">Interactive Systems</span><span class="ui brown basic label">Tools</span><span class="ui brown basic label">Pragmatics</span><span class="ui brown basic label">Pattern Recognition</span><span class="ui brown basic label">Superpowers</span><span class="ui brown basic label">Empowerment</span><span class="ui brown basic label">Vision</span><span class="ui brown basic label">Perception</span><span class="ui brown basic label">Fiction</span><span class="ui brown basic label">Situated Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Wesley Willett<!-- -->, <!-- -->Bon Adriel Aseniero<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Pierre Dragicevic<!-- -->, <!-- -->Yvonne Jansen<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Petra Isenberg<!-- -->. <b>Perception! Immersion! Empowerment!: Superpowers as Inspiration for Visualization</b>. <i>In undefined (IEEE &#x27;21)</i>. <!-- -->  Page: 1-<!-- -->11<!-- -->.  DOI: <a href="10.1109/TVCG.2021.3114844" target="_blank">10.1109/TVCG.2021.3114844</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2021-asha" class="ui large modal"><div class="header"><a href="/publications/dis-2021-asha" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2021-asha</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2021-asha.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2021-asha" target="_blank">Co-Designing Interactions between Pedestrians in Wheelchairs and Autonomous Vehicles</a></h1><p class="meta"><a href="/people/ashratuz-zavin-asha"><img src="/static/images/people/ashratuz-zavin-asha.jpg" class="ui circular spaced image mini-profile"/><strong>Ashratuz Zavin Asha</strong></a> , <a href="/people/christopher-smith"><img src="/static/images/people/christopher-smith.jpg" class="ui circular spaced image mini-profile"/><strong>Christopher Smith</strong></a> , <a href="/people/georgina-freeman"><img src="/static/images/people/georgina-freeman.jpg" class="ui circular spaced image mini-profile"/><strong>Georgina Freeman</strong></a> , <span>Sean Crump</span> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p><p><a href="/static/publications/dis-2021-asha.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>dis-2021-asha.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In the near future, mixed traffic consisting of manual and autonomous vehicles (AVs) will be common. Questions surrounding how vulnerable road users such as pedestrians in wheelchairs (PWs) will make crossing decisions in these new situations are underexplored. We conducted a remote co-design study with one of the researchers of this work who has the lived experience as a powered wheelchair user and applied inclusive design practices. This allowed us to identify and reflect on interface design ideas that can help PWs make safe crossing decisions at intersections. Through an iterative five-week study, we implemented interfaces that can be placed on the vehicle, on the wheelchair, and on the street infrastructure and evaluated them during the co-design sessions using a VR simulator testbed. Informed by our findings, we discuss design insights for implementing inclusive interfaces to improve interactions between autonomous vehicles and vulnerable road users.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Pedestrians In Wheelchairs</span><span class="ui brown basic label">Co Design</span><span class="ui brown basic label">Autonomous Vehicles</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ashratuz Zavin Asha<!-- -->, <!-- -->Christopher Smith<!-- -->, <!-- -->Georgina Freeman<!-- -->, <!-- -->Sean Crump<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Co-Designing Interactions between Pedestrians in Wheelchairs and Autonomous Vehicles</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;21)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="10.1145/3461778.3462068" target="_blank">10.1145/3461778.3462068</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2021-blair" class="ui large modal"><div class="header"><a href="/publications/dis-2021-blair" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2021-blair</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2021-blair.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2021-blair" target="_blank">Participatory Art for Public Exploration of Algorithmic Decision-Making</a></h1><p class="meta"><a href="/people/kathryn-blair"><img src="/static/images/people/kathryn-blair.jpg" class="ui circular spaced image mini-profile"/><strong>Kathryn Blair</strong></a> , <span>Pil Hansen</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p><p><a href="/static/publications/dis-2021-blair.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>dis-2021-blair.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Machine learning and predictive algorithms find patterns in large stores of data and make predictions which corporations and governments use to support decision-making. Yet, the system&#x27;s representation of reality can be more influential to outcomes than the complexities of daily life. They become problematic when they undermine the inclusivity of public decision making, and when their use perpetuates social or economic inequality. To address these challenges, the public must be able to participate in discourse about the implications of algorithmic systems. I propose a series of participatory installations exploring the impacts of algorithmic systems, providing contexts for active exploration of these concerns. I will conduct phenomenographic interviews to better understand how visitors experience art installations about technical topics, providing insight for subsequent installations. I will consolidate the results into a set of best practices about engaging the public on these topics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Fine Arts</span><span class="ui brown basic label">Physical Artifact</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Social Impact Of Technology</span><span class="ui brown basic label">Education</span><span class="ui brown basic label">Participatory Art Installation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kathryn Blair<!-- -->, <!-- -->Pil Hansen<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Participatory Art for Public Exploration of Algorithmic Decision-Making</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;21)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->4<!-- -->.  DOI: <a href="10.1145/3461778.3462068" target="_blank">10.1145/3461778.3462068</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2021-wannamaker" class="ui large modal"><div class="header"><a href="/publications/dis-2021-wannamaker" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2021-wannamaker</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2021-wannamaker.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2021-wannamaker" target="_blank">I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking</a></h1><p class="meta"><a href="/people/kendra-wannamaker"><img src="/static/images/people/kendra-wannamaker.jpg" class="ui circular spaced image mini-profile"/><strong>Kendra Wannamaker</strong></a> , <span>Sandeep Kollannur</span> , <span>Marian Dörk</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/dis-2021-wannamaker.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>dis-2021-wannamaker.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present I/O Bits, a prototype personal informatics system that explores the potential for user-driven and situated self-tracking. With simple tactile inputs and small e-paper visualizations, I/O Bits are dedicated physical devices that allow individuals to track and visualize different kinds of personal activities in-situ. This is in contrast to most self-tracking systems, which automate data collection, centralize information displays, or integrate into multi-purpose devices like smartwatches or mobile phones. We report findings from an e-paper visualization workshop and a prototype deployment where participants constructed their own I/O Bits and used them to track a range of personal data. Based on these experiences, we contribute insights and opportunities for situated and user-driven personal informatics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Personal Informatics</span><span class="ui brown basic label">Situated Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kendra Wannamaker<!-- -->, <!-- -->Sandeep Kollannur<!-- -->, <!-- -->Marian Dörk<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;21)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="http://hdl.handle.net/1880/113555" target="_blank">http://hdl.handle.net/1880/113555</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/yhMKURtgFZ0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/yhMKURtgFZ0?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/yhMKURtgFZ0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2021-danyluk" class="ui large modal"><div class="header"><a href="/publications/chi-2021-danyluk" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2021-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2021-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2021-danyluk" target="_blank">A Design Space Exploration of Worlds in Miniature</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img src="/static/images/people/kurtis-danyluk.jpg" class="ui circular spaced image mini-profile"/><strong>Kurtis Danyluk</strong></a> , <span>Barrett Ens</span> , <span>Bernhard Jenny</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/chi-2021-danyluk.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2021-danyluk.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Worlds-in-Miniature (WiMs) are interactive worlds within a world and combine the advantages of an input space, a cartographicmap, and an overview+detail interface. They have been used across the extended virtuality spectrum for a variety of applications.Building on an analysis of examples of WiMs from the research literature we contribute a design space for WiMs based on sevendesign dimensions. Further, we expand upon existing definitions of WiMs to provide a definition that applies across the extendedreality spectrum. We identify the design dimensions of size-scope-scale, abstraction, geometry, reference frame, links, multiples, andvirtuality. Using our framework we describe existing Worlds-in-Miniature from the research literature and reveal unexplored researchareas. Finally, we generate new examples of WiMs using our framework to fill some of these gaps. With our findings, we identifyopportunities that can guide future research into WiMs.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Augmented Reality</span><span class="ui brown basic label">Meta Analysis Literature Survey</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Danyluk<!-- -->, <!-- -->Barrett Ens<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>A Design Space Exploration of Worlds in Miniature</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;21)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->20<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2021-ens" class="ui large modal"><div class="header"><a href="/publications/chi-2021-ens" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2021-ens</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2021-ens.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2021-ens" target="_blank">Grand Challenges in Immersive Analytics</a></h1><p class="meta"><span>Barrett Ens</span> , <span>Benjamin Bach</span> , <span>Maxime Cordeil</span> , <span>Ulrich Engelke</span> , <span>Marcos Serrano</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <span>Arnaud Prouzeau</span> , <span>Christoph Anthes</span> , <span>Wolfgang Büschel</span> , <span>Cody Dunne</span> , <span>Tim Dwyer</span> , <span>Jens Grubert</span> , <span>Jason H. Haga</span> , <span>Nurit Kishenbaum</span> , <span>Dylan Kobayashi</span> , <span>Tica Lin</span> , <span>Monsurat Olaosebikan</span> , <span>Fabian Pointecker</span> , <span>David Saffo</span> , <span>Nazmus Saquib</span> , <span>Dieter Schmalsteig</span> , <span>Danielle Albers Szafir</span> , <span>Matthew Whitlock</span> , <span>Yalong Yang</span></p><p><a href="/static/publications/chi-2021-ens.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2021-ens.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Immersive Analytics</span><span class="ui brown basic label">Grand Research Challenges</span><span class="ui brown basic label">Data Visualisation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Virtual Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Barrett Ens<!-- -->, <!-- -->Benjamin Bach<!-- -->, <!-- -->Maxime Cordeil<!-- -->, <!-- -->Ulrich Engelke<!-- -->, <!-- -->Marcos Serrano<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Arnaud Prouzeau<!-- -->, <!-- -->Christoph Anthes<!-- -->, <!-- -->Wolfgang Büschel<!-- -->, <!-- -->Cody Dunne<!-- -->, <!-- -->Tim Dwyer<!-- -->, <!-- -->Jens Grubert<!-- -->, <!-- -->Jason H. Haga<!-- -->, <!-- -->Nurit Kishenbaum<!-- -->, <!-- -->Dylan Kobayashi<!-- -->, <!-- -->Tica Lin<!-- -->, <!-- -->Monsurat Olaosebikan<!-- -->, <!-- -->Fabian Pointecker<!-- -->, <!-- -->David Saffo<!-- -->, <!-- -->Nazmus Saquib<!-- -->, <!-- -->Dieter Schmalsteig<!-- -->, <!-- -->Danielle Albers Szafir<!-- -->, <!-- -->Matthew Whitlock<!-- -->, <!-- -->Yalong Yang<!-- -->. <b>Grand Challenges in Immersive Analytics</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;21)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->17<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2021-hammad" class="ui large modal"><div class="header"><a href="/publications/chi-2021-hammad" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2021-hammad</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2021-hammad.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2021-hammad" target="_blank">Homecoming: Exploring Returns to Long-Term Single Player Games</a></h1><p class="meta"><span>Noor Hammad</span> , <span>Owen Brierley</span> , <a href="/people/zachary-mckendrick"><img src="/static/images/people/zachary-mckendrick.jpg" class="ui circular spaced image mini-profile"/><strong>Zachary McKendrick</strong></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <span>Patrick Finn</span> , <span>Jessica Hammer</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p><p><a href="/static/publications/chi-2021-hammad.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2021-hammad.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present an autobiographical design journey exploring the experience of returning to long-term single player games. Continuing progress from a previously saved game, particularly when substantial time has passed, is an understudied area in games research. To begin our exploration in this domain, we investigated what the return experience is like first-hand. By returning to four long-term single player games played extensively in the past, we revealed a phenomenon we call The Pivot Point, a ‘eureka’ moment in return gameplay. The pivot point anchors our design explorations, where we created prototypes to leverage the pivot point in reconnecting with the experience. These return experiences and subsequent prototyping iterations inform our understanding of how to design better returns to gameplay, which can benefit both producers and consumers of long-term single player games.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Long Term Single Player Game</span><span class="ui brown basic label">Autobiographical Design</span><span class="ui brown basic label">Pivot Point</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Noor Hammad<!-- -->, <!-- -->Owen Brierley<!-- -->, <!-- -->Zachary McKendrick<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Jessica Hammer<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Homecoming: Exploring Returns to Long-Term Single Player Games</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;21)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3411764.3445357" target="_blank">https://doi.org/10.1145/3411764.3445357</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="gi-2021-mactavish" class="ui large modal"><div class="header"><a href="/publications/gi-2021-mactavish" target="_blank"><i class="fas fa-link fa-fw"></i>gi-2021-mactavish</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">GI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/gi-2021-mactavish.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/gi-2021-mactavish" target="_blank">Perspective Charts</a></h1><p class="meta"><span>Mia MacTavish</span> , <span>Katayoon Etemad</span> , <span>Faramarz Samavati</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/gi-2021-mactavish.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>gi-2021-mactavish.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Sp4Vt8mMhCs&amp;list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Sp4Vt8mMhCs&amp;list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/Sp4Vt8mMhCs&amp;list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce three novel data visualizations, called perspective charts, based on the concept of size constancy in linear perspective projection. Bar charts are a popular and commonly used tool for the interpretation of datasets, however, representing datasets with multi-scale variation is challenging in a bar chart due to limitations in viewing space. Each of our designs focuses on the static representation of datasets with large ranges with respect to important variations in the data. Through a user study, we measure the effectiveness of our designs for representing these datasets in comparison to traditional methods, such as a standard bar chart or a broken-axis bar chart, and state-of-the-art methods, such as a scale-stack bar chart. The evaluation reveals that our designs allow pieces of data to be visually compared at a level of accuracy similar to traditional visualizations. Our designs demonstrate advantages when compared to state-of-the-art visualizations designed to represent datasets with large outliers.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mia MacTavish<!-- -->, <!-- -->Katayoon Etemad<!-- -->, <!-- -->Faramarz Samavati<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Perspective Charts</b>. <i>In undefined (GI &#x27;21)</i>. <!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="http://hdl.handle.net/1880/113671" target="_blank">http://hdl.handle.net/1880/113671</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="ijac-2021-hosseini" class="ui large modal"><div class="header"><a href="/publications/ijac-2021-hosseini" target="_blank"><i class="fas fa-link fa-fw"></i>ijac-2021-hosseini</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IJAC 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/ijac-2021-hosseini.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/ijac-2021-hosseini" target="_blank">Optically illusive architecture (OIA): Introduction and evaluation using virtual reality</a></h1><p class="meta"><span>Seyed Vahab Hosseini</span> , <span>Usman R Alim</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Joshua M Taron</span></p><p><a href="/static/publications/ijac-2021-hosseini.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>ijac-2021-hosseini.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Architects and designers communicate their ideas within a range of representational methods. No single instance of these methods, either in the form of orthographic projections or perspectival representation, can address all questions regarding the design, but as a whole, they demonstrate a comprehensive range of information about the building or object they intend to represent. This explicates an inevitable degree of deficiency in representation, regardless of its type. In addition, perspective-based optical illusions manipulate our spatial perception by deliberately misrepresenting the reality. In this regard, they are not new concepts to architectural representation. As a consequence, Optically Illusive Architecture (OIA) is proposed, not as a solution to fill the gap between the representing and represented spaces, but as a design paradigm whose concept derives from and accounts for this gap. By OIA we aim to cast light to an undeniable role of viewpoints in designing architectural spaces. The idea is to establish a methodology in a way that the deficiency of current representational techniques—manifested as specific thread of optical illusions—flourishes into thoughtful results embodied as actual architectural spaces. Within our design paradigm, we define a framework to be able to effectively analyze its precedents, generate new space, and evaluate their efficiencies. Moreover, the framework raises a hierarchical set of questions to differentiate OIA from a visual gimmick. Furthermore, we study two OIA-driven environments, by conducting empirical studies using Virtual Reality (VR). These studies bear essential information, in terms of design performance, and the public’s ability to engage and interact with an OIA space, prior to the actual fabrication of the structures.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Architectural Representation</span><span class="ui brown basic label">Optical Illusion</span><span class="ui brown basic label">Design Evaluation</span><span class="ui brown basic label">Virtual Reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Seyed Vahab Hosseini<!-- -->, <!-- -->Usman R Alim<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Joshua M Taron<!-- -->. <b>Optically illusive architecture (OIA): Introduction and evaluation using virtual reality</b>. <i>In undefined (IJAC &#x27;21)</i>. <!-- -->  Page: 1-<!-- -->24<!-- -->.  DOI: <a href="10.1177/14780771211016600" target="_blank">10.1177/14780771211016600</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="cupum-2021-rout" class="ui large modal"><div class="header"><a href="/publications/cupum-2021-rout" target="_blank"><i class="fas fa-link fa-fw"></i>cupum-2021-rout</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">Urban Informatics and Future Cities</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/cupum-2021-rout.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/cupum-2021-rout" target="_blank">(Big) Data in Urban Design Practice: Supporting High-Level Design Tasks Using a Visualization of Human Movement Data from Smartphones</a></h1><p class="meta"><span>Angela Rout</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/cupum-2021-rout.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>cupum-2021-rout.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Me8cU6RoCiA" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Me8cU6RoCiA?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/Me8cU6RoCiA/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present the SmartCampus visualization tool, representing spatiotemporal data of over 200 student pathways and restpoints on a university campus. Based on our experiences with SmartCampus, we also propose a task-based framework that de-scribes how practicing urban designers (specifically, architects) can use human movement data visualizations in their work. Although extensive amounts of location data are produced daily by smartphones, existing geospatial tools are not customized to specifically support high-level urban design tasks. To help identify opportunities in urban design for visualizing human movement data from devices such as smartphones, we used our SmartCampus prototype to facilitate a series of 3 participatory design sessions (3 participants), a targeted online survey (14 participants), and semi-structured interviews (6 participants) with architectural experts. Our findings showcase the need for location analysis tools tailored to concrete urban design practices, and also highlight opportunities for Smart City researchers interested in developing domain specific, visualization tools.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Smartphone Data</span><span class="ui brown basic label">GPS</span><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Architecture</span><span class="ui brown basic label">Urban Design</span><span class="ui brown basic label">Task Based Framework</span><span class="ui brown basic label">High Level Tasks</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Angela Rout<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>(Big) Data in Urban Design Practice: Supporting High-Level Design Tasks Using a Visualization of Human Movement Data from Smartphones</b>. <i>In undefined (Urban Informatics and Future C &#x27;es)</i>. <!-- -->  Page: 1-<!-- -->301-318<!-- -->.  DOI: <a href="http://hdl.handle.net/1880/113114" target="_blank">http://hdl.handle.net/1880/113114</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2021-pratte" class="ui large modal"><div class="header"><a href="/publications/tei-2021-pratte" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2021-pratte</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2021-pratte.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2021-pratte" target="_blank">Evoking Empathy: A Framework for Describing Empathy Tools</a></h1><p class="meta"><a href="/people/sydney-pratte"><img src="/static/images/people/sydney-pratte.jpg" class="ui circular spaced image mini-profile"/><strong>Sydney Pratte</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p><p><a href="/static/publications/tei-2021-pratte.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>tei-2021-pratte.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/JBCzPt5ILxo" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/JBCzPt5ILxo?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/JBCzPt5ILxo/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Empathy tools are experiences designed to evoke empathetic responses by placing the user in another’s lived and felt experience. The problem is that designers do not have a common vocabulary to describe empathy tool experiences; consequently, it is difficult to compare/contrast empathy tool designs or to think about their efficacy. To address this problem, we analyzed 26 publications on empathy tools to develop a descriptive framework for designers of empathy tools. Based on our analysis, we found that empathy tools can be described along three dimensions: (i) the amount of agency the tool allows, (ii) the user’s perspective while using the tool, and (iii) the type of sensations that are experienced. We show that this framework can be used to describe a wide variety of empathy tools and provide recommendations for empathy tool designers, as well as techniques for measuring the efficacy of an empathy tool experience.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Empathy</span><span class="ui brown basic label">Empathy Tools</span><span class="ui brown basic label">Design Strategies</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sydney Pratte<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Evoking Empathy: A Framework for Describing Empathy Tools</b>. <i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;21)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://dl.acm.org/doi/10.1145/3430524.3440644" target="_blank">https://dl.acm.org/doi/10.1145/3430524.3440644</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2020-suzuki" class="ui large modal"><div class="header"><a href="/publications/uist-2020-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2020-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2020-suzuki" target="_blank">RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Rubaiat Habib Kazi</span> , <span>Li-Yi Wei</span> , <span>Stephen DiVerdi</span> , <span>Wilmot Li</span> , <span>Daniel Leithinger</span></p><p><a href="/static/publications/uist-2020-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2020-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/L0p-BNU9rXU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/L0p-BNU9rXU?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/L0p-BNU9rXU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Real Time Authoring</span><span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Tangible Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Li-Yi Wei<!-- -->, <!-- -->Stephen DiVerdi<!-- -->, <!-- -->Wilmot Li<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3379337.3415892" target="_blank">https://doi.org/10.1145/3379337.3415892</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2020-yixian" class="ui large modal"><div class="header"><a href="/publications/uist-2020-yixian" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2020-yixian</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-yixian.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2020-yixian" target="_blank">ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World</a></h1><p class="meta"><span>Yan Yixian</span> , <span>Kazuki Takashima</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>Takayuki Tanno</span> , <span>Kazuyuki Fujita</span> , <span>Yoshifumi Kitamura</span></p><p><a href="/static/publications/uist-2020-yixian.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2020-yixian.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/j2iSNDkBxAY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/j2iSNDkBxAY?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/j2iSNDkBxAY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls---autonomous robotic encounter-type haptic wall-shaped props---that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user&#x27;s movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of autonomous mobile reusable props, which we view as an important design direction for haptics in VR.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Encountered Type Haptic Devices</span><span class="ui brown basic label">Immersive Experience</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Yan Yixian<!-- -->, <!-- -->Kazuki Takashima<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Takayuki Tanno<!-- -->, <!-- -->Kazuyuki Fujita<!-- -->, <!-- -->Yoshifumi Kitamura<!-- -->. <b>ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3379337.3415859" target="_blank">https://doi.org/10.1145/3379337.3415859</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="iros-2020-hedayati" class="ui large modal"><div class="header"><a href="/publications/iros-2020-hedayati" target="_blank"><i class="fas fa-link fa-fw"></i>iros-2020-hedayati</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IROS 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/iros-2020-hedayati.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/iros-2020-hedayati" target="_blank">PufferBot: Actuated Expandable Structures for Aerial Robots</a></h1><p class="meta"><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><p><a href="/static/publications/iros-2020-hedayati.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>iros-2020-hedayati.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/XtPepCxWcBg" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/XtPepCxWcBg?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/XtPepCxWcBg/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drone’s propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique charac- terizations which may be useful in different situations. Finally, we present three motivating scenarios in which PufferBot might be useful beyond existing static propeller guard structures.</p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hooman Hedayati<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Daniel Szafir<!-- -->. <b>PufferBot: Actuated Expandable Structures for Aerial Robots</b>. <i>In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS &#x27;20)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->6<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2020-danyluk" class="ui large modal"><div class="header"><a href="/publications/tvcg-2020-danyluk" target="_blank"><i class="fas fa-link fa-fw"></i>tvcg-2020-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TVCG 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2020-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2020-danyluk" target="_blank">Touch and Beyond: Comparing Physical and Virtual Reality Visualizations</a></h1><p class="meta"><span>Kurtis Thorvald Danyluk</span> , <span>Teoman Tomo Ulusoy</span> , <a href="/people/wei-wei"><img src="/static/images/people/wei-wei.jpg" class="ui circular spaced image mini-profile"/><strong>Wei Wei</strong></a> , <span>Wesley J. Willett</span></p><p><a href="/static/publications/tvcg-2020-danyluk.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>tvcg-2020-danyluk.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We compare physical and virtual reality (VR) versions of simple data visualizations and explore how the addition of virtual annotation and filtering tools affects how viewers solve basic data analysis tasks. We report on two studies, inspired by previous examinations of data physicalizations. The first study examines differences in how viewers interact with physical hand-scale, virtual hand-scale, and virtual table-scale visualizations and the impact that the different forms had on viewer’s problem solving behavior. A second study examines how interactive annotation and filtering tools might support new modes of use that transcend the limitations of physical representations. Our results highlight challenges associated with virtual reality representations and hint at the potential of interactive annotation and filtering tools in VR visualizations.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Human Computer Interaction</span><span class="ui brown basic label">Visualization</span><span class="ui brown basic label">Data Visualization</span><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Physicalization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Thorvald Danyluk<!-- -->, <!-- -->Teoman Tomo Ulusoy<!-- -->, <!-- -->Wei Wei<!-- -->, <!-- -->Wesley J. Willett<!-- -->. <b>Touch and Beyond: Comparing Physical and Virtual Reality Visualizations</b>. <i>In IEEE Transactions on Visualization and Computer Graphics (TVCG &#x27;20)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="http://dx.doi.org/10.1109/TVCG.2020.3023336" target="_blank">http://dx.doi.org/10.1109/TVCG.2020.3023336</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="nime-2020-ko" class="ui large modal"><div class="header"><a href="/publications/nime-2020-ko" target="_blank"><i class="fas fa-link fa-fw"></i>nime-2020-ko</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">NIME 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/nime-2020-ko.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/nime-2020-ko" target="_blank">Touch Responsive Augmented Violin Interface System II: Integrating Sensors into a 3D Printed Fingerboard</a></h1><p class="meta"><span>Chantelle Ko</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p><p><a href="/static/publications/nime-2020-ko.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>nime-2020-ko.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present TRAVIS II, an augmented acoustic violin with touch sensors integrated into its 3D printed fingerboard that track left-hand finger gestures in real time. The fingerboard has four strips of conductive PLA filament which produce an electric signal when fingers press down on each string. While these sensors are physically robust, they are mechanically assembled and thus easy to replace if damaged. The performer can also trigger presets via four FSRs attached to the body of the violin. The instrument is completely wireless, giving the performer the freedom to move throughout the performance space. While the sensing fingerboard is installed in place of the traditional fingerboard, all other electronics can be removed from the augmented instrument, maintaining the aesthetics of a traditional violin. Our design allows violinists to naturally create music for interactive performance and improvisation without requiring new instrumental techniques. In this paper, we describe the design of the instrument, experiments leading to the sensing fingerboard, and performative applications of the instrument.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Violin</span><span class="ui brown basic label">Touch Sensor</span><span class="ui brown basic label">FSR</span><span class="ui brown basic label">Fingerboard</span><span class="ui brown basic label">Augmented</span><span class="ui brown basic label">3 D Printing</span><span class="ui brown basic label">Conductive Filament</span><span class="ui brown basic label">Interactive</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Chantelle Ko<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Touch Responsive Augmented Violin Interface System II: Integrating Sensors into a 3D Printed Fingerboard</b>. <i>In undefined (NIME &#x27;20)</i>. <!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a target="_blank"></a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/INmDzkcIO14" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/INmDzkcIO14?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/INmDzkcIO14/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="imwut-2020-wang" class="ui large modal"><div class="header"><a href="/publications/imwut-2020-wang" target="_blank"><i class="fas fa-link fa-fw"></i>imwut-2020-wang</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IMWUT 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/imwut-2020-wang.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/imwut-2020-wang" target="_blank">AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</a></h1><p class="meta"><span>Xiyue Wang</span> , <span>Kazuki Takashima</span> , <span>Tomoaki Adachi</span> , <span>Patrick Finn</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <span>Yoshifumi Kitamura</span></p><p><a href="/static/publications/imwut-2020-wang.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>imwut-2020-wang.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/fxxvZBY80ug" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/fxxvZBY80ug?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/fxxvZBY80ug/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Natural disasters cause long-lasting mental health problems such as PTSD in children. Following the 2011 Earthquake and Tsunami in Japan, we witnessed a shift of toy block play behavior in young children who suffered from stress after the disaster. The behavior reflected their emotional responses to the traumatic event. In this paper, we explore the feasibility of using data captured from block-play to assess children&#x27;s stress after a major natural disaster. We prototyped sets of sensor-embedded toy blocks, AssessBlocks, that automate quantitative play data acquisition. During a three-year period, the blocks were dispatched to fifty-two post-disaster children. Within a free play session, we captured block features, a child&#x27;s playing behavior, and stress evaluated by several methods. The result from our analysis reveal correlations between block play features and stress measurements and show initial promise of using the effectiveness of using AssessBlocks to assess children&#x27;s stress after a disaster. We provide detailed insights into the potential as well as the challenges of our approach and unique conditions. From these insights we summarize guidelines for future research in automated play assessment systems that support children&#x27;s mental health.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Well Being</span><span class="ui brown basic label">Toy Blocks</span><span class="ui brown basic label">PTSD</span><span class="ui brown basic label">Tangibles For Health</span><span class="ui brown basic label">Stress Assessment</span><span class="ui brown basic label">Play</span><span class="ui brown basic label">Children</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Xiyue Wang<!-- -->, <!-- -->Kazuki Takashima<!-- -->, <!-- -->Tomoaki Adachi<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Yoshifumi Kitamura<!-- -->. <b>AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</b>. <i>In Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->29<!-- -->.  DOI: <a href="https://doi.org/10.1145/3381016" target="_blank">https://doi.org/10.1145/3381016</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="imx-2020-mok" class="ui large modal"><div class="header"><a href="/publications/imx-2020-mok" target="_blank"><i class="fas fa-link fa-fw"></i>imx-2020-mok</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IMX 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/imx-2020-mok.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/imx-2020-mok" target="_blank">Talk Like Somebody is Watching: Understanding and Supporting Novice Live Streamers</a></h1><p class="meta"><a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><strong>Terrance Mok</strong></a> , <a href="/people/colin-auyeung"><img src="/static/images/people/colin-auyeung.jpg" class="ui circular spaced image mini-profile"/><strong>Colin Au Yeung</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p><p><a href="/static/publications/imx-2020-mok.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>imx-2020-mok.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We built a chatbot system–Audience Bot–that simulates an audience for novice live streamers to engage with while streaming. New live streamers on platforms like Twitch are expected to perform and talk to themselves, even while no one is watching. We ran an observational lab study on how Audience Bot assists novice live streamers as they acclimate to multitasking–simultaneously playing a video game while performing for a (simulated) audience.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Live Streams</span><span class="ui brown basic label">Streamers</span><span class="ui brown basic label">Chatbot</span><span class="ui brown basic label">Audience</span><span class="ui brown basic label">Virtual Audience</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Terrance Mok<!-- -->, <!-- -->Colin Au Yeung<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Talk Like Somebody is Watching: Understanding and Supporting Novice Live Streamers</b>. <i>In undefined (IMX &#x27;20)</i>. <!-- -->  Page: 1-<!-- -->6<!-- -->.  DOI: <a href="10.1145/3391614.3399392" target="_blank">10.1145/3391614.3399392</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-suzuki" class="ui large modal"><div class="header"><a href="/publications/chi-2020-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-suzuki" target="_blank">RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Hooman Hedayati</span> , <span>Clement Zheng</span> , <span>James Bohn</span> , <span>Daniel Szafir</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D Gross</span> , <span>Daniel Leithinger</span></p><p><a href="/static/publications/chi-2020-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2020-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/4OWU60gTOFE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/4OWU60gTOFE?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/4OWU60gTOFE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Room Scale Haptics</span><span class="ui brown basic label">Haptic Interfaces</span><span class="ui brown basic label">Swarm Robots</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Hooman Hedayati<!-- -->, <!-- -->Clement Zheng<!-- -->, <!-- -->James Bohn<!-- -->, <!-- -->Daniel Szafir<!-- -->, <!-- -->Ellen Yi-Luen Do<!-- -->, <!-- -->Mark D Gross<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->11<!-- -->.  DOI: <a href="https://doi.org/10.1145/3313831.3376523" target="_blank">https://doi.org/10.1145/3313831.3376523</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-anjani" class="ui large modal"><div class="header"><a href="/publications/chi-2020-anjani" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-anjani</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-anjani" target="_blank">Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</a></h1><p class="meta"><span>Laurensia Anjani</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><strong>Terrance Mok</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Wooi Boon Goh</span></p><p><a href="/static/publications/chi-2020-anjani.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2020-anjani.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present a mixed-methods study of viewers on their practices and motivations around watching mukbang — video streams of people eating large quantities of food. Viewers&#x27; experiences provide insight on future technologies for multisensorial video streams and technology-supported commensality (eating with others). We surveyed 104 viewers and interviewed 15 of them about their attitudes and reflections on their mukbang viewing habits, their physiological aspects of watching someone eat, and their perceived social relationship with mukbangers. Based on our findings, we propose design implications for remote commensality, and for synchronized multisensorial video streaming content.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Video Streams</span><span class="ui brown basic label">Mukbang</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Laurensia Anjani<!-- -->, <!-- -->Terrance Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Wooi Boon Goh<!-- -->. <b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3313831.3376567" target="_blank">https://doi.org/10.1145/3313831.3376567</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Dkp8A_em90M" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Dkp8A_em90M?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/Dkp8A_em90M/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-asha" class="ui large modal"><div class="header"><a href="/publications/chi-2020-asha" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-asha</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020 LBW</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-asha.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-asha" target="_blank">Views from the Wheelchair: Understanding Interaction between Autonomous Vehicle and Pedestrians with Reduced Mobility</a></h1><p class="meta"><a href="/people/ashratuz-zavin-asha"><img src="/static/images/people/ashratuz-zavin-asha.jpg" class="ui circular spaced image mini-profile"/><strong>Ashratuz Zavin Asha</strong></a> , <a href="/people/christopher-smith"><img src="/static/images/people/christopher-smith.jpg" class="ui circular spaced image mini-profile"/><strong>Christopher Smith</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p><p><a href="/static/publications/chi-2020-asha.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2020-asha.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/JNc49desa44" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/JNc49desa44?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/JNc49desa44/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We are interested in the ways pedestrians will interact with autonomous vehicle (AV) in a future AV transportation ecosystem, when nonverbal cues from the driver such as eye movements, hand gestures, etc. are no longer provided. In this work, we examine a subset of this challenge: interaction between pedestrian with reduced mobility (PRM) and AV. This study explores interface designs between AVs and people in a wheelchair to help them interact with AVs by conducting a preliminary design study. We have assessed the data collected from the study using qualitative analysis and presented different findings on AV-PRM interactions. Our findings reflect on the importance of visual interfaces, changes to the wheelchair and the creative use of the street infrastructure.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Autonomous Vehicle</span><span class="ui brown basic label">Pedestrian With Reduced Mobility</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ashratuz Zavin Asha<!-- -->, <!-- -->Christopher Smith<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Views from the Wheelchair: Understanding Interaction between Autonomous Vehicle and Pedestrians with Reduced Mobility</b>. <i>In undefined (CHI 202 &#x27;BW)</i>. <!-- -->  Page: 1-<!-- -->8<!-- -->.  DOI: <a href="10.1145/3334480.3383041" target="_blank">10.1145/3334480.3383041</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-goffin" class="ui large modal"><div class="header"><a href="/publications/chi-2020-goffin" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-goffin</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-goffin.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-goffin" target="_blank">Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</a></h1><p class="meta"><span>Pascal Goffin</span> , <span>Tanja Blascheck</span> , <span>Petra Isenberg</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/chi-2020-goffin.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2020-goffin.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/wPaVdSWM8hU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/wPaVdSWM8hU?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/wPaVdSWM8hU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We describe a design space of view manipulation interactions for small data-driven contextual visualizations (word-scale visualizations). These interaction techniques support an active reading experience and engage readers through exploration of embedded visualizations whose placement and content connect them to specific terms in a document. A reader could, for example, use our proposed interaction techniques to explore word-scale visualizations of stock market trends for companies listed in a market overview article. When readers wish to engage more deeply with the data, they can collect, arrange, compare, and navigate the document using the embedded word-scale visualizations, permitting more visualization-centric analyses. We support our design space with a concrete implementation, illustrate it with examples from three application domains, and report results from two experiments. The experiments show how view manipulation interactions helped readers examine embedded visualizations more quickly and with less scrolling and yielded qualitative feedback on usability and future opportunities.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Glyphs</span><span class="ui brown basic label">Word Scale Visualization</span><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Text Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Pascal Goffin<!-- -->, <!-- -->Tanja Blascheck<!-- -->, <!-- -->Petra Isenberg<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3313831.3376842" target="_blank">https://doi.org/10.1145/3313831.3376842</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-hou" class="ui large modal"><div class="header"><a href="/publications/chi-2020-hou" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-hou</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-hou" target="_blank">Autonomous Vehicle-Cyclist Interaction: Peril and Promise</a></h1><p class="meta"><span>Ming Hou</span> , <a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><strong>Karthik Mahadevan</strong></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p><p><a href="/static/publications/chi-2020-hou.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2020-hou.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/fsgbUeAaFfI" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/fsgbUeAaFfI?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/fsgbUeAaFfI/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Autonomous vehicles (AVs) will redefine interactions between road users. Presently, cyclists and drivers communicate through implicit cues (vehicle motion) and explicit but imprecise signals (hand gestures, horns). Future AVs could consistently communicate awareness and intent and other feedback to cyclists based on their sensor data. We present an exploration of AV-cyclist interaction, starting with preliminary design studies which informed the implementation of an immersive VR AV-cyclist simulator, and the design and evaluation of a number of AV-cyclist interfaces. Our findings suggest that AV-cyclist interfaces can improve rider confidence in lane merging scenarios. We contribute an AV-cyclist immersive simulator, insights on trade-offs of various aspects of AV-cyclist interaction design including modalities, location, and complexity, and positive results suggesting improved rider confidence due to AV-cyclist interaction. While we are encouraged by the potential positive impact AV-cyclist interfaces can have on cyclist culture, we also emphasize the risks over-reliance can pose to cyclists.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Autonomous Vehicle Cyclist Interaction</span><span class="ui brown basic label">Interfaces For Communicating Intent And Awareness</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ming Hou<!-- -->, <!-- -->Karthik Mahadevan<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3313831.3376884" target="_blank">https://doi.org/10.1145/3313831.3376884</a></p></div></div><div class="block"><h1>Talk</h1><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/DtxkWAW9B1s" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/DtxkWAW9B1s?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/DtxkWAW9B1s/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2020-suzuki" class="ui large modal"><div class="header"><a href="/publications/tei-2020-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2020-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2020-suzuki" target="_blank">LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Ryosuke Nakayama</span> , <span>Dan Liu</span> , <span>Yasuaki Kakehi</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p><p><a href="/static/publications/tei-2020-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>tei-2020-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/0LHeTkOMR84" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/0LHeTkOMR84?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/0LHeTkOMR84/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore interactions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust, making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various applications. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Shape Changing Interfaces</span><span class="ui brown basic label">Inflatables</span><span class="ui brown basic label">Large Scale Interactions</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Ryosuke Nakayama<!-- -->, <!-- -->Dan Liu<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces</b>. <i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->9<!-- -->.  DOI: <a href="https://dl.acm.org/doi/10.1145/3374920.3374941" target="_blank">https://dl.acm.org/doi/10.1145/3374920.3374941</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div><div class="ui vertical segment stackable" style="text-align:center"><a class="ui button" href="/publications">+ 30 more publications</a></div></div><div id="people" class="category"><h1 class="ui horizontal divider header"><i class="child icon"></i>Students</h1><div class="people-category"><h2>PhD Students</h2><div class="ui grid"><a class="four wide column person" href="/people/ashratuz-zavin-asha"><img class="ui circular image medium-profile" src="/static/images/people/ashratuz-zavin-asha.jpg"/><p><b>Ashratuz Zavin Asha</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/georgina-freeman"><img class="ui circular image medium-profile" src="/static/images/people/georgina-freeman.jpg"/><p><b>Georgina Freeman</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/kathryn-blair"><img class="ui circular image medium-profile" src="/static/images/people/kathryn-blair.jpg"/><p><b>Kathryn Blair</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/kurtis-danyluk"><img class="ui circular image medium-profile" src="/static/images/people/kurtis-danyluk.jpg"/><p><b>Kurtis Danyluk</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/roberta-cabral-mota"><img class="ui circular image medium-profile" src="/static/images/people/roberta-cabral-mota.jpg"/><p><b>Roberta Cabral Mota</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/sydney-pratte"><img class="ui circular image medium-profile" src="/static/images/people/sydney-pratte.jpg"/><p><b>Sydney Pratte</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/terrance-mok"><img class="ui circular image medium-profile" src="/static/images/people/terrance-mok.jpg"/><p><b>Terrance Mok</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/tim-au-yeung"><img class="ui circular image medium-profile" src="/static/images/people/tim-au-yeung.jpg"/><p><b>Tim Au Yeung</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/zachary-mckendrick"><img class="ui circular image medium-profile" src="/static/images/people/zachary-mckendrick.jpg"/><p><b>Zachary McKendrick</b></p><p>PhD Student</p></a></div></div><div class="people-category"><h2>Masters Students</h2><div class="ui grid"><a class="four wide column person" href="/people/adnan-karim"><img class="ui circular image medium-profile" src="/static/images/people/adnan-karim.jpg"/><p><b>Adnan Karim</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/april-zhang"><img class="ui circular image medium-profile" src="/static/images/people/april-zhang.jpg"/><p><b>April Zhang</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/christopher-smith"><img class="ui circular image medium-profile" src="/static/images/people/christopher-smith.jpg"/><p><b>Christopher Smith</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/colin-auyeung"><img class="ui circular image medium-profile" src="/static/images/people/colin-auyeung.jpg"/><p><b>Colin Au Yeung</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/desmond-larsen-rosner"><img class="ui circular image medium-profile" src="/static/images/people/desmond-larsen-rosner.jpg"/><p><b>Desmond Larsen-Rosner</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/iryna-luchak"><img class="ui circular image medium-profile" src="/static/images/people/iryna-luchak.jpg"/><p><b>Iryna Luchak</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/marcus-friedel"><img class="ui circular image medium-profile" src="/static/images/people/marcus-friedel.jpg"/><p><b>Marcus Friedel</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/neil-chulpongsatorn"><img class="ui circular image medium-profile" src="/static/images/people/neil-chulpongsatorn.jpg"/><p><b>Neil Chulpongsatorn</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/priya-dhawka"><img class="ui circular image medium-profile" src="/static/images/people/priya-dhawka.jpg"/><p><b>Priya Dhawka</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/samin-farajian"><img class="ui circular image medium-profile" src="/static/images/people/samin-farajian.jpg"/><p><b>Samin Farajian</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/shivesh-jadon"><img class="ui circular image medium-profile" src="/static/images/people/shivesh-jadon.jpg"/><p><b>Shivesh Jadon</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/wei-wei"><img class="ui circular image medium-profile" src="/static/images/people/wei-wei.jpg"/><p><b>Wei Wei</b></p><p>MSc Student</p></a></div></div><div class="ui vertical segment stackable" style="text-align:center"><a class="ui button" href="/people">+ see more members</a></div></div><div id="location" class="category"><h1 class="ui horizontal divider header"><i class="map outline icon"></i>Location</h1><div id="map" class="ui grid"><div class="ten wide column"><div class="feature map"><iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d2506.544200524445!2d-114.1279042!3d51.079963549999995!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x53716f0c07993c17%3A0xb8f1352e9e5dfa06!2sMath+Science%2C+Calgary%2C+AB+T2N+4V8%2C+Canada!5e0!3m2!1sen!2sus!4v1439359680603" frameBorder="0" style="border:0"></iframe></div></div><div class="six wide column"><div class="ui segment"><h1>Interactions Lab</h1><p>680 Math Science Building,<br/>University of Calgary<br/>Calgary, AB T2N 4V8, Canada</p></div></div></div><div class="ui vertical segment stackable" style="text-align:center"><a class="ui button" href="/location">+ learn more about our space</a></div></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img style="max-width:180px;margin:30px auto" src="/static/images/logo-6.png"/><div class="content"><img style="max-width:200px;margin:0px auto" src="/static/images/logo-4.png"/><div class="sub header">Department of Computer Science</div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{}},"page":"/","query":{},"buildId":"TtqpnG459e06x8_W6ySjW","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/" src="/_next/static/TtqpnG459e06x8_W6ySjW/pages/index.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/TtqpnG459e06x8_W6ySjW/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.c898a08c8bf8d90e85ef.js" async=""></script><script src="/_next/static/runtime/main-fad3ab09a2039edff31b.js" async=""></script></body></html>