<!DOCTYPE html><html><head><link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.css" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/css/lightbox.css" rel="stylesheet"/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link href="/static/css/style.css" rel="stylesheet"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox-plus-jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              $('.publication').on('click', function(event) {
                if (event.target.className !== 'author-link') {
                  const id = this.dataset.id
                  $('#'+id).modal({
                    onHidden: function() {
                      const html = $(this).html()
                      $(this).html(html)
                    }
                  })
                  .modal('show')
                }
              })
            })
          </script><meta charSet="utf-8" class="next-head"/><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><link rel="preload" href="/_next/static/dc4qxLD17Vjk7Tj2rgurK/pages/person.js" as="script"/><link rel="preload" href="/_next/static/dc4qxLD17Vjk7Tj2rgurK/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.98f1a4e4c94db9943918.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-22d58f57abab872d6e70.js" as="script"/></head><body><div id="__next"><div><title>Lora Oehlberg - Interactions Lab | University of Calgary HCI Group</title><div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/"><b style="color:#00716C">UCalgary iLab</b></a></div><div class="right menu"><a class="item" href="/publications">Publications</a><a class="item active" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/news">News</a><a class="item" href="/location">Location</a></div></div></div><div class="ui stackable grid"><div class="one wide column"></div><div class="eleven wide column centered"><div id="person" class="category" style="text-align:center"><img class="ui circular image large-profile" src="/static/images/people/lora-oehlberg.jpg" style="margin:auto"/><h1>Lora Oehlberg</h1><p>Assistant Professor</p><p><a href="https://pages.cpsc.ucalgary.ca/~lora.oehlberg/" target="_blank"><i class="fas fa-link fa-fw"></i>https://pages.cpsc.ucalgary.ca/~lora.oehlberg/</a></p><p><a href="https://scholar.google.ca/citations?hl=en&amp;user=8GzaBdwAAAAJ" target="_blank"><i class="fas fa-graduation-cap fa-fw"></i>Google Scholar</a></p><div class="ui horizontal small divided link list"></div></div><div id="publications" class="category"><h1 class="ui horizontal divider header"><i class="file alternate outline icon"></i>Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="chi-2020-anjani"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">CHI 2020</span></p><p style="font-size:1.3em;color:#00716C"><b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b></p><p><span>Laurensia Anjani</span> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Wooi Boon Goh</span></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-hou"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">CHI 2020</span></p><p style="font-size:1.3em;color:#00716C"><b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b></p><p><span>Ming Hou</span> , <a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Karthik Mahadevan</span></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="mobilehci-2019-hung"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/mobilehci-2019-hung.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">MobileHCI 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</b></p><p><span>Michael Hung</span> , <a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-ledo"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">DIS 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</b></p><p><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <span>Jo Vermeulen</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Saul Greenberg</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Sebastian Boring</span></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2019-mikalauskas"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-mikalauskas.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">TEI 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</b></p><p><span>Claire Mikalauskas</span> , <span>April Viczko</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="hri-2018-feick"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">HRI 2018</span></p><p style="font-size:1.3em;color:#00716C"><b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b></p><p><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Martin Feick</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>Andr√© Miede</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-dillman"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-dillman.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">CHI 2018</span></p><p style="font-size:1.3em;color:#00716C"><b>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</b></p><p><span>Kody R. Dillman</span> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Alex Mitchell</span></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-ledo"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">CHI 2018</span></p><p style="font-size:1.3em;color:#00716C"><b>Evaluation Strategies for HCI Toolkit Research</b></p><p><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <span>Steven Houben</span> , <span>Jo Vermeulen</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nicolai Marquardt</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Saul Greenberg</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-feick"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">CHI 2018</span><span class="ui big label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p style="font-size:1.3em;color:#00716C"><b>Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</b></p><p><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Martin Feick</span></a> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p></div></div></div><div id="publications-modal"><div id="chi-2020-anjani" class="ui large modal"><div class="header"><a href="/publications/chi-2020-anjani" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-anjani</a>¬†¬†-¬†¬†<a href="/publications/chi-2020-anjani.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-anjani" target="_blank">Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</a></h1><p class="meta"><span>Laurensia Anjani</span> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Wooi Boon Goh</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present a mixed-methods study of viewers on their practices and motivations around watching mukbang ‚Äî video streams of people eating large quantities of food. Viewers&#x27; experiences provide insight on future technologies for multisensorial video streams and technology-supported commensality (eating with others). We surveyed 104 viewers and interviewed 15 of them about their attitudes and reflections on their mukbang viewing habits, their physiological aspects of watching someone eat, and their perceived social relationship with mukbangers. Based on our findings, we propose design implications for remote commensality, and for synchronized multisensorial video streaming content.</p><div class="ui labels">Keywords: ¬†<span class="ui large grey label">video streams</span><span class="ui large grey label">mukbang</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Laurensia Anjani<!-- -->, <!-- -->Terrance Tin Hoi Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Wooi Boon Goh<!-- -->.¬†<b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b>.¬†<i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>.¬†<!-- -->ACM, New York, NY, USA<!-- -->¬† Page: 1-<!-- -->13<!-- -->.¬† DOI: <a href="https://doi.org/10.1145/3313831.3376567" target="_blank">https://doi.org/10.1145/3313831.3376567</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-hou" class="ui large modal"><div class="header"><a href="/publications/chi-2020-hou" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-hou</a>¬†¬†-¬†¬†<a href="/publications/chi-2020-hou.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-hou" target="_blank">Autonomous Vehicle-Cyclist Interaction: Peril and Promise</a></h1><p class="meta"><span>Ming Hou</span> , <a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><strong>Karthik Mahadevan</strong></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p></p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ming Hou<!-- -->, <!-- -->Karthik Mahadevan<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->.¬†<b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b>.¬†<i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>.¬†<!-- -->ACM, New York, NY, USA<!-- -->¬† Page: 1-<!-- -->.¬† DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mobilehci-2019-hung" class="ui large modal"><div class="header"><a href="/publications/mobilehci-2019-hung" target="_blank"><i class="fas fa-link fa-fw"></i>mobilehci-2019-hung</a>¬†¬†-¬†¬†<a href="/publications/mobilehci-2019-hung.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">MobileHCI 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/mobilehci-2019-hung.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/mobilehci-2019-hung" target="_blank">WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</a></h1><p class="meta"><span>Michael Hung</span> , <a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Pen-based input is often treated as auxiliary to mobile devices. We posit that cross-device interactions can inspire and extend the design space of pen-based interactions into new, expressive directions. We realize this through WatchPen, a smartwatch mounted on a passive, capacitive stylus that: (1) senses the usage context and leverages it for expression (e.g., changing colour), (2) contains tools and parameters within the display, and (3) acts as an on-demand output. As a result, it provides users with a dynamic relationship between inputs and outputs, awareness of current tool selection and parameters, and increased expressive match (e.g., added ability to mimic physical tools, showing clipboard contents). We discuss and reflect upon a series of interaction techniques that demonstrate WatchPen within a drawing application. We highlight the expressive power of leveraging multiple sensing and output capabilities across both the watch-augmented stylus and the tablet surface.</p><div class="ui labels">Keywords: ¬†<span class="ui large grey label">smartwatch</span><span class="ui large grey label">cross-device interaction</span><span class="ui large grey label">pen interaction</span><span class="ui large grey label">interaction techniques</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Michael Hung<!-- -->, <!-- -->David Ledo<!-- -->, <!-- -->Lora Oehlberg<!-- -->.¬†<b>WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</b>.¬†<i>In Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &#x27;19)</i>.¬†<!-- -->ACM, New York, NY, USA<!-- -->¬† Page: 1-<!-- -->8<!-- -->.¬† DOI: <a href="https://doi.org/10.1145/3338286.3340122" target="_blank">https://doi.org/10.1145/3338286.3340122</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-ledo" class="ui large modal"><div class="header"><a href="/publications/dis-2019-ledo" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2019-ledo</a>¬†¬†-¬†¬†<a href="/publications/dis-2019-ledo.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-ledo" target="_blank">Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</a></h1><p class="meta"><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <span>Jo Vermeulen</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><strong>Saul Greenberg</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Sebastian Boring</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Astral is a prototyping tool for authoring mobile and smart object interactive behaviours. It mirrors selected display contents of desktop applications onto mobile devices (smartphones and smartwatches), and streams/remaps mobile sensor data to desktop input events (mouse or keyboard) to manipulate selected desktop contents. This allows designers to use familiar desktop applications (e.g. PowerPoint, AfterEffects) to prototype rich interactive behaviours. Astral combines and integrates display mirroring, sensor streaming and input remapping, where designers can exploit familiar desktop applications to prototype, explore and fine-tune dynamic interactive behaviours. With Astral, designers can visually author rules to test real-time behaviours while interactions take place, as well as after the interaction has occurred. We demonstrate Astral&#x27;s applicability, workflow and expressiveness within the interaction design process through both new examples and replication of prior approaches that illustrate how various familiar desktop applications are leveraged and repurposed.</p><div class="ui labels">Keywords: ¬†<span class="ui large grey label">smart objects</span><span class="ui large grey label">mobile interfaces</span><span class="ui large grey label">prototyping</span><span class="ui large grey label">design tool</span><span class="ui large grey label">interactive behaviour</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Saul Greenberg<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Sebastian Boring<!-- -->.¬†<b>Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</b>.¬†<i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;19)</i>.¬†<!-- -->ACM, New York, NY, USA<!-- -->¬† Page: 1-<!-- -->14<!-- -->.¬† DOI: <a href="https://doi.org/10.1145/3322276.3322329" target="_blank">https://doi.org/10.1145/3322276.3322329</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2019-mikalauskas" class="ui large modal"><div class="header"><a href="/publications/tei-2019-mikalauskas" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2019-mikalauskas</a>¬†¬†-¬†¬†<a href="/publications/tei-2019-mikalauskas.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-mikalauskas.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2019-mikalauskas" target="_blank">Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</a></h1><p class="meta"><span>Claire Mikalauskas</span> , <span>April Viczko</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>While improvised theatre (improv) is often performed on a bare stage, improvisers sometimes incorporate physical props to inspire new directions for a scene and to enrich their performance. A tech booth can improvise light and sound technical elements, but coordinating with improvisers&#x27; actions on-stage is challenging. Our goal is to inform the design of an augmented prop that lets improvisers tangibly control light and sound technical elements while performing. We interviewed five professional improvisers about their use of physical props in improv, and their expectations of a possible augmented prop that controls technical theatre elements. We propose a set of guidelines for the design of an augmented prop that fits with the existing world of unpredictable improvised performance.</p><div class="ui labels">Keywords: ¬†<span class="ui large grey label">props</span><span class="ui large grey label">performer-controlled technology</span><span class="ui large grey label">improvisational theatre</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Claire Mikalauskas<!-- -->, <!-- -->April Viczko<!-- -->, <!-- -->Lora Oehlberg<!-- -->.¬†<b>Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</b>.¬†<i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;19)</i>.¬†<!-- -->ACM, New York, NY, USA<!-- -->¬† Page: 1-<!-- -->9<!-- -->.¬† DOI: <a href="https://doi.org/10.1145/3294109.3295631" target="_blank">https://doi.org/10.1145/3294109.3295631</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="hri-2018-feick" class="ui large modal"><div class="header"><a href="/publications/hri-2018-feick" target="_blank"><i class="fas fa-link fa-fw"></i>hri-2018-feick</a>¬†¬†-¬†¬†<a href="/publications/hri-2018-feick.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">HRI 2018</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/hri-2018-feick" target="_blank">The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</a></h1><p class="meta"><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><strong>Martin Feick</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>Andr√© Miede</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we discuss the role of the movement trajectory and velocity enabled by our tele-robotic system (ReMa) for remote collaboration on physical tasks. Our system reproduces changes in object orientation and position at a remote location using a humanoid robotic arm. However, even minor kinematics differences between robot and human arm can result in awkward or exaggerated robot movements. As a result, user communication with the robotic system can become less efficient, less fluent and more time intensive.</p><div class="ui labels">Keywords: ¬†<span class="ui large grey label">movement trajectory &amp; velocity</span><span class="ui large grey label">remote collaboration</span><span class="ui large grey label">robot surrogate</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Andr√© Miede<!-- -->, <!-- -->Ehud Sharlin<!-- -->.¬†<b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b>.¬†<i>In Adjunct Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction (HRI &#x27;18)</i>.¬†<!-- -->ACM, New York, NY, USA<!-- -->¬† Page: 1-<!-- -->2<!-- -->.¬† DOI: <a href="https://doi.org/10.1145/3173386.3176959" target="_blank">https://doi.org/10.1145/3173386.3176959</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-dillman" class="ui large modal"><div class="header"><a href="/publications/chi-2018-dillman" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-dillman</a>¬†¬†-¬†¬†<a href="/publications/chi-2018-dillman.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-dillman.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-dillman" target="_blank">A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</a></h1><p class="meta"><span>Kody R. Dillman</span> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Alex Mitchell</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences.</p><div class="ui labels">Keywords: ¬†<span class="ui large grey label">game design</span><span class="ui large grey label">guidance</span><span class="ui large grey label">interaction cues</span><span class="ui large grey label">augmented reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kody R. Dillman<!-- -->, <!-- -->Terrance Tin Hoi Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Alex Mitchell<!-- -->.¬†<b>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</b>.¬†<i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>.¬†<!-- -->ACM, New York, NY, USA<!-- -->¬† Page: 1-<!-- -->12<!-- -->.¬† DOI: <a href="https://doi.org/10.1145/3173574.3173714" target="_blank">https://doi.org/10.1145/3173574.3173714</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-ledo" class="ui large modal"><div class="header"><a href="/publications/chi-2018-ledo" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-ledo</a>¬†¬†-¬†¬†<a href="/publications/chi-2018-ledo.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-ledo" target="_blank">Evaluation Strategies for HCI Toolkit Research</a></h1><p class="meta"><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <span>Steven Houben</span> , <span>Jo Vermeulen</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><strong>Nicolai Marquardt</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><strong>Saul Greenberg</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/3lAwhCk60C4" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/3lAwhCk60C4?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/3lAwhCk60C4/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what &#x27;evaluating&#x27; a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.</p><div class="ui labels">Keywords: ¬†<span class="ui large grey label">user interfaces</span><span class="ui large grey label">design</span><span class="ui large grey label">evaluation</span><span class="ui large grey label">prototyping</span><span class="ui large grey label">toolkits</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Steven Houben<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Nicolai Marquardt<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Saul Greenberg<!-- -->.¬†<b>Evaluation Strategies for HCI Toolkit Research</b>.¬†<i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>.¬†<!-- -->ACM, New York, NY, USA<!-- -->¬† Page: 1-<!-- -->17<!-- -->.¬† DOI: <a href="https://doi.org/10.1145/3173574.3173610" target="_blank">https://doi.org/10.1145/3173574.3173610</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-feick" class="ui large modal"><div class="header"><a href="/publications/chi-2018-feick" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-feick</a>¬†¬†-¬†¬†<a href="/publications/chi-2018-feick.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-feick.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-feick" target="_blank">Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</a></h1><p class="meta"><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><strong>Martin Feick</strong></a> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/sfxTHsPJWHY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/sfxTHsPJWHY?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/sfxTHsPJWHY/maxresdefault.jpg&gt;&lt;span&gt;‚ñ∂&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Remote collaborators working together on physical objects have difficulty building a shared understanding of what each person is talking about. Conventional video chat systems are insufficient for many situations because they present a single view of the object in a flattened image. To understand how this limited perspective affects collaboration, we designed the Remote Manipulator (ReMa), which can reproduce orientation manipulations on a proxy object at a remote site. We conducted two studies with ReMa, with two main findings. First, a shared perspective is more effective and preferred compared to the opposing perspective offered by conventional video chat systems. Second, the physical proxy and video chat complement one another in a combined system: people used the physical proxy to understand objects, and used video chat to perform gestures and confirm remote actions.</p><div class="ui labels">Keywords: ¬†<span class="ui large grey label">cscw</span><span class="ui large grey label">remote collaboration</span><span class="ui large grey label">object-focused collaboration</span><span class="ui large grey label">physical telepresence</span><span class="ui large grey label">collaborative physical tasks</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Terrance Tin Hoi Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Ehud Sharlin<!-- -->.¬†<b>Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</b>.¬†<i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>.¬†<!-- -->ACM, New York, NY, USA<!-- -->¬† Page: 1-<!-- -->13<!-- -->.¬† DOI: <a href="https://doi.org/10.1145/3173574.3173855" target="_blank">https://doi.org/10.1145/3173574.3173855</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div></div><div class="one wide column"></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img style="max-width:60px;margin:30px auto" src="/static/images/logo-3.png"/><div class="content"><h1 style="font-size:2.2rem">Interactions Lab</h1><div class="sub header">University of Calgary<br/>Department of Computer Science</div></div><img style="max-width:200px;margin:0px auto" src="/static/images/logo-1.png"/></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"id":"lora-oehlberg"}},"page":"/person","query":{"id":"lora-oehlberg"},"buildId":"dc4qxLD17Vjk7Tj2rgurK","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/person" src="/_next/static/dc4qxLD17Vjk7Tj2rgurK/pages/person.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/dc4qxLD17Vjk7Tj2rgurK/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.98f1a4e4c94db9943918.js" async=""></script><script src="/_next/static/runtime/main-22d58f57abab872d6e70.js" async=""></script></body></html>