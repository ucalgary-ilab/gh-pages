(window.webpackJsonp=window.webpackJsonp||[]).push([["4b5f"],{"+/Wn":function(e){e.exports={date:"2018-06",title:"Engaging 'At-Risk' Students through Maker Culture Activities",authors:["Sowmya Somanath","Laura Morrison","Janette Hughes","Ehud Sharlin","Mario Costa Sousa"],series:"TEI 2016",doi:"https://doi.org/10.1145/2839462.2839482",keywords:"DIY, 'at-risk' students, maker culture, education, young learners",pages:9,abstract:"This paper presents a set of lessons learnt from introducing maker culture and DIY paradigms to 'at-risk' students (age 12-14). Our goal is to engage 'at-risk' students through maker culture activities. While improved technology literacy is one of the outcomes we also wanted the learners to use technology to realize concepts and ideas, and to gain freedom of thinking similar to creators, artists and designers. We present our study and a set of high level suggestions to enable thinking about how maker culture activities can facilitate engagement and creative use of technology by 1) thinking about creativity in task, 2) facilitating different entry points, 3) the importance of personal relevance, and 4) relevance to education.",dir:"content/output/publications",base:"tei-2016-somanath.json",ext:".json",sourceBase:"tei-2016-somanath.yaml",sourceExt:".yaml"}},"+iuc":function(e,t,i){i("wgeU"),i("FlQf"),i("bBy9"),i("B9jh"),i("dL40"),i("xvv9"),i("V+O7"),e.exports=i("WEpk").Set},"+sZ5":function(e){e.exports={date:"2020-07",title:"Touch Responsive Augmented Violin Interface System II: Integrating Sensors into a 3D Printed Fingerboard",authors:["Chantelle Ko","Lora Oehlberg"],series:"NIME 2020",keywords:"violin, touch sensor, FSR, fingerboard, augmented, 3D printing, conductive filament, interactive",talk:"https://www.youtube.com/watch?v=INmDzkcIO14",pages:13,abstract:"We present TRAVIS II, an augmented acoustic violin with touch sensors integrated into its 3D printed fingerboard that track left-hand finger gestures in real time. The fingerboard has four strips of conductive PLA filament which produce an electric signal when fingers press down on each string. While these sensors are physically robust, they are mechanically assembled and thus easy to replace if damaged. The performer can also trigger presets via four FSRs attached to the body of the violin. The instrument is completely wireless, giving the performer the freedom to move throughout the performance space. While the sensing fingerboard is installed in place of the traditional fingerboard, all other electronics can be removed from the augmented instrument, maintaining the aesthetics of a traditional violin. Our design allows violinists to naturally create music for interactive performance and improvisation without requiring new instrumental techniques. In this paper, we describe the design of the instrument, experiments leading to the sensing fingerboard, and performative applications of the instrument.",dir:"content/output/publications",base:"nime-2020-ko.json",ext:".json",sourceBase:"nime-2020-ko.yaml",sourceExt:".yaml"}},"+yqD":function(e){e.exports={date:"2020-04",title:"Autonomous Vehicle-Cyclist Interaction: Peril and Promise",authors:["Ming Hou","Karthik Mahadevan","Sowmya Somanath","Ehud Sharlin","Lora Oehlberg"],series:"CHI 2020",doi:"https://doi.org/10.1145/3313831.3376884",keywords:"autonomous vehicle-cyclist interaction, interfaces for communicating intent and awareness",video:"https://www.youtube.com/watch?v=fsgbUeAaFfI",talk:"https://www.youtube.com/watch?v=DtxkWAW9B1s",pages:12,abstract:"Autonomous vehicles (AVs) will redefine interactions between road users. Presently, cyclists and drivers communicate through implicit cues (vehicle motion) and explicit but imprecise signals (hand gestures, horns). Future AVs could consistently communicate awareness and intent and other feedback to cyclists based on their sensor data. We present an exploration of AV-cyclist interaction, starting with preliminary design studies which informed the implementation of an immersive VR AV-cyclist simulator, and the design and evaluation of a number of AV-cyclist interfaces. Our findings suggest that AV-cyclist interfaces can improve rider confidence in lane merging scenarios. We contribute an AV-cyclist immersive simulator, insights on trade-offs of various aspects of AV-cyclist interaction design including modalities, location, and complexity, and positive results suggesting improved rider confidence due to AV-cyclist interaction. While we are encouraged by the potential positive impact AV-cyclist interfaces can have on cyclist culture, we also emphasize the risks over-reliance can pose to cyclists.",dir:"content/output/publications",base:"chi-2020-hou.json",ext:".json",sourceBase:"chi-2020-hou.yaml",sourceExt:".yaml"}},"1D+r":function(e){e.exports={date:"2016-05",title:"Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration",authors:["David Lopez","Lora Oehlberg","Candemir Doger","Tobias Isenberg"],series:"TVCG 2016",doi:"https://doi.org/10.1109/TVCG.2015.2440233",keywords:"visualization of 3D data, human-computer interaction, expert interaction, direct-touch input, mobile displays, stereoscopic environments, VR, AR, conceptual model of interaction, interaction reference frame mapping, observational study",video:"https://www.youtube.com/watch?v=jBtHgTYpJl0",talk:"https://vimeo.com/245846750",pages:13,abstract:"We discuss touch-based navigation of 3D visualizations in a combined monoscopic and stereoscopic viewing environment. We identify a set of interaction modes, and a workflow that helps users transition between these modes to improve their interaction experience. In our discussion we analyze, in particular, the control-display space mapping between the different reference frames of the stereoscopic and monoscopic displays. We show how this mapping supports interactive data exploration, but may also lead to conflicts between the stereoscopic and monoscopic views due to users' movement in space; we resolve these problems through synchronization. To support our discussion, we present results from an exploratory observational evaluation with domain experts in fluid mechanics and structural biology. These experts explored domain-specific datasets using variations of a system that embodies the interaction modes and workflows; we report on their interactions and qualitative feedback on the system and its workflow.",dir:"content/output/publications",base:"tvcg-2016-lopez.json",ext:".json",sourceBase:"tvcg-2016-lopez.yaml",sourceExt:".yaml"}},"1kbU":function(e){e.exports={date:"2016-06",title:"Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones",authors:["Brennan Jones","Kody Dillman","Richard Tang","Anthony Tang","Ehud Sharlin","Lora Oehlberg","Carman Neustaedter","Scott Bateman"],series:"DIS 2016",doi:"https://doi.org/10.1145/2901790.2901847",keywords:"cscw, telepresence, video communication, shared experiences, teleoperation, drones, collaboration, hri",video:"https://www.youtube.com/watch?v=10hbJHIQVX8",pages:13,abstract:"People are increasingly using mobile video to communicate, collaborate, and share experiences while on the go. Yet this presents challenges in adequately sharing camera views with remote users. In this paper, we study the use of semi-autonomous drones for video conferencing, where an outdoor user (using a smartphone) is connected to a desktop user who can explore the environment from the drone's perspective. We describe findings from a study where pairs collaborated to complete shared navigation and search tasks. We illustrate the benefits of providing the desktop user with a view that is elevated, manipulable, and decoupled from the outdoor user. In addition, we articulate how participants overcame challenges in communicating environmental information and navigational cues, negotiated control of the view, and used the drone as a tool for sharing experiences. This provides a new way of thinking about mobile video conferencing where cameras that are decoupled from both users play an integral role in communication, collaboration, and sharing experiences.",dir:"content/output/publications",base:"dis-2016-jones.json",ext:".json",sourceBase:"dis-2016-jones.yaml",sourceExt:".yaml"}},"3L7d":function(e){e.exports={date:"2021-10",title:"HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots",authors:["Ryo Suzuki","Eyal Ofek","Mike Sinclair","Daniel Leithinger","Mar Gonzalez-Franco"],series:"UIST 2021",doi:"https://doi.org/10.1145/3472749.3474821",keywords:"virtual reality, encountered-type haptics, tabletop mobile robots, swarm user interfaces",video:"https://www.youtube.com/watch?v=HTiZgOESJyQ",pages:16,abstract:"HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic ap- proaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in- time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various ap- plications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.",dir:"content/output/publications",base:"uist-2021-suzuki.json",ext:".json",sourceBase:"uist-2021-suzuki.yaml",sourceExt:".yaml"}},"3M7L":function(e){e.exports={date:"2020-10",title:"RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching",authors:["Ryo Suzuki","Rubaiat Habib Kazi","Li-Yi Wei","Stephen DiVerdi","Wilmot Li","Daniel Leithinger"],series:"UIST 2020",doi:"https://doi.org/10.1145/3379337.3415892",keywords:"augmented reality, embedded data visualization, real-time authoring, sketching interfaces, tangible interaction",award:"Honorable Mention",video:"https://www.youtube.com/watch?v=L0p-BNU9rXU",pages:16,abstract:"We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.",dir:"content/output/publications",base:"uist-2020-suzuki.json",ext:".json",sourceBase:"uist-2020-suzuki.yaml",sourceExt:".yaml"}},"4hZ1":function(e,t,i){"use strict";var a=i("KI45"),n=a(i("pbKT")),o=a(i("0iUn")),s=a(i("AT/M")),r=a(i("sLSF")),c=a(i("Tit0")),l=a(i("MI3g")),u=a(i("a7VT")),p=a(i("dfwq")),h=a(i("ttDY"));function d(e){var t=function(){if("undefined"==typeof Reflect||!n.default)return!1;if(n.default.sham)return!1;if("function"==typeof Proxy)return!0;try{return Date.prototype.toString.call((0,n.default)(Date,[],function(){})),!0}catch(e){return!1}}();return function(){var i,a=(0,u.default)(e);if(t){var o=(0,u.default)(this).constructor;i=(0,n.default)(a,arguments,o)}else i=a.apply(this,arguments);return(0,l.default)(this,i)}}Object.defineProperty(t,"__esModule",{value:!0});var m=i("q1tI"),g="undefined"==typeof window;t.default=function(){var e,t=new h.default;function i(i){e=i.props.reduceComponentsToState((0,p.default)(t),i.props),i.props.handleStateChange&&i.props.handleStateChange(e)}return function(a){(0,c.default)(l,a);var n=d(l);function l(e){var a;return(0,o.default)(this,l),a=n.call(this,e),g&&(t.add((0,s.default)(a)),i((0,s.default)(a))),a}return(0,r.default)(l,null,[{key:"rewind",value:function(){var i=e;return e=void 0,t.clear(),i}}]),(0,r.default)(l,[{key:"componentDidMount",value:function(){t.add(this),i(this)}},{key:"componentDidUpdate",value:function(){i(this)}},{key:"componentWillUnmount",value:function(){t.delete(this),i(this)}},{key:"render",value:function(){return null}}]),l}(m.Component)}},"5KnR":function(e){e.exports={date:"2018-05",title:"PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices",authors:["Hyunjoo Oh","Tung D. Ta","Ryo Suzuki","Mark D. Gross","Yoshihiro Kawahara","Lining Yao"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3174015",keywords:"paper electronics, 3d sculpting, paper craft, fabrication techniques, prototyping",video:"https://www.youtube.com/watch?v=DTd863suDN0",pages:12,abstract:"We present PEP (Printed Electronic Papercrafts), a set of design and fabrication techniques to integrate electronic based interactivities into printed papercrafts via 3D sculpting. We explore the design space of PEP, integrating four functions into 3D paper products: actuation, sensing, display, and communication, leveraging the expressive and technical opportunities enabled by paper-like functional layers with a stack of paper. We outline a seven-step workflow, introduce a design tool we developed as an add-on to an existing CAD environment, and demonstrate example applications that combine the electronic enabled functionality, the capability of 3D sculpting, and the unique creative affordances by the materiality of paper.",dir:"content/output/publications",base:"chi-2018-oh.json",ext:".json",sourceBase:"chi-2018-oh.yaml",sourceExt:".yaml"}},"5rD/":function(e){e.exports={date:"2017-06",title:"Critiquing Physical Prototypes for a Remote Audience",authors:["Terrance Mok","Lora Oehlberg"],series:"DIS 2017",doi:"https://doi.org/10.1145/3064663.3064722",pages:13,keywords:"design review, prototype critique, remote collaboration, material experience, open hardware, video conferencing",video:"https://youtu.be/ORN9jljPncc",abstract:"We present an observational study of physical prototype critique that highlights some of the challenges of communicating physical behaviors and materiality at a distance. Geographically distributed open hardware communities often conduct user feedback and peer critique sessions via video conference. However, people have difficulty using current video conferencing tools to demonstrate and critique physical designs. To examine the challenges of remote critique, we conducted an observational lab study in which participants critiqued pairs of physical prototypes (prosthetic hands) for a face-to-face or remote collaborator. In both conditions, participants' material experiences were an important part of their critique, however their attention was divided between interacting with the prototype and finding strategies to communicate `invisible' features. Based on our findings, we propose design implications for remote collaboration tools that support the sharing of material experiences and prototype critique.",dir:"content/output/publications",base:"dis-2017-mok.json",ext:".json",sourceBase:"dis-2017-mok.yaml",sourceExt:".yaml"}},"6T/A":function(e){e.exports={fileMap:{"content/output/labs.json":[{id:"utouch",description:"Physical Interaction and Human-Robot Interaction",prof:"Ehud Sharlin",url:"https://utouch.cpsc.ucalgary.ca/"},{id:"curiosity",description:"Human-Centered Design for Creativity & Curiosity",prof:"Lora Oehlberg",url:"http://pages.cpsc.ucalgary.ca/~lora.oehlberg/"},{id:"dataexperience",description:"Visual Data-driven Tools and Experiences",prof:"Wesley Willett",url:"https://dataexperience.cpsc.ucalgary.ca/"},{id:"suzuki",description:"Tangible UI, Mixed Reality, and Robotics",prof:"Ryo Suzuki",url:"https://ryosuzuki.org/"},{id:"c3-lab",description:"Tech to Bridge Cultural Barriers, Improve Collaboration, & Build Community",prof:"Helen Ai He",url:"https://helenaihe.com/research/"},{id:"grouplab",description:"Research in HCI, CSCW, and UbiComp",prof:"Saul Greenberg (Emeritus)",url:"http://grouplab.cpsc.ucalgary.ca/"},{id:"ricelab",description:"Rethinking Interaction, Collaboration, & Engagement",prof:"Anthony Tang (Adjunct - University of Toronto)",url:"https://ricelab.github.io/"},{id:"innovis",description:"Innovations in Visualization Laboratory",prof:"Sheelagh Carpendale (Adjunct - Simon Fraser University)",url:"http://sheelaghcarpendale.ca/"}],"content/output/booktitles.json":{CHI:{booktitle:"Proceedings of the CHI Conference on Human Factors in Computing Systems",publisher:"ACM, New York, NY, USA"},"CHI EA":{booktitle:"Extended Abstracts of the CHI Conference on Human Factors in Computing Systems",publisher:"ACM, New York, NY, USA"},UIST:{booktitle:"Proceedings of the Annual ACM Symposium on User Interface Software and Technology",publisher:"ACM, New York, NY, USA"},IMWUT:{booktitle:"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",publisher:"ACM, New York, NY, USA"},DIS:{booktitle:"Proceedings of the ACM on Designing Interactive Systems Conference",publisher:"ACM, New York, NY, USA"},MobileHCI:{booktitle:"Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services",publisher:"ACM, New York, NY, USA"},TEI:{booktitle:"Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction",publisher:"ACM, New York, NY, USA"},HRI:{booktitle:"Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction",publisher:"ACM, New York, NY, USA"},VR:{booktitle:"Proceedings of the IEEE Conference on Virtual Reality and 3D User Interfaces",publisher:"IEEE, New York, NY, USA"},TVCG:{booktitle:"IEEE Transactions on Visualization and Computer Graphics",publisher:"IEEE, New York, NY, USA"},IROS:{booktitle:"Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems",publisher:"IEEE, New York, NY, USA"},"C&C":{booktitle:"Proceedings of the ACM on Creativity and Cognition",publisher:"ACM, New York, NY, USA"},dir:"content/output",base:"booktitles.json",ext:".json",sourceBase:"booktitles.yaml",sourceExt:".yaml"},"content/output/people/anthony-chen.json":{name:"Anthony Chen",type:"alumni",past:"master",now:"UCLA",url:"https://xac.is/",dir:"content/output/people",base:"anthony-chen.json",ext:".json",sourceBase:"anthony-chen.yaml",sourceExt:".yaml"},"content/output/news.json":[{date:"2020-08-07",text:"David Ledo defended his PhD dissertation",icon:"fas fa-graduation-cap"},{date:"2020-07-10",text:"One paper accepted to UIST 2020",image:"uist-2020.jpg"},{date:"2020-06-07",text:"One paper accepted to IROS 2020",image:"iros-2020.jpg"},{date:"2020-03-07",text:"One paper accepted to IMWUT 2020",image:"imwut.jpg"},{date:"2020-02-05",text:"Four papers accepted to CHI 2020",image:"chi-2020.jpg"}],"content/output/facility.json":{"Prototyping Tools":[{name:"Form 3",img:"form-3",url:"https://formlabs.com/3d-printers/form-3/"},{name:"Form Wash",img:"form-wash",url:"https://formlabs.com/wash-cure/"},{name:"Form Cure",img:"form-cure",url:"https://formlabs.com/wash-cure/"},{name:"Cetus MK3",img:"cetus",url:"https://shop.tiertime.com/product/cetus-3d-printer-mk3/"},{name:"Ultimaker 3",img:"ultimaker",url:"https://ultimaker.com/3d-printers/ultimaker-3"},{name:"X-Carve CNC Machine 1000mm",img:"x-carve",url:"https://www.inventables.com/technologies/x-carve"},{name:"Mayku Desktop Vacuum Former",img:"mayku",url:"https://www.mayku.me/"},{name:"Silver Bullet Die Cutter",img:"silver-bullet",url:"https://silverbulletcutters.com/"},{name:"Epilog Fusion M2 40inch Laser Cutter",img:"epilog",url:"https://www.epiloglaser.com/laser-machines/fusion-laser-series.htm"}],Electronics:[{name:"Voltera V-One PCB Printer",img:"voltera",url:"https://www.voltera.io/"},{name:"Bantam PCB Mill",img:"bantam",url:"https://www.bantamtools.com/"},{name:"Weller WE1010NA",img:"wellner",url:"https://www.weller-tools.com/we1010na/"},{name:"Eventek KPS305D DC Power Supply",img:"eventek",url:"https://www.amazon.com/dp/B071RNT1CD"},{name:"Reflow Oven T962",img:"t962",url:"https://www.amazon.com/dp/B01LZYEF90"},{name:"Andonstar AD407 Digital Microscope",img:"andonstar",url:"https://www.amazon.com/dp/B07VK52X9C"}],Kniiting:[{name:"Brother 930E Knitting Machine",img:"brother"},{name:"Pfaff Creative 4.5 Embroidery Machine",img:"pfaff",url:"http://www.pfaff.com/en-CA/Machines/creative-4-5"}],"Power Tools":[{name:"Hitachi C10FCG",img:"hitachi",url:"https://www.amazon.com/dp/B07217ZVP5"},{name:"WEN 4208 Drill Press",img:"drill-press",url:"https://www.amazon.com/dp/B00HQONFVE"},{name:"WEN 3959 Band Saw",img:"band-saw",url:"https://www.amazon.com/dp/B077QMBTLP"},{name:"Black+Decker 20V Drill",img:"black-decker",url:"https://www.amazon.com/dp/B00C625KVE"}],"AR/VR":[{name:"Vicon Motion Capture",img:"vicon",url:"https://www.vicon.com/"},{name:"Oculus Quest",img:"oculus-quest",url:"https://www.oculus.com/quest/"},{name:"Hololens 2",img:"hololens-2",url:"https://www.microsoft.com/en-us/hololens/"},{name:"Hololens 1",img:"hololens-1",url:"https://docs.microsoft.com/en-us/hololens/hololens1-hardware"},{name:"Azure Kinect DK",img:"azure-kinect",url:"https://azure.microsoft.com/en-us/services/kinect-dk/"}],Robotics:[{name:"Baxter Robot",img:"baxter",url:"https://www.rethinkrobotics.com/"},{name:"Sony TOIO",img:"toio",url:"https://www.sony.net/SonyInfo/design/stories/toio/"}],Photography:[{name:"Sony a7iii",img:"sony-a7",url:"https://www.sony.com/electronics/interchangeable-lens-cameras/ilce-7m3-body-kit"},{name:"Tamron 28-75mm F/2.8",img:"tamron",url:"https://www.tamron-usa.com/product/lenses/a036.html"},{name:"DJI Ronin SC 3-Axis Gimbal",img:"dji",url:"https://www.dji.com/ronin-sc"},{name:"COMAN KX3636 74inch",img:"conman",url:"https://www.amazon.com/dp/B01NA8PIZX"},{name:"Neewer Camera Slider 39inch",img:"camera-slider",url:"https://www.amazon.com/dp/B07WHXKLFV"},{name:"Emart Backdrop Stand",img:"emart",url:"https://www.amazon.com/dp/B074R9T4FX"},{name:"Newer 10x20ft Backdrop",img:"newer",url:"https://www.amazon.com/dp/B00SR28XPM"},{name:"Hpusn Softbox",img:"hpusn",url:"https://www.amazon.com/dp/B07NBP6D98"},{name:"LimoStudio Foldable Studio",img:"limostudio",url:"https://www.amazon.com/dp/B00OY9DOCY"}],dir:"content/output",base:"facility.json",ext:".json",sourceBase:"facility.yaml",sourceExt:".yaml"},"content/output/people/adnan-karim.json":{name:"Adnan Karim",type:"master",url:"https://sites.google.com/view/adnankarim/",email:"adnan.karim@ucalgary.ca",linkedin:"https://www.linkedin.com/in/adnan-karim-757b98101/",dir:"content/output/people",base:"adnan-karim.json",ext:".json",sourceBase:"adnan-karim.yaml",sourceExt:".yaml"},"content/output/people/april-zhang.json":{name:"April Zhang",type:"master",url:"https://aprilzhang.design/",dir:"content/output/people",base:"april-zhang.json",ext:".json",sourceBase:"april-zhang.yaml",sourceExt:".yaml"},"content/output/people/carl-gutwin.json":{name:"Carl Gutwin",type:"alumni",past:"phd",now:"University of Saskatchewan",url:"https://www.cs.usask.ca/~gutwin/",dir:"content/output/people",base:"carl-gutwin.json",ext:".json",sourceBase:"carl-gutwin.yaml",sourceExt:".yaml"},"content/output/people/ashratuz-zavin-asha.json":{name:"Ashratuz Zavin Asha",type:"master",url:"https://sites.google.com/cse.uiu.ac.bd/ashratuzzavinasha",scholar:"https://scholar.google.com/citations?user=E7gtMMoAAAAJ",dir:"content/output/people",base:"ashratuz-zavin-asha.json",ext:".json",sourceBase:"ashratuz-zavin-asha.yaml",sourceExt:".yaml"},"content/output/people/brennan-jones.json":{name:"Brennan Jones",type:"phd",url:"https://brennanjones.com/",scholar:"https://scholar.google.ca/citations?user=yzxiadIAAAAJ",dir:"content/output/people",base:"brennan-jones.json",ext:".json",sourceBase:"brennan-jones.yaml",sourceExt:".yaml"},"content/output/people/carman-neustaedter.json":{name:"Carman Neustaedter",type:"alumni",past:"phd",now:"Simon Fraser University",url:"https://carmster.com/",dir:"content/output/people",base:"carman-neustaedter.json",ext:".json",sourceBase:"carman-neustaedter.yaml",sourceExt:".yaml"},"content/output/people/charlotte-tang.json":{name:"Charlotte Tang",type:"alumni",past:"master",dir:"content/output/people",base:"charlotte-tang.json",ext:".json",sourceBase:"charlotte-tang.yaml",sourceExt:".yaml"},"content/output/people/bon-adriel-aseniero.json":{name:"Bon Adriel Aseniero",type:"alumni",past:"phd",now:"Autodesk Research",url:"http://bonadriel.com/",scholar:"https://scholar.google.com/citations?user=V4nRMoMAAAAJ",twitter:"https://twitter.com/HexenKoenig",facebook:"https://www.facebook.com/bonadriel",linkedin:"https://www.linkedin.com/in/bon-adriel-aseniero-47140560/",dir:"content/output/people",base:"bon-adriel-aseniero.json",ext:".json",sourceBase:"bon-adriel-aseniero.yaml",sourceExt:".yaml"},"content/output/people/anthony-tang.json":{name:"Anthony Tang",type:"faculty",title:"Adjunct Associate Professor",keywords:["Mixed Reality","CSCW"],order:6,url:"https://hcitang.github.io/",scholar:"https://scholar.google.com/citations?user=RG1EQowAAAAJ",twitter:"https://twitter.com/proclubboy",github:"http://github.com/hcitang",dir:"content/output/people",base:"anthony-tang.json",ext:".json",sourceBase:"anthony-tang.yaml",sourceExt:".yaml"},"content/output/people/christopher-smith.json":{name:"Christopher Smith",type:"master",url:"https://sites.google.com/cse.uiu.ac.bd/ashratuzzavinasha",linkedin:"https://www.linkedin.com/in/christopher-smith-uofc/",dir:"content/output/people",base:"christopher-smith.json",ext:".json",sourceBase:"christopher-smith.yaml",sourceExt:".yaml"},"content/output/people/dane-bertram.json":{name:"Dane Bertram",type:"alumni",past:"master",dir:"content/output/people",base:"dane-bertram.json",ext:".json",sourceBase:"dane-bertram.yaml",sourceExt:".yaml"},"content/output/people/carmen-hull.json":{name:"Carmen Hull",type:"phd",url:"https://www.carmenhull.com/",dir:"content/output/people",base:"carmen-hull.json",ext:".json",sourceBase:"carmen-hull.yaml",sourceExt:".yaml"},"content/output/people/david-ledo.json":{name:"David Ledo",type:"alumni",past:"phd",now:"Autodesk Research",url:"https://www.davidledo.com/",scholar:"https://scholar.google.com/citations?user=V_2BZDoAAAAJ",dir:"content/output/people",base:"david-ledo.json",ext:".json",sourceBase:"david-ledo.yaml",sourceExt:".yaml"},"content/output/people/doug-schaeffer.json":{name:"Doug Schaeffer",type:"alumni",past:"master",dir:"content/output/people",base:"doug-schaeffer.json",ext:".json",sourceBase:"doug-schaeffer.yaml",sourceExt:".yaml"},"content/output/people/christopher-rodriguez.json":{name:"Christopher Rodriguez",type:"undergrad",email:"christopher.rodrigue@ucalgary.ca",linkedin:"https://www.linkedin.com/in/christopher-rodriguez-74259217a/",dir:"content/output/people",base:"christopher-rodriguez.json",ext:".json",sourceBase:"christopher-rodriguez.yaml",sourceExt:".yaml"},"content/output/people/ehud-sharlin.json":{name:"Ehud Sharlin",type:"faculty",title:"Professor",keywords:["HRI","Robots","Drones"],order:1,url:"http://contacts.ucalgary.ca/info/cpsc/profiles/102-3264",scholar:"https://scholar.google.ca/citations?hl=en&user=eAFxlZIAAAAJ",dir:"content/output/people",base:"ehud-sharlin.json",ext:".json",sourceBase:"ehud-sharlin.yaml",sourceExt:".yaml"},"content/output/people/darcy-norman.json":{name:"D'Arcy Norman",type:"phd",url:"https://darcynorman.net/",twitter:"https://twitter.com/realdlnorman",dir:"content/output/people",base:"darcy-norman.json",ext:".json",sourceBase:"darcy-norman.yaml",sourceExt:".yaml"},"content/output/people/edward-tse.json":{name:"Edward Tse",type:"alumni",past:"phd",now:"NUITEQ",url:"https://ca.linkedin.com/in/edward-tse",dir:"content/output/people",base:"edward-tse.json",ext:".json",sourceBase:"edward-tse.yaml",sourceExt:".yaml"},"content/output/people/harrison-chen.json":{name:"Harrison Chen",type:"alumni",past:"undergrad",url:"https://harrisoneverettchen.com",email:"hechen@ucalgary.ca",github:"https://github.com/Hechen93",linkedin:"https://www.linkedin.com/in/harrison-chen-a90b5569/",dir:"content/output/people",base:"harrison-chen.json",ext:".json",sourceBase:"harrison-chen.yaml",sourceExt:".yaml"},"content/output/people/gregor-mcewan.json":{name:"Gregor McEwan",type:"alumni",past:"master",dir:"content/output/people",base:"gregor-mcewan.json",ext:".json",sourceBase:"gregor-mcewan.yaml",sourceExt:".yaml"},"content/output/people/georgina-freeman.json":{name:"Georgina Freeman",type:"phd",dir:"content/output/people",base:"georgina-freeman.json",ext:".json",sourceBase:"georgina-freeman.yaml",sourceExt:".yaml"},"content/output/people/helen-ai-he.json":{name:"Helen Ai He",type:"faculty",title:"Assistant Professor",keywords:["Inclusive Design","CSCW"],order:5,url:"https://helenaihe.com/research",scholar:"https://scholar.google.com/citations?user=FlMKf0gAAAAJ",twitter:"https://twitter.com/helenaihe",email:"helen.he1@ucalgary.ca",dir:"content/output/people",base:"helen-ai-he.json",ext:".json",sourceBase:"helen-ai-he.yaml",sourceExt:".yaml"},"content/output/people/grace-ferguson.json":{name:"Grace Ferguson",type:"undergrad",dir:"content/output/people",base:"grace-ferguson.json",ext:".json",sourceBase:"grace-ferguson.yaml",sourceExt:".yaml"},"content/output/people/jessi-stark.json":{name:"Jessi Stark",type:"alumni",past:"master",now:"University of Toronto",url:"https://jtstark.com/",scholar:"https://scholar.google.com/citations?user=aRkKN5UAAAAJ",twitter:"https://twitter.com/_jessistark",dir:"content/output/people",base:"jessi-stark.json",ext:".json",sourceBase:"jessi-stark.yaml",sourceExt:".yaml"},"content/output/people/james-tam.json":{name:"James Tam",type:"alumni",past:"master",now:"University of Calgary",url:"http://pages.cpsc.ucalgary.ca/~tamj/index.html",dir:"content/output/people",base:"james-tam.json",ext:".json",sourceBase:"james-tam.yaml",sourceExt:".yaml"},"content/output/people/jiannan-li.json":{name:"Jiannan Li",type:"alumni",past:"master",dir:"content/output/people",base:"jiannan-li.json",ext:".json",sourceBase:"jiannan-li.yaml",sourceExt:".yaml"},"content/output/people/karthik-mahadevan.json":{name:"Karthik Mahadevan",type:"alumni",past:"master",now:"University of Toronto",url:"https://karthikm0.github.io/",scholar:"https://scholar.google.ca/citations?user=aK4siPkAAAAJ",dir:"content/output/people",base:"karthik-mahadevan.json",ext:".json",sourceBase:"karthik-mahadevan.yaml",sourceExt:".yaml"},"content/output/people/kimberly-tee.json":{name:"Kimberly Tee",type:"alumni",past:"master",dir:"content/output/people",base:"kimberly-tee.json",ext:".json",sourceBase:"kimberly-tee.yaml",sourceExt:".yaml"},"content/output/people/kathryn-blair.json":{name:"Kathryn Blair",type:"phd",url:"http://kathrynblair.com/",dir:"content/output/people",base:"kathryn-blair.json",ext:".json",sourceBase:"kathryn-blair.yaml",sourceExt:".yaml"},"content/output/people/kendra-wannamaker.json":{name:"Kendra Wannamaker",type:"master",dir:"content/output/people",base:"kendra-wannamaker.json",ext:".json",sourceBase:"kendra-wannamaker.yaml",sourceExt:".yaml"},"content/output/people/lora-oehlberg.json":{name:"Lora Oehlberg",type:"faculty",title:"Associate Professor",keywords:["Tangible","Design Tools"],order:3,url:"https://pages.cpsc.ucalgary.ca/~lora.oehlberg/",scholar:"https://scholar.google.ca/citations?hl=en&user=8GzaBdwAAAAJ",dir:"content/output/people",base:"lora-oehlberg.json",ext:".json",sourceBase:"lora-oehlberg.yaml",sourceExt:".yaml"},"content/output/people/kurtis-danyluk.json":{name:"Kurtis Danyluk",type:"phd",scholar:"https://scholar.google.com/citations?user=vr-EF5IAAAAJ",dir:"content/output/people",base:"kurtis-danyluk.json",ext:".json",sourceBase:"kurtis-danyluk.yaml",sourceExt:".yaml"},"content/output/people/linda-tauscher.json":{name:"Linda Tauscher",type:"alumni",past:"master",dir:"content/output/people",base:"linda-tauscher.json",ext:".json",sourceBase:"linda-tauscher.yaml",sourceExt:".yaml"},"content/output/people/manjot-khangura.json":{name:"Manjot Khangura",type:"undergrad",email:"manjot.khangura@ucalgary.ca",linkedin:"https://www.linkedin.com/in/manjot-khangura/",dir:"content/output/people",base:"manjot-khangura.json",ext:".json",sourceBase:"manjot-khangura.yaml",sourceExt:".yaml"},"content/output/people/manuel-rodriguez.json":{name:"Manuel Rodriguez",type:"undergrad",email:"manuel.rodriguez@ucalgary.ca",linkedin:"https://www.linkedin.com/in/manuel-rodriguez/",dir:"content/output/people",base:"manuel-rodriguez.json",ext:".json",sourceBase:"manuel-rodriguez.yaml",sourceExt:".yaml"},"content/output/people/colin-auyeung.json":{name:"Colin Au Yeung",type:"undergrad",url:"https://colinauyeung.github.io/",dir:"content/output/people",base:"colin-auyeung.json",ext:".json",sourceBase:"colin-auyeung.yaml",sourceExt:".yaml"},"content/output/people/mark-roseman.json":{name:"Mark Roseman",type:"alumni",past:"master",dir:"content/output/people",base:"mark-roseman.json",ext:".json",sourceBase:"mark-roseman.yaml",sourceExt:".yaml"},"content/output/people/matthew-dunlap.json":{name:"Matthew Dunlap",type:"alumni",past:"master",dir:"content/output/people",base:"matthew-dunlap.json",ext:".json",sourceBase:"matthew-dunlap.yaml",sourceExt:".yaml"},"content/output/people/marcus-friedel.json":{name:"Marcus Friedel",type:"master",facebook:"https://www.facebook.com/marcus.friedel.3",email:"marcus.friedel@ucalgary.ca",linkedin:"https://www.linkedin.com/in/marcusfriedel/",dir:"content/output/people",base:"marcus-friedel.json",ext:".json",sourceBase:"marcus-friedel.yaml",sourceExt:".yaml"},"content/output/people/kaynen-mitchell.json":{name:"Kaynen Mitchell",type:"undergrad",email:"kaynen.mitchell1@ucalgary.ca",linkedin:"https://www.linkedin.com/in/kaynen-mitchell/",dir:"content/output/people",base:"kaynen-mitchell.json",ext:".json",sourceBase:"kaynen-mitchell.yaml",sourceExt:".yaml"},"content/output/people/martin-feick.json":{name:"Martin Feick",type:"phd",url:"http://martinfeick.com/",scholar:"https://scholar.google.de/citations?user=az0GkfQAAAAJ",github:"https://github.com/MartinFk",dir:"content/output/people",base:"martin-feick.json",ext:".json",sourceBase:"martin-feick.yaml",sourceExt:".yaml"},"content/output/people/micheal-rounding.json":{name:"Micheal Roudning",type:"alumni",past:"master",dir:"content/output/people",base:"micheal-rounding.json",ext:".json",sourceBase:"micheal-rounding.yaml",sourceExt:".yaml"},"content/output/people/kathryn-elliot-rounding.json":{name:"Kathryn Elliot-Rounding",type:"alumni",past:"master",dir:"content/output/people",base:"kathryn-elliot-rounding.json",ext:".json",sourceBase:"kathryn-elliot-rounding.yaml",sourceExt:".yaml"},"content/output/people/miaosen-wang.json":{name:"Miaosen Wang",type:"alumni",past:"master",dir:"content/output/people",base:"miaosen-wang.json",ext:".json",sourceBase:"miaosen-wang.yaml",sourceExt:".yaml"},"content/output/people/micheal-boyle.json":{name:"Micheal Boyle",type:"alumni",past:"phd",dir:"content/output/people",base:"micheal-boyle.json",ext:".json",sourceBase:"micheal-boyle.yaml",sourceExt:".yaml"},"content/output/people/nicolai-marquardt.json":{name:"Nicolai Marquardt",type:"alumni",past:"phd",now:"University College London",url:"http://www.nicolaimarquardt.com/",scholar:"https://scholar.google.com/citations?user=PXeN0RsAAAAJ",dir:"content/output/people",base:"nicolai-marquardt.json",ext:".json",sourceBase:"nicolai-marquardt.yaml",sourceExt:".yaml"},"content/output/people/nathalie-bressa.json":{name:"Nathalie Bressa",type:"visiting",dir:"content/output/people",base:"nathalie-bressa.json",ext:".json",sourceBase:"nathalie-bressa.yaml",sourceExt:".yaml"},"content/output/people/micheal-nunes.json":{name:"Micheal Nunes",type:"alumni",past:"master",dir:"content/output/people",base:"micheal-nunes.json",ext:".json",sourceBase:"micheal-nunes.yaml",sourceExt:".yaml"},"content/output/people/neil-chulpongsatorn.json":{name:"Neil Chulpongsatorn",type:"master",url:"https://thobthai.wixsite.com/neilchulpongsatorn",scholar:"https://scholar.google.com/citations?user=D0KbikIAAAAJ&hl=en",linkedin:"https://www.linkedin.com/in/neil-chulpongsatorn-187273204/",email:"thobthai.chulpongsat@ucalgary.ca",dir:"content/output/people",base:"neil-chulpongsatorn.json",ext:".json",sourceBase:"neil-chulpongsatorn.yaml",sourceExt:".yaml"},"content/output/people/ryo-suzuki.json":{name:"Ryo Suzuki",type:"faculty",title:"Assistant Professor",keywords:["Tangible","Robots","AR/VR"],order:4,url:"https://ryosuzuki.org",scholar:"https://scholar.google.com/citations?user=klWjaQIAAAAJ",twitter:"https://twitter.com/ryosuzk",facebook:"https://www.facebook.com/ryosuzk",email:"ryo.suzuki@ucalgary.ca",github:"https://github.com/ryosuzuki",linkedin:"https://www.linkedin.com/in/ryosuzuki/",dir:"content/output/people",base:"ryo-suzuki.json",ext:".json",sourceBase:"ryo-suzuki.yaml",sourceExt:".yaml"},"content/output/people/michael-hung.json":{name:"Michael Hung",type:"master",url:"https://michael-hung.ca",email:"myshung@ucalgary.ca",github:"https://github.com/murrrkle",linkedin:"https://www.linkedin.com/in/hungyukshing",dir:"content/output/people",base:"michael-hung.json",ext:".json",sourceBase:"michael-hung.yaml",sourceExt:".yaml"},"content/output/people/roberta-cabral-mota.json":{name:"Roberta Cabral Mota",type:"phd",url:"https://www.robertacrmota.com/",dir:"content/output/people",base:"roberta-cabral-mota.json",ext:".json",sourceBase:"roberta-cabral-mota.yaml",sourceExt:".yaml"},"content/output/people/saul-greenberg.json":{name:"Saul Greenberg",type:"faculty",title:"Emeritus Professor",keywords:["UbiComp","CSCW"],order:7,url:"http://saul.cpsc.ucalgary.ca/",scholar:"https://scholar.google.com/citations?user=TthhUuoAAAAJ",dir:"content/output/people",base:"saul-greenberg.json",ext:".json",sourceBase:"saul-greenberg.yaml",sourceExt:".yaml"},"content/output/people/samin-farajian.json":{name:"Samin Farajian",type:"master",url:"https://saminfarajian.github.io/work.html",linkedin:"https://www.linkedin.com/in/samin-farajian-736018140/",dir:"content/output/people",base:"samin-farajian.json",ext:".json",sourceBase:"samin-farajian.yaml",sourceExt:".yaml"},"content/output/people/setareh-manesh.json":{name:"Setareh Manesh",type:"alumni",past:"master",dir:"content/output/people",base:"setareh-manesh.json",ext:".json",sourceBase:"setareh-manesh.yaml",sourceExt:".yaml"},"content/output/people/roberto-diaz-marino.json":{name:"Roberto Diaz Marino",type:"alumni",past:"master",dir:"content/output/people",base:"roberto-diaz-marino.json",ext:".json",sourceBase:"roberto-diaz-marino.yaml",sourceExt:".yaml"},"content/output/people/shaun-kaasten.json":{name:"Shaun Kaasten",type:"alumni",past:"master",dir:"content/output/people",base:"shaun-kaasten.json",ext:".json",sourceBase:"shaun-kaasten.yaml",sourceExt:".yaml"},"content/output/people/sowmya-somanath.json":{name:"Sowmya Somanath",type:"alumni",past:"phd",now:"University of Victoria",url:"http://pages.cpsc.ucalgary.ca/~ssomanat/",scholar:"https://scholar.google.ca/citations?user=R9ar1NkAAAAJ",twitter:"https://twitter.com/sowmyasomanath",dir:"content/output/people",base:"sowmya-somanath.json",ext:".json",sourceBase:"sowmya-somanath.yaml",sourceExt:".yaml"},"content/output/people/soren-knudsen.json":{name:"Søren Knudsen",type:"alumni",past:"postdoc",now:"University of Copenhagen",url:"http://sorenknudsen.com/",dir:"content/output/people",base:"soren-knudsen.json",ext:".json",sourceBase:"soren-knudsen.yaml",sourceExt:".yaml"},"content/output/people/sabrina-lakhdhir.json":{name:"Sabrina Lakhdhir",type:"undergrad",dir:"content/output/people",base:"sabrina-lakhdhir.json",ext:".json",sourceBase:"sabrina-lakhdhir.yaml",sourceExt:".yaml"},"content/output/people/sheelagh-carpendale.json":{name:"Sheelagh Carpendale",type:"faculty",title:"Adjunct Professor",keywords:["Data Viz","Data Phyz"],order:8,url:"https://www.cs.sfu.ca/~sheelagh/",scholar:"https://scholar.google.com/citations?user=43LLX2kAAAAJ",dir:"content/output/people",base:"sheelagh-carpendale.json",ext:".json",sourceBase:"sheelagh-carpendale.yaml",sourceExt:".yaml"},"content/output/people/sydney-pratte.json":{name:"Sydney Pratte",type:"phd",url:"https://www.sydneypratte.ca/",dir:"content/output/people",base:"sydney-pratte.json",ext:".json",sourceBase:"sydney-pratte.yaml",sourceExt:".yaml"},"content/output/people/shivesh-jadon.json":{name:"Shivesh Jadon",type:"master",url:"https://shivesh.dev/",scholar:"https://scholar.google.com/citations?user=EwqYU2sAAAAJ&hl=en",linkedin:"https://www.linkedin.com/in/shiveshjadon/",twitter:"https://twitter.com/ShiveshJadon",dir:"content/output/people",base:"shivesh-jadon.json",ext:".json",sourceBase:"shivesh-jadon.yaml",sourceExt:".yaml"},"content/output/people/terrance-mok.json":{name:"Terrance Mok",type:"phd",url:"http://terrancemok.com/",scholar:"https://scholar.google.ca/citations?user=nHkJDSEAAAAJ",twitter:"https://twitter.com/terrancem",linkedin:"https://www.linkedin.com/in/terrance-mok-22421011",dir:"content/output/people",base:"terrance-mok.json",ext:".json",sourceBase:"terrance-mok.yaml",sourceExt:".yaml"},"content/output/people/tim-au-yeung.json":{name:"Tim Au Yeung",type:"phd",linkedin:"https://www.linkedin.com/in/tim-au-yeung-3a29816",dir:"content/output/people",base:"tim-au-yeung.json",ext:".json",sourceBase:"tim-au-yeung.yaml",sourceExt:".yaml"},"content/output/people/tian-xia.json":{name:"Tian Xia",type:"undergrad",linkedin:"https://www.linkedin.com/in/tianxiacs/",email:"tian.xia2@ucalgary.ca",dir:"content/output/people",base:"tian-xia.json",ext:".json",sourceBase:"tian-xia.yaml",sourceExt:".yaml"},"content/output/people/teddy-seyed.json":{name:"Teddy Seyed",type:"alumni",past:"phd",now:"Microsoft Research",url:"https://www.microsoft.com/en-us/research/people/teddy/",scholar:"https://scholar.google.com/citations?user=A8VSir8AAAAJ",dir:"content/output/people",base:"teddy-seyed.json",ext:".json",sourceBase:"teddy-seyed.yaml",sourceExt:".yaml"},"content/output/people/theodore-ogrady.json":{name:"Theodore (Ted) O'Grady",type:"alumni",past:"master",dir:"content/output/people",base:"theodore-ogrady.json",ext:".json",sourceBase:"theodore-ogrady.yaml",sourceExt:".yaml"},"content/output/people/wesley-willett.json":{name:"Wesley Willett",type:"faculty",title:"Associate Professor",keywords:["Data Viz","Data Phyz","AR"],order:3,url:"http://www.wjwillett.net",scholar:"https://scholar.google.ca/citations?user=Q17-rckAAAAJ",dir:"content/output/people",base:"wesley-willett.json",ext:".json",sourceBase:"wesley-willett.yaml",sourceExt:".yaml"},"content/output/people/stephanie-smale.json":{name:"Stephanie Smale",type:"alumni",past:"master",dir:"content/output/people",base:"stephanie-smale.json",ext:".json",sourceBase:"stephanie-smale.yaml",sourceExt:".yaml"},"content/output/publications/chi-2015-aseniero.json":{date:"2015-04",title:"Stratos: Using Visualization to Support Decisions in Strategic Software Release Planning",authors:["Bon Adriel Aseniero","Tiffany Wun","David Ledo","Guenther Ruhe","Anthony Tang","Sheelagh Carpendale"],series:"CHI 2015",doi:"https://doi.org/10.1145/2702123.2702426",keywords:"software engineering, information visualization, release planning",video:"https://www.youtube.com/watch?v=qm57aHjTAYc",pages:10,abstract:"Software is typically developed incrementally and released in stages. Planning these releases involves deciding which features of the system should be implemented for each release. This is a complex planning process involving numerous trade-offs-constraints and factors that often make decisions difficult. Since the success of a product depends on this plan, it is important to understand the trade-offs between different release plans in order to make an informed choice. We present STRATOS, a tool that simultaneously visualizes several software release plans. The visualization shows several attributes about each plan that are important to planners. Multiple plans are shown in a single layout to help planners find and understand the trade-offs between alternative plans. We evaluated our tool via a qualitative study and found that STRATOS enables a range of decision-making processes, helping participants decide on which plan is most optimal.",dir:"content/output/publications",base:"chi-2015-aseniero.json",ext:".json",sourceBase:"chi-2015-aseniero.yaml",sourceExt:".yaml"},"content/output/publications/assets-2017-suzuki.json":{date:"2017-10",title:"FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers",authors:["Ryo Suzuki","Abigale Stangl","Mark D. Gross","Tom Yeh"],series:"ASSETS 2020",doi:"https://doi.org/10.1145/3132525.3132548",keywords:"visual impairment, dynamic tactile markers, tangible interfaces, interactive tactile graphics",video:"https://www.youtube.com/watch?v=VbwIZ9V6i_g",pages:10,abstract:"For people with visual impairments, tactile graphics are an important means to learn and explore information. However, raised line tactile graphics created with traditional materials such as embossing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dynamic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily reconfigured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, feature identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as education and data exploration.",dir:"content/output/publications",base:"assets-2017-suzuki.json",ext:".json",sourceBase:"assets-2017-suzuki.yaml",sourceExt:".yaml"},"content/output/people/william-wright.json":{name:"William Wright",type:"masters",url:"http://pages.cpsc.ucalgary.ca/~wwright/",scholar:"https://scholar.google.com/citations?user=V4nRMoMAAAAJ",twitter:"https://twitter.com/HexenKoenig",facebook:"https://www.facebook.com/bonadriel",linkedin:"https://www.linkedin.com/in/bon-adriel-aseniero-47140560/",dir:"content/output/people",base:"william-wright.json",ext:".json",sourceBase:"william-wright.yaml",sourceExt:".yaml"},"content/output/publications/chi-2017-aoki.json":{date:"2017-05",title:"Environmental Protection and Agency: Motivations, Capacity, and Goals in Participatory Sensing",authors:["Paul Aoki","Allison Woodruff","Baladitya Yellapragada","Wesley Willett"],series:"CHI 2017",doi:"https://doi.org/10.1145/3025453.3025667",keywords:"citizen science, environmental sensing",pages:13,abstract:'In this paper we consider various genres of citizen science from the perspective of citizen participants. As a mode of scientific inquiry, citizen science has the potential to "scale up" scientific data collection efforts and increase lay engagement with science. However, current technological directions risk losing sight of the ways in which citizen science is actually practiced. As citizen science is increasingly used to describe a wide range of activities, we begin by presenting a framework of citizen science genres. We then present findings from four interlocking qualitative studies and technological interventions of community air quality monitoring efforts, examining the motivations and capacities of citizen participants and characterizing their alignment with different types of citizen science. Based on these studies, we suggest that data acquisition involves complex multi-dimensional tradeoffs, and the commonly held view that citizen science systems are a win-win for citizens and science may be overstated.',dir:"content/output/publications",base:"chi-2017-aoki.json",ext:".json",sourceBase:"chi-2017-aoki.yaml",sourceExt:".yaml"},"content/output/publications/chi-2015-willett.json":{date:"2015-04",title:"Lightweight Relief Shearing for Enhanced Terrain Perception on Interactive Maps",authors:["Wesley Willett","Bernhard Jenny","Tobias Isenberg","Pierre Dragicevic"],series:"CHI 2015",doi:"https://doi.org/10.1145/2702123.2702172",keywords:"plan oblique relief, interaction, depth perception, terrain maps, relief shearing",pages:10,video:"https://www.youtube.com/watch?v=YW31lmzQzpc",abstract:"We explore interactive relief shearing, a set of non-intrusive, direct manipulation interactions that expose depth and shape information in terrain maps using ephemeral animations. Reading and interpreting topography and relief on terrain maps is an important aspect of map use, but extracting depth information from 2D maps is notoriously difficult. Modern mapping software attempts to alleviate this limitation by presenting digital terrain using 3D views. However, 3D views introduce occlusion, complicate distance estimations, and typically require more complex interactions. In contrast, our approach reveals depth information via shearing animations on 2D maps, and can be paired with existing interactions such as pan and zoom. We examine explicit, integrated, and hybrid interactions for triggering relief shearing and present a version that uses device tilt to control depth effects. Our evaluation shows that these interactive techniques improve depth perception when compared to standard 2D and perspective views.",dir:"content/output/publications",base:"chi-2015-willett.json",ext:".json",sourceBase:"chi-2015-willett.yaml",sourceExt:".yaml"},"content/output/publications/chi-2015-jones.json":{date:"2015-04",title:"Mechanics of Camera Work in Mobile Video Collaboration",authors:["Brennan Jones","Anna Witcraft","Scott Bateman","Carman Neustaedter","Anthony Tang"],series:"CHI 2015",doi:"https://doi.org/10.1145/2702123.2702345",keywords:"video communication, collaboration, mobile computing, handheld devices, cscw",pages:10,video:"https://www.youtube.com/watch?v=V133YGkLxC8",abstract:"Mobile video conferencing, where one or more participants are moving about in the real world, enables entirely new interaction scenarios (e.g., asking for help to construct or repair an object, or showing a physical location). While we have a good understanding of the challenges of video conferencing in office or home environments, we do not fully understand the mechanics of camera work-how people use mobile devices to communicate with one another-during mobile video calls. To provide an understanding of what people do in mobile video collaboration, we conducted an observational study where pairs of participants completed tasks using a mobile video conferencing system. Our analysis suggests that people use the camera view deliberately to support their interactions-for example, to convey a message or to ask questions-but the limited field of view, and the lack of camera control can make it a frustrating experience.",dir:"content/output/publications",base:"chi-2015-jones.json",ext:".json",sourceBase:"chi-2015-jones.yaml",sourceExt:".yaml"},"content/output/publications/chi-2015-oehlberg.json":{date:"2015-05",title:"Patterns of Physical Design Remixing in Online Maker Communities",authors:["Lora Oehlberg","Wesley Willett","Wendy E. Mackay"],series:"CHI 2015",doi:"https://doi.org/10.1145/2702123.2702175",keywords:"customization, maker communities, user innovation, collaboration, hacking, remixing",pages:12,talk:"https://www.youtube.com/watch?v=vJrVjH04nGc",abstract:"Makers participate in remixing culture by drawing inspiration from, combining, and adapting designs for physical objects. To examine how makers remix each others' designs on a community scale, we analyzed metadata from over 175,000 digital designs from Thingiverse, the largest online design community for digital fabrication. Remixed designs on Thingiverse are predominantly generated designs from Customizer a built-in web app for adjusting parametric designs. However, we find that these designs do not elicit subsequent user activity and the authors who generate them tend not to contribute additional content to Thingiverse. Outside of Customizer, influential sources of remixing include complex assemblies and design primitives, as well as non-physical resources posing as physical designs. Building on our findings, we discuss ways in which online maker communities could become more than just design repositories and better support collaborative remixing.",dir:"content/output/publications",base:"chi-2015-oehlberg.json",ext:".json",sourceBase:"chi-2015-oehlberg.yaml",sourceExt:".yaml"},"content/output/publications/chi-2017-hull.json":{date:"2017-05",title:"Building with Data: Architectural Models as Inspiration for Data Physicalization",authors:["Carmen Hull","Wesley Willett"],series:"CHI 2017",doi:"https://doi.org/10.1145/3025453.3025850",keywords:"design process, architectural models, data physicalization, embodied interaction, data visualization",pages:12,video:"https://www.youtube.com/watch?v=bkqLNgYIXek",abstract:"In this paper we analyze the role of physical scale models in the architectural design process and apply insights from architecture for the creation and use of data physicalizations. Based on a survey of the architecture literature on model making and ten interviews with practicing architects, we describe the role of physical models as a tool for exploration and communication. From these observations, we identify trends in the use of physical models in architecture, which have the potential to inform the design of data physicalizations. We identify four functions of architectural modeling that can be directly adapted for use in the process of building rich data models. Finally, we discuss how the visualization community can apply observations from architecture to the design of new data physicalizations.",dir:"content/output/publications",base:"chi-2017-hull.json",ext:".json",sourceBase:"chi-2017-hull.yaml",sourceExt:".yaml"},"content/output/publications/chi-2018-dillman.json":{date:"2018-04",title:"A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality",authors:["Kody R. Dillman","Terrance Mok","Anthony Tang","Lora Oehlberg","Alex Mitchell"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3173714",keywords:"game design, guidance, interaction cues, augmented reality",pages:12,talk:"https://www.youtube.com/watch?v=3FoZStToALQ",abstract:"Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences.",dir:"content/output/publications",base:"chi-2018-dillman.json",ext:".json",sourceBase:"chi-2018-dillman.yaml",sourceExt:".yaml"},"content/output/publications/chi-2017-somanath.json":{date:"2017-05",title:"'Maker' within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India",authors:["Sowmya Somanath","Lora Oehlberg","Janette Hughes","Ehud Sharlin","Mario Costa Sousa"],series:"CHI 2017",doi:"https://doi.org/10.1145/3025453.3025849",pages:13,keywords:"India, HCI4D, physical computing, DIY, young learners, maker culture",video:"https://www.youtube.com/watch?v=NpIME1h1mH8",abstract:"Do-it-yourself (DIY) inspired activities have gained popularity as a means of creative expression and self-directed learning. However, DIY culture is difficult to implement in places with limited technology infrastructure and traditional learning cultures. Our goal is to understand how learners in such a setting react to DIY activities. We present observations from a physical computing workshop with 12 students (13-15 years old) conducted at a high school in India. We observed unique challenges for these students when tackling DIY activities: a high monetary and psychological cost to exploration, limited independent learning resources, difficulties with finding intellectual courage and assumed technical language proficiency. Our participants, however, overcome some of these challenges by adopting their own local strategies: resilience, nonverbal and verbal learning techniques, and creating documentation and fallback circuit versions. Based on our findings, we discuss a set of lessons learned about makerspaces in a context with socio-technical challenges.",dir:"content/output/publications",base:"chi-2017-somanath.json",ext:".json",sourceBase:"chi-2017-somanath.yaml",sourceExt:".yaml"},"content/output/publications/chi-2018-feick.json":{date:"2018-04",title:"Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration",authors:["Martin Feick","Terrance Tin Hoi Mok","Anthony Tang","Lora Oehlberg","Ehud Sharlin"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3173855",keywords:"cscw, remote collaboration, object-focused collaboration, physical telepresence, collaborative physical tasks",pages:13,award:"Honorable Mention",video:"https://www.youtube.com/watch?v=sfxTHsPJWHY",abstract:"Remote collaborators working together on physical objects have difficulty building a shared understanding of what each person is talking about. Conventional video chat systems are insufficient for many situations because they present a single view of the object in a flattened image. To understand how this limited perspective affects collaboration, we designed the Remote Manipulator (ReMa), which can reproduce orientation manipulations on a proxy object at a remote site. We conducted two studies with ReMa, with two main findings. First, a shared perspective is more effective and preferred compared to the opposing perspective offered by conventional video chat systems. Second, the physical proxy and video chat complement one another in a combined system: people used the physical proxy to understand objects, and used video chat to perform gestures and confirm remote actions.",dir:"content/output/publications",base:"chi-2018-feick.json",ext:".json",sourceBase:"chi-2018-feick.yaml",sourceExt:".yaml"},"content/output/publications/chi-2018-heshmat.json":{date:"2018-04",title:"Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing",authors:["Yasamin Heshmat","Brennan Jones","Xiaoxuan Xiong","Carman Neustaedter","Anthony Tang","Bernhard E. Riecke","Lillian Yang"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3173933",keywords:"video communication, telepresence robots, leisure activities, social presence, geocaching",pages:13,abstract:"People often enjoy sharing outdoor activities together such as walking and hiking. However, when family and friends are separated by distance it can be difficult if not impossible to share such activities. We explore this design space by investigating the benefits and challenges of using a telepresence robot to support outdoor leisure activities. In our study, participants participated in the outdoor activity of geocaching where one person geocached with the help of a remote partner via a telepresence robot. We compared a wide field of view (WFOV) camera to a 360° camera. Results show the benefits of having a physical embodiment and a sense of immersion with the 360° view. Yet challenges related to a lack of environmental awareness, safety issues, and privacy concerns resulting from bystander interactions. These findings illustrate the need to design telepresence robots with the environment and public in mind to provide an enhanced sensory experience while balancing safety and privacy issues resulting from being amongst the general public.",dir:"content/output/publications",base:"chi-2018-heshmat.json",ext:".json",sourceBase:"chi-2018-heshmat.yaml",sourceExt:".yaml"},"content/output/people/sasha-ivanov.json":{name:"Sasha Ivanov",type:"master",dir:"content/output/people",base:"sasha-ivanov.json",ext:".json",sourceBase:"sasha-ivanov.yaml",sourceExt:".yaml"},"content/output/publications/chi-2017-ledo.json":{date:"2017-05",title:"Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices",authors:["David Ledo","Fraser Anderson","Ryan Schmidt","Lora Oehlberg","Saul Greenberg","Tovi Grossman"],series:"CHI 2017",doi:"https://doi.org/10.1145/3025453.3025652",pages:10,keywords:"fabrication, 3d printing, smart objects, rapid prototyping, toolkits, prototyping tool, interaction design",video:"https://www.youtube.com/watch?v=ORN9jljPncc","video-2":"https://www.youtube.com/watch?v=ifEIR7cHWxc",talk:"https://www.youtube.com/watch?v=QB0kbcu_CsY",abstract:'Interactive, smart objects – customized to individuals and uses – are central to many movements, such as tangibles, the internet of things (IoT), and ubiquitous computing. Yet, rapid prototyping both the form and function of these custom objects can be problematic, particularly for those with limited electronics or programming experience. Designers often need to embed custom circuitry; program its workings; and create a form factor that not only reflects the desired user experience but can also house the required circuitry and electronics. To mitigate this, we created Pineal, a design tool that lets end-users: (1) modify 3D models to include a smart watch or phone as its heart; (2) specify high-level interactive behaviours through visual programming; and (3) have the phone or watch act out such behaviours as the objects\' "smarts". Furthermore, a series of prototypes show how Pineal exploits mobile sensing and output, and automatically generates 3D printed form-factors for rich, interactive, objects.',dir:"content/output/publications",base:"chi-2017-ledo.json",ext:".json",sourceBase:"chi-2017-ledo.yaml",sourceExt:".yaml"},"content/output/publications/chi-2018-neustaedter.json":{date:"2018-04",title:"The Benefits and Challenges of Video Calling for Emergency Situations",authors:["Carman Neustaedter","Brennan Jones","Kenton O'Hara","Abigail Sellen"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3174231",keywords:"collaboration, situation awareness, emergency calling, call takers, mobile video calling, dispatchers",pages:13,award:"Honorable Mention",abstract:"In the coming years, emergency calling services in North America will begin to incorporate new modalities for reporting emergencies, including video-based calling. The challenge is that we know little of how video calling systems should be designed and what benefits or challenges video calling might bring. We conducted observations and contextual interviews within three emergency response call centres to investigate these points. We focused on the work practices of call takers and dispatchers. Results show that video calls could provide valuable contextual information about a situation and help to overcome call taker challenges with information ambiguity, location, deceit, and communication issues. Yet video calls have the potential to introduce issues around control, information overload, and privacy if systems are not designed well. These results point to the need to think about emergency video calling along a continuum of visual modalities ranging from audio calls accompanied with images or video clips to one-way video streams to two-way video streams where camera control and camera work need to be carefully designed.",dir:"content/output/publications",base:"chi-2018-neustaedter.json",ext:".json",sourceBase:"chi-2018-neustaedter.yaml",sourceExt:".yaml"},"content/output/people/zachary-mckendrick.json":{name:"Zachary McKendrick",type:"phd",url:"https://scpa.ucalgary.ca/manageprofile/profiles/zachary-mckendrick",linkedin:"https://www.linkedin.com/in/zach-mckendrick-7a24bb3b",dir:"content/output/people",base:"zachary-mckendrick.json",ext:".json",sourceBase:"zachary-mckendrick.yaml",sourceExt:".yaml"},"content/output/publications/chi-2018-ledo.json":{date:"2018-04",title:"Evaluation Strategies for HCI Toolkit Research",authors:["David Ledo","Steven Houben","Jo Vermeulen","Nicolai Marquardt","Lora Oehlberg","Saul Greenberg"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3173610",keywords:"user interfaces, design, evaluation, prototyping, toolkits",pages:17,video:"https://www.youtube.com/watch?v=3lAwhCk60C4",talk:"https://www.youtube.com/watch?v=NOhsvN_Kv-I",abstract:"Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what 'evaluating' a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.",dir:"content/output/publications",base:"chi-2018-ledo.json",ext:".json",sourceBase:"chi-2018-ledo.yaml",sourceExt:".yaml"},"content/output/publications/chi-2020-anjani.json":{date:"2020-04",title:"Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers",authors:["Laurensia Anjani","Terrance Mok","Anthony Tang","Lora Oehlberg","Wooi Boon Goh"],series:"CHI 2020",doi:"https://doi.org/10.1145/3313831.3376567",keywords:"video streams, mukbang",pages:13,talk:"https://www.youtube.com/watch?v=Dkp8A_em90M",abstract:"We present a mixed-methods study of viewers on their practices and motivations around watching mukbang — video streams of people eating large quantities of food. Viewers' experiences provide insight on future technologies for multisensorial video streams and technology-supported commensality (eating with others). We surveyed 104 viewers and interviewed 15 of them about their attitudes and reflections on their mukbang viewing habits, their physiological aspects of watching someone eat, and their perceived social relationship with mukbangers. Based on our findings, we propose design implications for remote commensality, and for synchronized multisensorial video streaming content.",dir:"content/output/publications",base:"chi-2020-anjani.json",ext:".json",sourceBase:"chi-2020-anjani.yaml",sourceExt:".yaml"},"content/output/publications/chi-2019-danyluk.json":{date:"2019-04",title:"Look-From Camera Control for 3D Terrain Maps",authors:["Kurtis Danyluk","Bernhard Jenny","Wesley Willett"],series:"CHI 2019",doi:"https://doi.org/10.1145/3290605.3300594",keywords:"terrain, touch, map interaction, look-from camera control",award:"Honorable Mention",pages:12,abstract:"We introduce three lightweight interactive camera control techniques for 3D terrain maps on touch devices based on a look-from metaphor (Discrete Look-From-At, Continuous Look-From-Forwards, and Continuous Look-From-Towards). These techniques complement traditional touch screen pan, zoom, rotate, and pitch controls allowing viewers to quickly transition between top-down, oblique, and ground-level views. We present the results of a study in which we asked participants to perform elevation comparison and line-of-sight determination tasks using each technique. Our results highlight how look-from techniques can be integrated on top of current direct manipulation navigation approaches by combining several direct manipulation operations into a single look-from operation. Additionally, they show how look-from techniques help viewers complete a variety of common and challenging map-based tasks.",dir:"content/output/publications",base:"chi-2019-danyluk.json",ext:".json",sourceBase:"chi-2019-danyluk.yaml",sourceExt:".yaml"},"content/output/publications/chi-2018-wuertz.json":{date:"2018-04",title:"A Design Framework for Awareness Cues in Distributed Multiplayer Games",authors:["Jason Wuertz","Sultan A. Alharthi","William A. Hamilton","Scott Bateman","Carl Gutwin","Anthony Tang","Zachary O. Toups","Jessica Hammer"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3173817",keywords:"workspace awareness, situation awareness, game design, distributed multiplayer games, awareness cues",pages:14,abstract:"In the physical world, teammates develop situation awareness about each other's location, status, and actions through cues such as gaze direction and ambient noise. To support situation awareness, distributed multiplayer games provide awareness cues - information that games automatically make available to players to support cooperative gameplay. The design of awareness cues can be extremely complex, impacting how players experience games and work with teammates. Despite the importance of awareness cues, designers have little beyond experiential knowledge to guide their design. In this work, we describe a design framework for awareness cues, providing insight into what information they provide, how they communicate this information, and how design choices can impact play experience. Our research, based on a grounded theory analysis of current games, is the first to provide a characterization of awareness cues, providing a palette for game designers to improve design practice and a starting point for deeper research into collaborative play.",dir:"content/output/publications",base:"chi-2018-wuertz.json",ext:".json",sourceBase:"chi-2018-wuertz.yaml",sourceExt:".yaml"},"content/output/people/donald-cox.json":{name:"Donald Cox",type:"alumni",past:"master",dir:"content/output/people",base:"donald-cox.json",ext:".json",sourceBase:"donald-cox.yaml",sourceExt:".yaml"},"content/output/publications/chi-2018-oh.json":{date:"2018-05",title:"PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices",authors:["Hyunjoo Oh","Tung D. Ta","Ryo Suzuki","Mark D. Gross","Yoshihiro Kawahara","Lining Yao"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3174015",keywords:"paper electronics, 3d sculpting, paper craft, fabrication techniques, prototyping",video:"https://www.youtube.com/watch?v=DTd863suDN0",pages:12,abstract:"We present PEP (Printed Electronic Papercrafts), a set of design and fabrication techniques to integrate electronic based interactivities into printed papercrafts via 3D sculpting. We explore the design space of PEP, integrating four functions into 3D paper products: actuation, sensing, display, and communication, leveraging the expressive and technical opportunities enabled by paper-like functional layers with a stack of paper. We outline a seven-step workflow, introduce a design tool we developed as an add-on to an existing CAD environment, and demonstrate example applications that combine the electronic enabled functionality, the capability of 3D sculpting, and the unique creative affordances by the materiality of paper.",dir:"content/output/publications",base:"chi-2018-oh.json",ext:".json",sourceBase:"chi-2018-oh.yaml",sourceExt:".yaml"},"content/output/publications/chi-2020-suzuki.json":{date:"2020-05",title:"RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots",authors:["Ryo Suzuki","Hooman Hedayati","Clement Zheng","James Bohn","Daniel Szafir","Ellen Yi-Luen Do","Mark D Gross","Daniel Leithinger"],series:"CHI 2020",doi:"https://doi.org/10.1145/3313831.3376523",keywords:"virtual reality, room-scale haptics, haptic interfaces, swarm robots",video:"https://www.youtube.com/watch?v=4OWU60gTOFE",pages:11,abstract:"RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.",dir:"content/output/publications",base:"chi-2020-suzuki.json",ext:".json",sourceBase:"chi-2020-suzuki.yaml",sourceExt:".yaml"},"content/output/publications/chi-2020-goffin.json":{date:"2020-04",title:"Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations",authors:["Pascal Goffin","Tanja Blascheck","Petra Isenberg","Wesley Willett"],series:"CHI 2020",doi:"https://doi.org/10.1145/3313831.3376842",keywords:"glyphs, word-scale visualization, information visualization, interaction techniques, text visualization",pages:13,video:"https://www.youtube.com/watch?v=wPaVdSWM8hU",abstract:"We describe a design space of view manipulation interactions for small data-driven contextual visualizations (word-scale visualizations). These interaction techniques support an active reading experience and engage readers through exploration of embedded visualizations whose placement and content connect them to specific terms in a document. A reader could, for example, use our proposed interaction techniques to explore word-scale visualizations of stock market trends for companies listed in a market overview article. When readers wish to engage more deeply with the data, they can collect, arrange, compare, and navigate the document using the embedded word-scale visualizations, permitting more visualization-centric analyses. We support our design space with a concrete implementation, illustrate it with examples from three application domains, and report results from two experiments. The experiments show how view manipulation interactions helped readers examine embedded visualizations more quickly and with less scrolling and yielded qualitative feedback on usability and future opportunities.",dir:"content/output/publications",base:"chi-2020-goffin.json",ext:".json",sourceBase:"chi-2020-goffin.yaml",sourceExt:".yaml"},"content/output/publications/chi-2021-hammad.json":{date:"2021-05",title:"Homecoming: Exploring Returns to Long-Term Single Player Games",authors:["Noor Hammad","Owen Brierley","Zachary McKendrick","Sowmya Somanath","Patrick Finn","Jessica Hammer","Ehud Sharlin"],series:"CHI 2021",doi:"https://doi.org/10.1145/3411764.3445357",keywords:"long-term single player game, autobiographical design, pivot point",pages:13,video:null,abstract:"We present an autobiographical design journey exploring the experience of returning to long-term single player games. Continuing progress from a previously saved game, particularly when substantial time has passed, is an understudied area in games research. To begin our exploration in this domain, we investigated what the return experience is like first-hand. By returning to four long-term single player games played extensively in the past, we revealed a phenomenon we call The Pivot Point, a ‘eureka’ moment in return gameplay. The pivot point anchors our design explorations, where we created prototypes to leverage the pivot point in reconnecting with the experience. These return experiences and subsequent prototyping iterations inform our understanding of how to design better returns to gameplay, which can benefit both producers and consumers of long-term single player games.",dir:"content/output/publications",base:"chi-2021-hammad.json",ext:".json",sourceBase:"chi-2021-hammad.yaml",sourceExt:".yaml"},"content/output/publications/chi-2021-danyluk.json":{date:"2021-05",title:"A Design Space Exploration of Worlds in Miniature",authors:["Kurtis Danyluk","Barrett Ens","Bernhard Jenny","Wesley Willett"],series:"CHI 2021",doi:null,keywords:"Virtual/Augmented Reality, Meta-Analysis/Literature Survey",pages:20,video:null,abstract:"Worlds-in-Miniature (WiMs) are interactive worlds within a world and combine the advantages of an input space, a cartographicmap, and an overview+detail interface. They have been used across the extended virtuality spectrum for a variety of applications.Building on an analysis of examples of WiMs from the research literature we contribute a design space for WiMs based on sevendesign dimensions. Further, we expand upon existing definitions of WiMs to provide a definition that applies across the extendedreality spectrum. We identify the design dimensions of size-scope-scale, abstraction, geometry, reference frame, links, multiples, andvirtuality. Using our framework we describe existing Worlds-in-Miniature from the research literature and reveal unexplored researchareas. Finally, we generate new examples of WiMs using our framework to fill some of these gaps. With our findings, we identifyopportunities that can guide future research into WiMs.",dir:"content/output/publications",base:"chi-2021-danyluk.json",ext:".json",sourceBase:"chi-2021-danyluk.yaml",sourceExt:".yaml"},"content/output/publications/chi-2021-ens.json":{date:"2021-05",title:"Grand Challenges in Immersive Analytics",authors:["Barrett Ens","Benjamin Bach","Maxime Cordeil","Ulrich Engelke","Marcos Serrano","Wesley Willett","Arnaud Prouzeau","Christoph Anthes","Wolfgang Büschel","Cody Dunne","Tim Dwyer","Jens Grubert","Jason H. Haga","Nurit Kishenbaum","Dylan Kobayashi","Tica Lin","Monsurat Olaosebikan","Fabian Pointecker","David Saffo","Nazmus Saquib","Dieter Schmalsteig","Danielle Albers Szafir","Matthew Whitlock","Yalong Yang"],series:"CHI 2021",doi:null,keywords:"Immersive analytics, grand research challenges, data visualisation, augmented reality, virtual reality",pages:17,video:null,abstract:"Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.",dir:"content/output/publications",base:"chi-2021-ens.json",ext:".json",sourceBase:"chi-2021-ens.yaml",sourceExt:".yaml"},"content/output/publications/cnc-2019-hammad.json":{date:"2019-06",title:"Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning",authors:["Nour Hammad","Elaheh Sanoubari","Patrick Finn","Sowmya Somanath","James E. Young","Ehud Sharlin"],year:2019,series:"C&C 2019",keywords:"interaction design, cyborgs, user experience, performing art techniques",video:"https://www.youtube.com/watch?v=HFH59__Fkok",doi:"https://doi.org/10.1145/3325480.3325508",abstract:"We present Mutation: performing arts based approach that can help decrease the cognitive load associated with cyborg transitioning. Cyborgs are human-machine hybrids with organic and mechatronic body parts that can be implanted or worn. The transition into and out of experiencing additional body parts is not fully understood. Our goal is to draw from performing arts techniques in order to help decrease the cognitive load associated with becoming and unbecoming a cyborg. Actors constantly shift between states, whether from one character to another, or from pre- to post- performance. We contribute a straightforward adaptation of classic performing art practices to cyborg transitioning, and a study where actors used these protocols in order to enter a cyborg state, perform as a cyborg, and then exit the cyborg state. Our work on Mutation suggests that classic performing art practices can be useful in cyborg transitioning, as well as in other technology augmented experiences.",dir:"content/output/publications",base:"cnc-2019-hammad.json",ext:".json",sourceBase:"cnc-2019-hammad.yaml",sourceExt:".yaml"},"content/output/publications/cupum-2021-rout.json":{date:"2021-02",title:"(Big) Data in Urban Design Practice: Supporting High-Level Design Tasks Using a Visualization of Human Movement Data from Smartphones",authors:["Angela Rout","Wesley Willett"],series:"Urban Informatics and Future Cities",publisher:"Springer",pages:"301-318",doi:"http://hdl.handle.net/1880/113114",video:"https://www.youtube.com/watch?v=Me8cU6RoCiA",keywords:"information visualization, smartphone data, GPS, data visualization, architecture, urban design, task-based framework, high-level tasks",abstract:"We present the SmartCampus visualization tool, representing spatiotemporal data of over 200 student pathways and restpoints on a university campus. Based on our experiences with SmartCampus, we also propose a task-based framework that de-scribes how practicing urban designers (specifically, architects) can use human movement data visualizations in their work. Although extensive amounts of location data are produced daily by smartphones, existing geospatial tools are not customized to specifically support high-level urban design tasks. To help identify opportunities in urban design for visualizing human movement data from devices such as smartphones, we used our SmartCampus prototype to facilitate a series of 3 participatory design sessions (3 participants), a targeted online survey (14 participants), and semi-structured interviews (6 participants) with architectural experts. Our findings showcase the need for location analysis tools tailored to concrete urban design practices, and also highlight opportunities for Smart City researchers interested in developing domain specific, visualization tools.",dir:"content/output/publications",base:"cupum-2021-rout.json",ext:".json",sourceBase:"cupum-2021-rout.yaml",sourceExt:".yaml"},"content/output/people/yibo-sun.json":{name:"Yibo Sun",type:"alumni",past:"master",dir:"content/output/people",base:"yibo-sun.json",ext:".json",sourceBase:"yibo-sun.yaml",sourceExt:".yaml"},"content/output/publications/chi-2020-hou.json":{date:"2020-04",title:"Autonomous Vehicle-Cyclist Interaction: Peril and Promise",authors:["Ming Hou","Karthik Mahadevan","Sowmya Somanath","Ehud Sharlin","Lora Oehlberg"],series:"CHI 2020",doi:"https://doi.org/10.1145/3313831.3376884",keywords:"autonomous vehicle-cyclist interaction, interfaces for communicating intent and awareness",video:"https://www.youtube.com/watch?v=fsgbUeAaFfI",talk:"https://www.youtube.com/watch?v=DtxkWAW9B1s",pages:12,abstract:"Autonomous vehicles (AVs) will redefine interactions between road users. Presently, cyclists and drivers communicate through implicit cues (vehicle motion) and explicit but imprecise signals (hand gestures, horns). Future AVs could consistently communicate awareness and intent and other feedback to cyclists based on their sensor data. We present an exploration of AV-cyclist interaction, starting with preliminary design studies which informed the implementation of an immersive VR AV-cyclist simulator, and the design and evaluation of a number of AV-cyclist interfaces. Our findings suggest that AV-cyclist interfaces can improve rider confidence in lane merging scenarios. We contribute an AV-cyclist immersive simulator, insights on trade-offs of various aspects of AV-cyclist interaction design including modalities, location, and complexity, and positive results suggesting improved rider confidence due to AV-cyclist interaction. While we are encouraged by the potential positive impact AV-cyclist interfaces can have on cyclist culture, we also emphasize the risks over-reliance can pose to cyclists.",dir:"content/output/publications",base:"chi-2020-hou.json",ext:".json",sourceBase:"chi-2020-hou.yaml",sourceExt:".yaml"},"content/output/publications/dis-2017-mok.json":{date:"2017-06",title:"Critiquing Physical Prototypes for a Remote Audience",authors:["Terrance Mok","Lora Oehlberg"],series:"DIS 2017",doi:"https://doi.org/10.1145/3064663.3064722",pages:13,keywords:"design review, prototype critique, remote collaboration, material experience, open hardware, video conferencing",video:"https://youtu.be/ORN9jljPncc",abstract:"We present an observational study of physical prototype critique that highlights some of the challenges of communicating physical behaviors and materiality at a distance. Geographically distributed open hardware communities often conduct user feedback and peer critique sessions via video conference. However, people have difficulty using current video conferencing tools to demonstrate and critique physical designs. To examine the challenges of remote critique, we conducted an observational lab study in which participants critiqued pairs of physical prototypes (prosthetic hands) for a face-to-face or remote collaborator. In both conditions, participants' material experiences were an important part of their critique, however their attention was divided between interacting with the prototype and finding strategies to communicate `invisible' features. Based on our findings, we propose design implications for remote collaboration tools that support the sharing of material experiences and prototype critique.",dir:"content/output/publications",base:"dis-2017-mok.json",ext:".json",sourceBase:"dis-2017-mok.yaml",sourceExt:".yaml"},"content/output/publications/dis-2016-jones.json":{date:"2016-06",title:"Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones",authors:["Brennan Jones","Kody Dillman","Richard Tang","Anthony Tang","Ehud Sharlin","Lora Oehlberg","Carman Neustaedter","Scott Bateman"],series:"DIS 2016",doi:"https://doi.org/10.1145/2901790.2901847",keywords:"cscw, telepresence, video communication, shared experiences, teleoperation, drones, collaboration, hri",video:"https://www.youtube.com/watch?v=10hbJHIQVX8",pages:13,abstract:"People are increasingly using mobile video to communicate, collaborate, and share experiences while on the go. Yet this presents challenges in adequately sharing camera views with remote users. In this paper, we study the use of semi-autonomous drones for video conferencing, where an outdoor user (using a smartphone) is connected to a desktop user who can explore the environment from the drone's perspective. We describe findings from a study where pairs collaborated to complete shared navigation and search tasks. We illustrate the benefits of providing the desktop user with a view that is elevated, manipulable, and decoupled from the outdoor user. In addition, we articulate how participants overcame challenges in communicating environmental information and navigational cues, negotiated control of the view, and used the drone as a tool for sharing experiences. This provides a new way of thinking about mobile video conferencing where cameras that are decoupled from both users play an integral role in communication, collaboration, and sharing experiences.",dir:"content/output/publications",base:"dis-2016-jones.json",ext:".json",sourceBase:"dis-2016-jones.yaml",sourceExt:".yaml"},"content/output/publications/dis-2019-ledo.json":{date:"2019-06",title:"Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications",authors:["David Ledo","Jo Vermeulen","Sheelagh Carpendale","Saul Greenberg","Lora Oehlberg","Sebastian Boring"],series:"DIS 2019",doi:"https://doi.org/10.1145/3322276.3322329",keywords:"smart objects, mobile interfaces, prototyping, design tool, interactive behaviour",pages:14,abstract:"Astral is a prototyping tool for authoring mobile and smart object interactive behaviours. It mirrors selected display contents of desktop applications onto mobile devices (smartphones and smartwatches), and streams/remaps mobile sensor data to desktop input events (mouse or keyboard) to manipulate selected desktop contents. This allows designers to use familiar desktop applications (e.g. PowerPoint, AfterEffects) to prototype rich interactive behaviours. Astral combines and integrates display mirroring, sensor streaming and input remapping, where designers can exploit familiar desktop applications to prototype, explore and fine-tune dynamic interactive behaviours. With Astral, designers can visually author rules to test real-time behaviours while interactions take place, as well as after the interaction has occurred. We demonstrate Astral's applicability, workflow and expressiveness within the interaction design process through both new examples and replication of prior approaches that illustrate how various familiar desktop applications are leveraged and repurposed.",dir:"content/output/publications",base:"dis-2019-ledo.json",ext:".json",sourceBase:"dis-2019-ledo.yaml",sourceExt:".yaml"},"content/output/publications/dis-2018-pham.json":{date:"2018-06",title:"Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design",authors:["Tran Pham","Jo Vermeulen","Anthony Tang","Lindsay MacDonald Vermeulen"],series:"DIS 2018",doi:"https://doi.org/10.1145/3196709.3196719",keywords:"augmented reality, gestures, gesture elicitation, hololens",pages:14,abstract:"Because gesture design for augmented reality (AR) remains idiosyncratic, people cannot necessarily use gestures learned in one AR application in another. To design discoverable gestures, we need to understand what gestures people expect to use. We explore how the scale of AR affects the gestures people expect to use to interact with 3D holograms. Using an elicitation study, we asked participants to generate gestures in response to holographic task referents, where we varied the scale of holograms from desktop-scale to room-scale objects. We found that the scale of objects and scenes in the AR experience moderates the generated gestures. Most gestures were informed by physical interaction, and when people interacted from a distance, they sought a good perspective on the target object before and during the interaction. These results suggest that gesture designers need to account for scale, and should not simply reuse gestures across different hologram sizes.",dir:"content/output/publications",base:"dis-2018-pham.json",ext:".json",sourceBase:"dis-2018-pham.yaml",sourceExt:".yaml"},"content/output/publications/dis-2019-seyed.json":{date:"2019-06",title:"Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways",authors:["Teddy Seyed","Anthony Tang"],series:"DIS 2019",doi:"https://doi.org/10.1145/3322276.3322305",keywords:"fashion, haute couture, e-textiles, maker culture, fashion-tech, wearables, avant-garde, haute-tech couture, modular",award:"Honorable Mention",pages:13,abstract:"Drawing upon multiple disciplines, avant-garde fashion-tech teams push the boundaries between fashion and technology. Many are well trained in envisioning aesthetic qualities of garments, but few have formal training on designing and fabricating technologies themselves. We introduce Mannequette, a prototyping tool for fashion-tech garments that enables teams to experiment with interactive technologies at early stages of their design processes. Mannequette provides an abstraction of light-based outputs and sensor-based inputs for garments through a DJ mixer-like interface that allows for dynamic changes and recording/playback of visual effects. The base of Mannequette can also be incorporated into the final garment, where it is then connected to the final components. We conducted an 8-week deployment study with eight design teams who created new garments for a runway show. Our results revealed Mannequette allowed teams to repeatedly consider new design and technical options early in their creative processes, and to communicate more effectively across disciplinary backgrounds.",dir:"content/output/publications",base:"dis-2019-seyed.json",ext:".json",sourceBase:"dis-2019-seyed.yaml",sourceExt:".yaml"},"content/output/publications/dis-2019-nakayama.json":{date:"2019-06",title:"MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction",authors:["Ryosuke Nakayama","Ryo Suzuki","Satoshi Nakamaru","Ryuma Niiyama","Yoshihiro Kawahara","Yasuaki Kakehi"],series:"DIS 2019",doi:"https://doi.org/10.1145/3322276.3322337",keywords:"shape-changing interfaces, programming by demonstration, soft robots, pneumatic actuation, tangible interactions",award:"Best Paper",video:"https://www.youtube.com/watch?v=ZkCcazfFD-M",pages:11,abstract:"We introduce MorphIO, entirely soft sensing and actuation modules for programming by demonstration of soft robots and shape-changing interfaces.MorphIO's hardware consists of a soft pneumatic actuator containing a conductive sponge sensor.This allows both input and output of three-dimensional deformation of a soft material.Leveraging this capability, MorphIO enables a user to record and later playback physical motion of programmable shape-changing materials.In addition, the modular design of MorphIO's unit allows the user to construct various shapes and topologies through magnetic connection.We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects.Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.",dir:"content/output/publications",base:"dis-2019-nakayama.json",ext:".json",sourceBase:"dis-2019-nakayama.yaml",sourceExt:".yaml"},"content/output/publications/gi-2021-mactavish.json":{date:"2021-05",title:"Perspective Charts",authors:["Mia MacTavish","Katayoon Etemad","Faramarz Samavati","Wesley Willett"],series:"GI 2021",doi:"http://hdl.handle.net/1880/113671",keywords:"information visualization",video:"https://www.youtube.com/watch?v=Sp4Vt8mMhCs&list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-",pages:10,abstract:"We introduce three novel data visualizations, called perspective charts, based on the concept of size constancy in linear perspective projection. Bar charts are a popular and commonly used tool for the interpretation of datasets, however, representing datasets with multi-scale variation is challenging in a bar chart due to limitations in viewing space. Each of our designs focuses on the static representation of datasets with large ranges with respect to important variations in the data. Through a user study, we measure the effectiveness of our designs for representing these datasets in comparison to traditional methods, such as a standard bar chart or a broken-axis bar chart, and state-of-the-art methods, such as a scale-stack bar chart. The evaluation reveals that our designs allow pieces of data to be visually compared at a level of accuracy similar to traditional visualizations. Our designs demonstrate advantages when compared to state-of-the-art visualizations designed to represent datasets with large outliers.",dir:"content/output/publications",base:"gi-2021-mactavish.json",ext:".json",sourceBase:"gi-2021-mactavish.yaml",sourceExt:".yaml"},"content/output/publications/dis-2021-wannamaker.json":{date:"2021-06",title:"I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking",authors:["Kendra Wannamaker","Sandeep Kollannur","Marian Dörk","Wesley Willett"],series:"DIS 2021",doi:"http://hdl.handle.net/1880/113555",keywords:"information visualization, personal informatics, situated visualization",pages:10,talk:"https://www.youtube.com/watch?v=yhMKURtgFZ0",abstract:"We present I/O Bits, a prototype personal informatics system that explores the potential for user-driven and situated self-tracking. With simple tactile inputs and small e-paper visualizations, I/O Bits are dedicated physical devices that allow individuals to track and visualize different kinds of personal activities in-situ. This is in contrast to most self-tracking systems, which automate data collection, centralize information displays, or integrate into multi-purpose devices like smartwatches or mobile phones. We report findings from an e-paper visualization workshop and a prototype deployment where participants constructed their own I/O Bits and used them to track a range of personal data. Based on these experiences, we contribute insights and opportunities for situated and user-driven personal informatics.",dir:"content/output/publications",base:"dis-2021-wannamaker.json",ext:".json",sourceBase:"dis-2021-wannamaker.yaml",sourceExt:".yaml"},"content/output/publications/dis-2018-mikalauskas.json":{date:"2018-06",title:"Improvising with an Audience-Controlled Robot Performer",authors:["Claire Mikalauskas","Tiffany Wun","Kevin Ta","Joshua Horacsek","Lora Oehlberg"],series:"DIS 2018",doi:"https://doi.org/10.1145/3196709.3196757",pages:10,keywords:"human-robot interaction, improvised theatre, creativity-support tools, crowdsourcing",video:"https://youtu.be/ORN9jljPncc",abstract:"In improvisational theatre (improv), actors perform unscripted scenes together, collectively creating a narrative. Audience suggestions introduce randomness and build audience engagement, but can be challenging to mediate at scale. We present Robot Improv Puppet Theatre (RIPT), which includes a performance robot (Pokey) who performs gestures and dialogue in short-form improv scenes based on audience input from a mobile interface. We evaluated RIPT in several initial informal performances, and in a rehearsal with seven professional improvisers. The improvisers noted how audience prompts can have a big impact on the scene - highlighting the delicate balance between ambiguity and constraints in improv. The open structure of RIPT performances allows for multiple interpretations of how to perform with Pokey, including one-on-one conversations or multi-performer scenes. While Pokey lacks key qualities of a good improviser, improvisers found his serendipitous dialogue and gestures particularly rewarding.",dir:"content/output/publications",base:"dis-2018-mikalauskas.json",ext:".json",sourceBase:"dis-2018-mikalauskas.yaml",sourceExt:".yaml"},"content/output/publications/iros-2020-hedayati.json":{date:"2020-09",title:"PufferBot: Actuated Expandable Structures for Aerial Robots",authors:["Hooman Hedayati","Ryo Suzuki","Daniel Leithinger","Daniel Szafir"],series:"IROS 2020",video:"https://www.youtube.com/watch?v=XtPepCxWcBg",pages:6,abstract:"We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drone’s propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique charac- terizations which may be useful in different situations. Finally, we present three motivating scenarios in which PufferBot might be useful beyond existing static propeller guard structures.",dir:"content/output/publications",base:"iros-2020-hedayati.json",ext:".json",sourceBase:"iros-2020-hedayati.yaml",sourceExt:".yaml"},"content/output/publications/imwut-2020-wang.json":{date:"2020-06",title:"AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters",authors:["Xiyue Wang","Kazuki Takashima","Tomoaki Adachi","Patrick Finn","Ehud Sharlin","Yoshifumi Kitamura"],series:"IMWUT 2020",doi:"https://doi.org/10.1145/3381016",keywords:"well being, toy blocks, PTSD, tangibles for health, stress assessment, play, children",pages:29,video:"https://www.youtube.com/watch?v=fxxvZBY80ug",abstract:"Natural disasters cause long-lasting mental health problems such as PTSD in children. Following the 2011 Earthquake and Tsunami in Japan, we witnessed a shift of toy block play behavior in young children who suffered from stress after the disaster. The behavior reflected their emotional responses to the traumatic event. In this paper, we explore the feasibility of using data captured from block-play to assess children's stress after a major natural disaster. We prototyped sets of sensor-embedded toy blocks, AssessBlocks, that automate quantitative play data acquisition. During a three-year period, the blocks were dispatched to fifty-two post-disaster children. Within a free play session, we captured block features, a child's playing behavior, and stress evaluated by several methods. The result from our analysis reveal correlations between block play features and stress measurements and show initial promise of using the effectiveness of using AssessBlocks to assess children's stress after a disaster. We provide detailed insights into the potential as well as the challenges of our approach and unique conditions. From these insights we summarize guidelines for future research in automated play assessment systems that support children's mental health.",dir:"content/output/publications",base:"imwut-2020-wang.json",ext:".json",sourceBase:"imwut-2020-wang.yaml",sourceExt:".yaml"},"content/output/publications/mobilehci-2015-ledo.json":{date:"2015-08",title:"Proxemic-Aware Controls: Designing Remote Controls for Ubiquitous Computing Ecologies",authors:["David Ledo","Saul Greenberg","Nicolai Marquardt","Sebastian Boring"],series:"MobileHCI 2015",doi:"https://doi.org/10.1145/2785830.2785871",keywords:"ubiquitous computing, proxemic-interaction, mobile interaction, control of appliances",video:"https://www.youtube.com/watch?v=1AlMUmD6E3U",pages:10,abstract:"Remote controls facilitate interactions at-a-distance with appliances. However, the complexity, diversity, and increasing number of digital appliances in ubiquitous computing ecologies make it increasingly difficult to: (1) discover which appliances are controllable; (2) select a particular appliance from the large number available; (3) view information about its status; and (4) control the appliance in a pertinent manner. To mitigate these problems we contribute proxemic-aware controls, which exploit the spatial relationships between a person's handheld device and all surrounding appliances to create a dynamic appliance control interface. Specifically, a person can discover and select an appliance by the way one orients a mobile device around the room, and then progressively view the appliance's status and control its features in increasing detail by simply moving towards it. We illustrate proxemic-aware controls of assorted appliances through various scenarios. We then provide a generalized conceptual framework that informs future designs of proxemic-aware controls.",dir:"content/output/publications",base:"mobilehci-2015-ledo.json",ext:".json",sourceBase:"mobilehci-2015-ledo.yaml",sourceExt:".yaml"},"content/output/publications/mobilehci-2019-hung.json":{date:"2019-10",title:"WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction",authors:["Michael Hung","David Ledo","Lora Oehlberg"],series:"MobileHCI 2019",doi:"https://doi.org/10.1145/3338286.3340122",keywords:"smartwatch, cross-device interaction, pen interaction, interaction techniques",video:"https://www.youtube.com/watch?v=ilyJBzTzQAA",pages:8,abstract:"Pen-based input is often treated as auxiliary to mobile devices. We posit that cross-device interactions can inspire and extend the design space of pen-based interactions into new, expressive directions. We realize this through WatchPen, a smartwatch mounted on a passive, capacitive stylus that: (1) senses the usage context and leverages it for expression (e.g., changing colour), (2) contains tools and parameters within the display, and (3) acts as an on-demand output. As a result, it provides users with a dynamic relationship between inputs and outputs, awareness of current tool selection and parameters, and increased expressive match (e.g., added ability to mimic physical tools, showing clipboard contents). We discuss and reflect upon a series of interaction techniques that demonstrate WatchPen within a drawing application. We highlight the expressive power of leveraging multiple sensing and output capabilities across both the watch-augmented stylus and the tablet surface.",dir:"content/output/publications",base:"mobilehci-2019-hung.json",ext:".json",sourceBase:"mobilehci-2019-hung.yaml",sourceExt:".yaml"},"content/output/publications/nime-2020-ko.json":{date:"2020-07",title:"Touch Responsive Augmented Violin Interface System II: Integrating Sensors into a 3D Printed Fingerboard",authors:["Chantelle Ko","Lora Oehlberg"],series:"NIME 2020",keywords:"violin, touch sensor, FSR, fingerboard, augmented, 3D printing, conductive filament, interactive",talk:"https://www.youtube.com/watch?v=INmDzkcIO14",pages:13,abstract:"We present TRAVIS II, an augmented acoustic violin with touch sensors integrated into its 3D printed fingerboard that track left-hand finger gestures in real time. The fingerboard has four strips of conductive PLA filament which produce an electric signal when fingers press down on each string. While these sensors are physically robust, they are mechanically assembled and thus easy to replace if damaged. The performer can also trigger presets via four FSRs attached to the body of the violin. The instrument is completely wireless, giving the performer the freedom to move throughout the performance space. While the sensing fingerboard is installed in place of the traditional fingerboard, all other electronics can be removed from the augmented instrument, maintaining the aesthetics of a traditional violin. Our design allows violinists to naturally create music for interactive performance and improvisation without requiring new instrumental techniques. In this paper, we describe the design of the instrument, experiments leading to the sensing fingerboard, and performative applications of the instrument.",dir:"content/output/publications",base:"nime-2020-ko.json",ext:".json",sourceBase:"nime-2020-ko.yaml",sourceExt:".yaml"},"content/output/publications/sui-2017-li.json":{date:"2017-10",title:"Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models",authors:["Nico Li","Wesley Willett","Ehud Sharlin","Mario Costa Sousa"],series:"SUI 2017",doi:"https://doi.org/10.1145/3131277.3132178",keywords:"terrain visualization, geospatial visualization, dynamic viewshed, topographic maps, tangible user interfaces",pages:9,video:"https://vimeo.com/275404995",talk:"https://www.youtube.com/watch?v=aVXUojoQF60",abstract:"We compare the effectiveness of 2D maps and 3D terrain models for visibility tasks and demonstrate how interactive dynamic viewsheds can improve performance for both types of terrain representations. In general, the two-dimensional nature of classic topographic maps limits their legibility and can make complex yet typical cartographic tasks like determining the visibility between locations difficult. Both 3D physical models and interactive techniques like dynamic viewsheds have the potential to improve viewers' understanding of topography, but their impact has not been deeply explored. We evaluate the effectiveness of 2D maps, 3D models, and interactive viewsheds for both simple and complex visibility tasks. Our results demonstrate the benefits of the dynamic viewshed technique and highlight opportunities for additional tactile interactions. Based on these findings we present guidelines for improving the design and usability of future topographic maps and models.",dir:"content/output/publications",base:"sui-2017-li.json",ext:".json",sourceBase:"sui-2017-li.yaml",sourceExt:".yaml"},"content/output/publications/tei-2016-somanath.json":{date:"2018-06",title:"Engaging 'At-Risk' Students through Maker Culture Activities",authors:["Sowmya Somanath","Laura Morrison","Janette Hughes","Ehud Sharlin","Mario Costa Sousa"],series:"TEI 2016",doi:"https://doi.org/10.1145/2839462.2839482",keywords:"DIY, 'at-risk' students, maker culture, education, young learners",pages:9,abstract:"This paper presents a set of lessons learnt from introducing maker culture and DIY paradigms to 'at-risk' students (age 12-14). Our goal is to engage 'at-risk' students through maker culture activities. While improved technology literacy is one of the outcomes we also wanted the learners to use technology to realize concepts and ideas, and to gain freedom of thinking similar to creators, artists and designers. We present our study and a set of high level suggestions to enable thinking about how maker culture activities can facilitate engagement and creative use of technology by 1) thinking about creativity in task, 2) facilitating different entry points, 3) the importance of personal relevance, and 4) relevance to education.",dir:"content/output/publications",base:"tei-2016-somanath.json",ext:".json",sourceBase:"tei-2016-somanath.yaml",sourceExt:".yaml"},"content/output/publications/dis-2019-mahadevan.json":{date:"2019-06",title:"AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator",authors:["Karthik Mahadevan","Elaheh Sanoubari","Sowmya Somanath","James E. Young","Ehud Sharlin"],series:"DIS 2019",doi:"https://doi.org/10.1145/3322276.3322328",keywords:"mixed traffic, pedestrian simulator, autonomous vehicle-pedestrian interaction",pages:12,abstract:"AV-pedestrian interaction will impact pedestrian safety, etiquette, and overall acceptance of AV technology. Evaluating AV-pedestrian interaction is challenging given limited availability of AVs and safety concerns. These challenges are compounded by \"mixed traffic\" conditions: studying AV-pedestrian interaction will be difficult in traffic consisting of vehicles varying in autonomy level. We propose immersive pedestrian simulators as design tools to study AV-pedestrian interaction, allowing rapid prototyping and evaluation of future AV-pedestrian interfaces. We present OnFoot: a VR-based simulator that immerses participants in mixed traffic conditions and allows examination of their behavior while controlling vehicles' autonomy-level, traffic and street characteristics, behavior of other virtual pedestrians, and integration of novel AV-pedestrian interfaces. We validated OnFoot against prior simulators and Wizard-of-Oz studies, and conducted a user study, manipulating vehicles' autonomy level, interfaces, and pedestrian group behavior. Our findings highlight the potential to use VR simulators as powerful tools for AV-pedestrian interaction design in mixed traffic.",dir:"content/output/publications",base:"dis-2019-mahadevan.json",ext:".json",sourceBase:"dis-2019-mahadevan.yaml",sourceExt:".yaml"},"content/output/publications/tei-2019-wun.json":{date:"2019-03",title:"You say Potato, I say Po-Data: Physical Template Tools for Authoring Visualizations",authors:["Tiffany Wun","Lora Oehlberg","Miriam Sturdee","Sheelagh Carpendale"],series:"TEI 2019",doi:"https://doi.org/10.1145/3294109.3295627",keywords:"potato, tangible tools, authoring visualizations, block-printing, physical template tools, information visualization",pages:10,abstract:"Providing data visualization authoring tools for the general public remains an ongoing challenge. Inspired by block-printing, we explore how visualization stamps as a physical visualization authoring tool could leverage both visual freedom and ease of repetition. We conducted a workshop with two groups---visualization experts and non-experts---where participants authored visualizations on paper using hand-carved stamps made from potatoes and sponges. The low-fidelity medium freed participants to test new stamp patterns and accept mistakes. From the created visualizations, we observed several unique traits and uses of block-printing tools for visualization authoring, including: modularity of patterns, annotation guides, creation of multiple patterns from one stamp, and various techniques to apply data onto paper. We discuss the issues around expressivity and effectiveness of block-printed stamps in visualization authoring, and identify implications for the design and assembly of primitives in potential visualization stamp kits, as well as applications for future use in non-digital environments.",dir:"content/output/publications",base:"tei-2019-wun.json",ext:".json",sourceBase:"tei-2019-wun.yaml",sourceExt:".yaml"},"content/output/publications/hri-2018-feick.json":{date:"2018-04",title:"The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration",authors:["Martin Feick","Lora Oehlberg","Anthony Tang","André Miede","Ehud Sharlin"],series:"HRI 2018",doi:"https://doi.org/10.1145/3173386.3176959",pages:2,keywords:"movement trajectory & velocity, remote collaboration, robot surrogate",abstract:"In this paper, we discuss the role of the movement trajectory and velocity enabled by our tele-robotic system (ReMa) for remote collaboration on physical tasks. Our system reproduces changes in object orientation and position at a remote location using a humanoid robotic arm. However, even minor kinematics differences between robot and human arm can result in awkward or exaggerated robot movements. As a result, user communication with the robotic system can become less efficient, less fluent and more time intensive.",dir:"content/output/publications",base:"hri-2018-feick.json",ext:".json",sourceBase:"hri-2018-feick.yaml",sourceExt:".yaml"},"content/output/people/paul-saulnier.json":{name:"Paul Saulnier",type:"alumni",past:"master",url:"http://paulsaulnier.com/",dir:"content/output/people",base:"paul-saulnier.json",ext:".json",sourceBase:"paul-saulnier.yaml",sourceExt:".yaml"},"content/output/publications/tei-2019-mikalauskas.json":{date:"2019-03",title:"Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology",authors:["Claire Mikalauskas","April Viczko","Lora Oehlberg"],series:"TEI 2019",doi:"https://doi.org/10.1145/3294109.3295631",pages:9,keywords:"props, performer-controlled technology, improvisational theatre",abstract:"While improvised theatre (improv) is often performed on a bare stage, improvisers sometimes incorporate physical props to inspire new directions for a scene and to enrich their performance. A tech booth can improvise light and sound technical elements, but coordinating with improvisers' actions on-stage is challenging. Our goal is to inform the design of an augmented prop that lets improvisers tangibly control light and sound technical elements while performing. We interviewed five professional improvisers about their use of physical props in improv, and their expectations of a possible augmented prop that controls technical theatre elements. We propose a set of guidelines for the design of an augmented prop that fits with the existing world of unpredictable improvised performance.",dir:"content/output/publications",base:"tei-2019-mikalauskas.json",ext:".json",sourceBase:"tei-2019-mikalauskas.yaml",sourceExt:".yaml"},"content/output/publications/chi-2018-suzuki.json":{date:"2018-05",title:"Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation",authors:["Ryo Suzuki","Jun Kato","Mark D. Gross","Tom Yeh"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3173773",keywords:"direct manipulation, tangible programming, swarm user interfaces, programming by demonstration",video:"https://www.youtube.com/watch?v=Gb7brajKCVE",pages:13,abstract:"We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high-level interface design. Inspired by current UI programming practices, we introduce a four-step workflow-create elements, abstract attributes, specify behaviors, and propagate changes-for Swarm UI programming. We propose a set of direct physical manipulation techniques to support each step in this workflow. To demonstrate these concepts, we developed Reactile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies-an in-class survey with 148 students and a lab interview with eight participants-confirm that our approach is intuitive and understandable for programming Swarm UIs.",dir:"content/output/publications",base:"chi-2018-suzuki.json",ext:".json",sourceBase:"chi-2018-suzuki.yaml",sourceExt:".yaml"},"content/output/publications/tvcg-2017-goffin.json":{date:"2017-01",title:"An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents",authors:["Pascal Goffin","Jeremy Boy","Wesley Willett","Petra Isenberg"],series:"TVCG 2017",doi:"https://doi.org/10.1109/TVCG.2016.2618797",keywords:"word-scale visualization, word-scale graphic, text visualization, sparklines, authoring tool, information visualization",pages:13,video:"https://vimeo.com/230834366",abstract:"We contribute an investigation of the design and function of word-scale graphics and visualizations embedded in text documents. Word-scale graphics include both data-driven representations such as word-scale visualizations and sparklines, and non-data-driven visual marks. Their design, function, and use has so far received little research attention. We present the results of an open ended exploratory study with nine graphic designers. The study resulted in a rich collection of different types of graphics, data provenance, and relationships between text, graphics, and data. Based on this corpus, we present a systematic overview of word-scale graphic designs, and examine how designers used them. We also discuss the designers' goals in creating their graphics, and characterize how they used word-scale graphics to visualize data, add emphasis, and create alternative narratives. Building on these examples, we discuss implications for the design of authoring tools for word-scale graphics and visualizations, and explore how new authoring environments could make it easier for designers to integrate them into documents.",dir:"content/output/publications",base:"tvcg-2017-goffin.json",ext:".json",sourceBase:"tvcg-2017-goffin.yaml",sourceExt:".yaml"},"content/output/publications/tvcg-2017-willett.json":{date:"2017-01",title:"Embedded Data Representations",authors:["Wesley Willett","Yvonne Jansen","Pierre Dragicevic"],series:"TVCG 2017",doi:"https://doi.org/10.1109/TVCG.2016.2598608",keywords:"information visualization, data physicalization, ambient displays, ubiquitous computing, augmented reality",pages:10,video:"https://vimeo.com/182971005",talk:"https://www.youtube.com/watch?v=ZS7lU60xChI",abstract:"We introduce embedded data representations, the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents – the real-world entities and spaces to which data corresponds – and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations, which display data in proximity to data referents, and embedded representations, which display data so that it spatially coincides with data referents. Drawing on examples from visualization, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and physicalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.",dir:"content/output/publications",base:"tvcg-2017-willett.json",ext:".json",sourceBase:"tvcg-2017-willett.yaml",sourceExt:".yaml"},"content/output/publications/tei-2019-tolley.json":{date:"2019-03",title:"WindyWall: Exploring Creative Wind Simulations",authors:["David Tolley","Thi Ngoc Tram Nguyen","Anthony Tang","Nimesha Ranasinghe","Kensaku Kawauchi","Ching-Chiuan Yen"],series:"TEI 2019",doi:"https://doi.org/10.1145/3294109.3295624",keywords:"tactile/haptic interaction, multimodal interaction, novel actuators/displays",video:"https://www.youtube.com/watch?v=Tn11UmsOsTE",pages:10,abstract:'Wind simulations are typically one-off implementations for specific applications. We introduce WindyWall, a platform for creative design and exploration of wind simulations. WindyWall is a three-panel 90-fan array that encapsulates users with 270? of wind coverage. We describe the design and implementation of the array panels, discussing how the panels can be re-arranged, where various wind simulations can be realized as simple effects. To understand how people perceive "wind" generated from WindyWall, we conducted a pilot study of wind magnitude perception using different wind activation patterns from WindyWall. Our findings suggest that: horizontal wind activations are perceived more readily than vertical ones, and that people\'s perceptions of wind are highly variable-most individuals will rate airflow differently in subsequent exposures. Based on our findings, we discuss the importance of developing a method for characterizing wind simulations, and provide design directions for others using fan arrays to simulate wind.',dir:"content/output/publications",base:"tei-2019-tolley.json",ext:".json",sourceBase:"tei-2019-tolley.yaml",sourceExt:".yaml"},"content/output/publications/tvcg-2019-walny.json":{date:"2019-08",title:"Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff",authors:["Jagoda Walny","Christian Frisson","Mieka West","Doris Kosminsky","Søren Knudsen","Sheelagh Carpendale","Wesley Willett"],series:"TVCG 2019",doi:"https://doi.org/10.1109/TVCG.2019.2934538",keywords:"information visualization, design handoff, data mapping, design process",award:"Best Paper",pages:10,video:"https://vimeo.com/360483702",talk:"https://vimeo.com/368703151",abstract:"Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.",dir:"content/output/publications",base:"tvcg-2019-walny.json",ext:".json",sourceBase:"tvcg-2019-walny.yaml",sourceExt:".yaml"},"content/output/publications/chi-2018-mahadevan.json":{date:"2018-04",title:"Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction",authors:["Karthik Mahadevan","Sowmya Somanath","Ehud Sharlin"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3174003",keywords:"autonomous vehicle-pedestrian interaction, perceived awareness and intent in autonomous vehicles",pages:12,video:"https://www.youtube.com/watch?v=D_hhcGVREGA",talk:"https://www.youtube.com/watch?v=08OEKuz93dY",abstract:"Drivers use nonverbal cues such as vehicle speed, eye gaze, and hand gestures to communicate awareness and intent to pedestrians. Conversely, in autonomous vehicles, drivers can be distracted or absent, leaving pedestrians to infer awareness and intent from the vehicle alone. In this paper, we investigate the usefulness of interfaces (beyond vehicle movement) that explicitly communicate awareness and intent of autonomous vehicles to pedestrians, focusing on crosswalk scenarios. We conducted a preliminary study to gain insight on designing interfaces that communicate autonomous vehicle awareness and intent to pedestrians. Based on study outcomes, we developed four prototype interfaces and deployed them in studies involving a Segway and a car. We found interfaces communicating vehicle awareness and intent: (1) can help pedestrians attempting to cross; (2) are not limited to the vehicle and can exist in the environment; and (3) should use a combination of modalities such as visual, auditory, and physical.",dir:"content/output/publications",base:"chi-2018-mahadevan.json",ext:".json",sourceBase:"chi-2018-mahadevan.yaml",sourceExt:".yaml"},"content/output/publications/tvcg-2016-lopez.json":{date:"2016-05",title:"Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration",authors:["David Lopez","Lora Oehlberg","Candemir Doger","Tobias Isenberg"],series:"TVCG 2016",doi:"https://doi.org/10.1109/TVCG.2015.2440233",keywords:"visualization of 3D data, human-computer interaction, expert interaction, direct-touch input, mobile displays, stereoscopic environments, VR, AR, conceptual model of interaction, interaction reference frame mapping, observational study",video:"https://www.youtube.com/watch?v=jBtHgTYpJl0",talk:"https://vimeo.com/245846750",pages:13,abstract:"We discuss touch-based navigation of 3D visualizations in a combined monoscopic and stereoscopic viewing environment. We identify a set of interaction modes, and a workflow that helps users transition between these modes to improve their interaction experience. In our discussion we analyze, in particular, the control-display space mapping between the different reference frames of the stereoscopic and monoscopic displays. We show how this mapping supports interactive data exploration, but may also lead to conflicts between the stereoscopic and monoscopic views due to users' movement in space; we resolve these problems through synchronization. To support our discussion, we present results from an exploratory observational evaluation with domain experts in fluid mechanics and structural biology. These experts explored domain-specific datasets using variations of a system that embodies the interaction modes and workflows; we report on their interactions and qualitative feedback on the system and its workflow.",dir:"content/output/publications",base:"tvcg-2016-lopez.json",ext:".json",sourceBase:"tvcg-2016-lopez.yaml",sourceExt:".yaml"},"content/output/publications/uist-2020-suzuki.json":{date:"2020-10",title:"RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching",authors:["Ryo Suzuki","Rubaiat Habib Kazi","Li-Yi Wei","Stephen DiVerdi","Wilmot Li","Daniel Leithinger"],series:"UIST 2020",doi:"https://doi.org/10.1145/3379337.3415892",keywords:"augmented reality, embedded data visualization, real-time authoring, sketching interfaces, tangible interaction",award:"Honorable Mention",video:"https://www.youtube.com/watch?v=L0p-BNU9rXU",pages:16,abstract:"We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.",dir:"content/output/publications",base:"uist-2020-suzuki.json",ext:".json",sourceBase:"uist-2020-suzuki.yaml",sourceExt:".yaml"},"content/output/publications/vr-2019-satriadi.json":{date:"2019-03",title:"Augmented Reality Map Navigation with Freehand Gestures",authors:["Kadek Ananta Satriadi","Barrett Ens","Maxime Cordeil","Bernhard Jenny","Tobias Czauderna","Wesley Willett"],series:"IEEE VR 2019",doi:"https://doi.org/10.1109/VR.2019.8798340",keywords:"augmented reality, gesture recognition, human computer interaction, interactive devices",pages:11,video:"https://www.youtube.com/watch?v=TE6AJEu8zdY",talk:"https://www.youtube.com/watch?v=jNeEbB3sTn0",abstract:"Freehand gesture interaction has long been proposed as a `natural' input method for Augmented Reality (AR) applications, yet has been little explored for intensive applications like multiscale navigation. In multiscale navigation, such as digital map navigation, pan and zoom are the predominant interactions. A position-based input mapping (e.g. grabbing metaphor) is intuitive for such interactions, but is prone to arm fatigue. This work focuses on improving digital map navigation in AR with mid-air hand gestures, using a horizontal intangible map display. First, we conducted a user study to explore the effects of handedness (unimanual and bimanual) and input mapping (position-based and rate-based). From these findings we designed DiveZoom and TerraceZoom, two novel hybrid techniques that smoothly transition between position- and rate-based mappings. A second user study evaluated these designs. Our results indicate that the introduced input-mapping transitions can reduce perceived arm fatigue with limited impact on performance.",dir:"content/output/publications",base:"vr-2019-satriadi.json",ext:".json",sourceBase:"vr-2019-satriadi.yaml",sourceExt:".yaml"},"content/output/publications/uist-2020-yixian.json":{date:"2020-10",title:"ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World",authors:["Yan Yixian","Kazuki Takashima","Anthony Tang","Takayuki Tanno","Kazuyuki Fujita","Yoshifumi Kitamura"],series:"UIST 2020",doi:"https://doi.org/10.1145/3379337.3415859",keywords:"encountered-type haptic devices, immersive experience",video:"https://www.youtube.com/watch?v=j2iSNDkBxAY",pages:13,abstract:"We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls---autonomous robotic encounter-type haptic wall-shaped props---that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user's movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of autonomous mobile reusable props, which we view as an important design direction for haptics in VR.",dir:"content/output/publications",base:"uist-2020-yixian.json",ext:".json",sourceBase:"uist-2020-yixian.yaml",sourceExt:".yaml"},"content/output/seminar.json":[{date:"2021-05-03",name:"Stefanie Mueller",affiliation:"MIT",url:"https://hcie.csail.mit.edu/stefanie-mueller.html",title:"Advancing Personal Fabrication by Making Physical Objects as Reprogrammable as Digital Data",abstract:"Computing has revolutionized how we process and interact with data today, unfortunately, these capabilities are constraint to the digital realm and cannot yet be applied to physical matter. For instance, today, we can already quickly update the appearance of a digital photo by applying a filter or adding and removing elements. However, updating physical objects in the same way is not possible today. In this talk, I will show my research group’s latest developments that bring us closer to a future in which physical objects are as reprogrammable as data is today. As a first example of this, I will show our research on a new reprogrammable material that can be applied to the surface of physical objects and that allows them to change their appearance within a few minutes. This allows us to update the color of clothing, shoes, and even entire rooms in the same way as we can update a digital photo today. I will then show additional developments that extend this concept to further integrate computing capabilities into physical objects, show our research on how we can print functional objects in one go without the need for assembly, and demonstrate how we can create unified prototyping environments that support engineers and designers in fabricating new types of physical objects.",bio:"Stefanie Mueller is the X-Career Development Assistant Professor in the MIT EECS department joint with MIT Mechanical Engineering and Head of the HCI Engineering Group at MIT CSAIL. For her research, Stefanie has received an NSF CAREER Award, an Alfred P. Sloan Fellowship, a Microsoft Research Faculty Fellowship, and was also named a Forbes 30 under 30 in Science. In addition, Stefanie’s work has been awarded several Best Paper and Honorable Mention Awards at the ACM CHI and ACM UIST conferences, the premier venues in Human-Computer Interaction. Stefanie has also served as the Program Chair of the ACM UIST 2020 conference and was a Subcommittee Chair for ACM CHI 2019 and 2020. At MIT, Stefanie served as a Program Co-Chair for the MIT EECS Rising Star Workshop in 2018 and is currently serving as the Head of the Human Computer Interaction Communities of Research (HCI CoR) at MIT CSAIL."},{date:"2021-04-26",name:"Charles Perin",affiliation:"University of Victoria",url:"http://charlesperin.net/",title:"Beyond Visualization Wizardry: The Role of Interaction in Data Visualization",abstract:'Visualization is not just a way of creating pretty pictures and "intuitive dashboards". It is not a magic wand that you can apply to your dataset to automatically turn a data mess into "actionable insights for transformative results". Far from this wizardry, I argue that understanding data comes at the cost of interacting with it. I will go through several research projects - ranging from manual reordering of matrices to active reading of visualizations to interaction discoverability to composite physicalizations to direct manipulation of graphical encodings - in an attempt to convince you that we can, and should, find better ways for people to interact with data visualizations.',bio:"Charles Perin is an Assistant Professor of Computer Science at the University of Victoria, where he co-leads the Victoria Interactive eXperiences with Information research group specializing in Human-Computer Interaction and Information Visualization. He and his students are particularly interested in designing and studying new interactions for visualizations and in understanding how people may make use of and interact with visualizations in their everyday lives; in designing visualization tools for authoring personal visualizations and for exploring and communicating open data; in sports visualization; and in visualization beyond the desktop. Before joining the University of Victoria in 2018, Charles was a Lecturer at City, University of London, before that a post-doctoral researcher at the University of Calgary, before that a PhD student at University Paris-Sud/INRIA, and long before that a kid in Brittany."},{date:"2021-04-19",name:"Nicolai Marquardt",affiliation:"University College London",url:"http://www.nicolaimarquardt.com/",title:"Journey through the Design Space of Cross-Device Interactions",abstract:"Designing interfaces or applications that move beyond the bounds of a single device screen enables new ways to engage with digital content. In this talk, I will guide you through the design space of cross-device interactions. In particular, I will give an overview of what the research field of cross-device interactions looks like, and what kind of techniques we can use for designing fluid cross-device interactions. I’ll also discuss a few of the open issues in the research field and suggest opportunities of where we can go next.",bio:"Nicolai Marquardt is Associate Professor at the University College London, where he is part of the Department of Computer Science, Faculty of Engineering and the Faculty of Brain Sciences. At the UCL Interaction Centre, he works on projects in the research areas of cross-device interaction, sensor-based systems, prototyping toolkits, and design methods. He received his PhD in Computer Science from the University of Calgary, Canada. Nicolai is co-author of the Sketching User Experiences Workbook (Morgan Kaufmann 2011) and the Proxemic Interactions textbook (Morgan & Claypool 2015)."},{date:"2021-04-12",name:"Benjamin Bach",affiliation:"University of Edinburgh",url:"https://www.designinformatics.org/person/benjaminbach/",title:"The Immersive Canvas: Data Visualization and Interaction for Immersive Analytics",abstract:"This talk explores the role of interactive data visualization for immersive analytics. Immersive analytics is becoming a complex field that combines many fields of expertise: analytics, big data, infrastructure, virtual and augmented systems, image recognition and many others, as well as human-computer interaction and visualization. In order to make sense of complex data, we need visualization interfaces: in immersive environments, data visualization is freed of the limitedness of the traditional desktop screen; able to expand into an infinite canvas and the third dimension. What potential does this bring for data visualization and immersive visualization? How can we leverage this potential? How is this changing our approach to visualizing and interacting with data?",bio:"Dr Benjamin Bach is a Lecturer in Design Informatics and Visualization at the University of Edinburgh. His research designs and investigates interactive information visualization interfaces to help people explore, communicate, and understand data. Before joining the University of Edinburgh in 2017, Benjamin worked as a postdoc at Harvard University (Visual Computing Group), Monash University, as well as the Microsoft-Research Inria Joint Centre. Benjamin was visiting researcher at the University of Washington and Microsoft Research in 2015. He obtained his PhD in 2014 from the Université Paris Sud where he worked at the Aviz Group at Inria. The PhD thesis entitled Connections, Changes, and Cubes: Unfolding Dynamic Networks for Visual Exploration got awarded an honorable mention as the Best Thesis by the IEEE Visualization Committee."},{date:"2021-03-29",name:"James Young",affiliation:"University of Manitoba",url:"http://hci.cs.umanitoba.ca/people/bio/james-e.young",title:"Warning, This robot is not what it seems! A discussion on deception and the future of social robots",abstract:"Social robots are designed to interact with people using human- or animal-like language, gestures, or other techniques. This approach promises intuitive interaction, and can be designed to shape a person's mood and behavior; social robots can even serve as companions. However, I argue that social robots - by design - are fundamentally rooted in deception, which highlights real potential dangers as these robots enter society. On the flip side, considering this deception also provides a positive way forward, a path for developing social robots that can be successful in both our everyday lives.",bio:"Dr. James Young is a professor of computer science at the University of Manitoba."},{date:"2021-03-15",name:"Jessica Cauchard",affiliation:"Ben Gurion University",url:"https://scholar.google.com/citations?user=Rk1SDB8AAAAJ&hl=en",title:"On Body & Out of Body Interactions",abstract:"Mobile devices have become ubiquitous over the last decade, changing the way we interact with technology and with one another. Mobile devices were at first personal devices carried in our hands or pockets. They are now changing form to fit our lifestyles and an increasingly demanding amount and diversity of information to display. My research focuses on the design, development, and evaluation of novel interaction techniques with mobile devices using a human-centered approach. In this presentation, I will in particular focus on two types of mobile technologies: wearables and drones. I will discuss the use of multiple modalities to interact with technology and in particular how haptics on wearables can support long-term tasks without interrupting the user’s attention. I will then discuss how autonomous devices such as drones re-invent our understanding of ubiquitous computing and present my current research on collocated natural human-drone interaction.",bio:"Dr. Jessica Cauchard is a lecturer in the department of Industrial Engineering and Management at Ben Gurion University of the Negev in Israel, where she recently founded the Magic Lab. Her research is rooted in the fields of Human-Computer and Human-Robot Interaction with a focus on novel interaction techniques and ubiquitous computing. Previously, she was faculty of Computer Science at the Interdisciplinary Center Herzliya between 2017 and 2019. Before moving to Israel, Dr. Cauchard worked as a postdoctoral scholar at Stanford University. She has a strong interest in autonomous vehicles and intelligent devices and how they change our device ecology. She completed her PhD in Computer Science at the University of Bristol, UK in 2013 and received a Magic Grant for her work on interacting with drones by the Brown Institute for Media Innovation in 2015"},{date:"2021-03-08",name:"Alberto de Salvatierra",affiliation:"UCalgary School of Architecture, Landscape and Planning",url:"https://sapl.ucalgary.ca/about/people/alberto-de-salvatierra",title:"Soft Infrastructures",abstract:'The COVID-19 pandemic has exposed the fragility of existing models of housing, collective life, and infrastructure. The 99% have been disproportionately marginalized by shelter-in-place orders and quarantines that assume they have the resources to weather this moment of extreme instability. The transition from a quarantine to a post-pandemic city will not only be a fight for collective human health and wellbeing, but will also be the staging ground for our last stand to prevent a forthcoming climate catastrophe. New paradigms of urban design and civic infrastructure must be decoupled from society’s carbon-intensive practices and archaic fetishes for "solidity" in building. "Soft Infrastructures" present an alternative modality for urban design. This lecture will discuss three on-going projects by the Center for Civilization: Civic Commons Catalyst, Eternal Ephemera, and Soft City/Soft Haus.',bio:"Alberto de Salvatierra is an assistant professor of urbanism and data in architecture at the University of Calgary's School of Architecture, Planning and Landscape, director of the Center for Civilization—a design research lab and international think tank, the founding principal of PROXIIMA, and a Global Shaper at the Calgary Hub of the Global Shapers Community—an initiative by the World Economic Forum based in Geneva, Switzerland. An interdisciplinary polymath, architectural designer, and landscape urbanist, Alberto’s research and work focuses on material flows as infrastructure at the urban and civilizational scales, while his collaborative research agenda centers on fostering, developing and writing on interdisciplinary pedagogy and practices. His work has been published widely and exhibited both domestically and abroad, such as in the United States, the United Kingdom, Mexico, Italy, Japan, Sweden and Serbia, and in such venues as the Priscilla Fowler Fine Art Gallery in Las Vegas, NV, Calatrava-designed Milwaukee Art Museum in Milwaukee, WI, and the National Building Museum in Washington, D.C. In 2019, he was part of the Harvard Kennedy School’s inaugural STS (Science, Technology and Society) program on Expertise, Trust and Democracy, and an invited panelist and delegate to the United Nations."},{date:"2021-03-01",name:"Alicia Nahmad Vazquez",affiliation:"UCalgary School of Architecture, Landscape and Planning",url:"https://sapl.ucalgary.ca/about/people/alicia-nahmad",title:"Design in the Age of Intelligent Machines",bio:"Alicia Nahmad Vazquez is the founder of Architecture Extrapolated (R-Ex) and an assistant professor in robotics and AI at the University of Calgary School of Architecture Planning and Landscape (SAPL) . She is also co-director of the Laboratory for Integrative Design at UofC. For the past 5 years, Alicia worked as studio master at the Architectural Associational Design Research Laboratory (DRL) master’s program. As a research-based practising architect, Alicia explores materials and digital design and fabrication technologies along with the digitization of building trades and the wisdom of traditional building cultures. Her projects include the construction of award-winning ‘Knit-Candela’ and diverse collaborations with practice and academic institutions. She holds a PhD in human-robot collaborative (HRC) design from Cardiff University and a MArch from the AADRL. Alicia previously worked on developing design tools for practices like Populous and Zaha Hadid Architects. Alicia has also been an Artist-In-Residence at Autodesk Pier 9 and has taught and lectured extensively in Latin America and Europe. Her research has been widely published internationally in journals and conference proceedings."},{date:"2021-02-08",name:"Rubaiat Habib",affiliation:"Adobe Research",url:"https://rubaiathabib.me/",title:"Dynamic Graphics as a Language",abstract:'How can we make animation as easy as sketching? How can we create dynamic contents in real-time, in the speed of thought? How will dynamic graphics shape our real-time communications and language? In this talk, I\'m going to present my research on animation, storytelling, and design, including the design of Sketchbook Motion that was crowned as "The best iPad app of the year 2016" by Apple. Most of us experience the power of animated media every day: animation makes it easy to communicate complex ideas beyond verbal language. However, only few of us have the skills to express ourselves through this medium. By making animation as easy, accessible, and fluid as sketching, I intend to make dynamic graphics a powerful medium to think, create, and communicate rapidly.',bio:"Rubaiat Habib is a Sr. Research Scientist at Adobe Research. His research interest lies at the intersection of Computer Graphics and HCI for creative thinking, design, and storytelling. His research in dynamic drawings and animations turned into products that reach a global audience. Rubaiat received several awards for his work including Apple App of the year 2016, three ACM CHI Best Paper Nominations, ACM CHI and ACM UIST Peoples choice best talk awards, and ACM CHI Golden Mouse awards for best research videos. For his PhD at the National University of Singapore, Rubaiat also received a Microsoft Research Asia PhD fellowship. Prior to Adobe, he worked at Autodesk Research and Microsoft Research. rubaiathabib.me"},{date:"2021-01-25",name:"Xing-Dong Yang",affiliation:"Dartmouth College",url:"https://www.cs.dartmouth.edu/~xingdong/",title:"Creating Smart Everyday Things",abstract:"In my vision, the user interfaces of the future are in a blend of smart physical and virtual environments. My research focuses on the physical side by bringing interactivity to everyday things. I believe this vision is only achievable if people with varying backgrounds and abilities can work together in an accessible and collaborative environment. In this talk, I will describe two major threads of research in interactive everyday things and hardware prototyping tools. The first thread investigates interactive systems to sense the context of use of the things or estimate a user’s intention when touch input data is noisy. For example, I will demonstrate a tablecloth augmented with a fabric sensor that can sense and recognize non-metallic objects placed on a table, such as food, different types of fruits, liquids, plastic, and paper products. I will also show examples of how this technique can be used for contextual applications. The second thread investigates tools to lower the bar of entry to prototyping electronics, which is an essential skill needed to create smart everyday things. The goal of this line of work is to enable more people with varying backgrounds and abilities to create smart everyday things and eventually a better user experience of smart environments. For example, I will demonstrate an audio-tactile tutorial system for blind or low vision learners to understand circuit diagrams, which is an important task in the circuit prototyping pipeline. Both of these threads share a common goal that is to create a better user experience in smart environments.",bio:"Xing-Dong Yang is an Assistant Professor of Computer Science at Dartmouth College. His research is broadly in Human-Computer Interaction (HCI), where he creates interactive systems using sensing techniques and haptics to enable new applications in smart physical and virtual environments. Xing-Dong’s work is recognized through a Best Paper award at UIST 2019, eight Honorable Mention awards with one at UIST 2020, six at CHI (2010, 2016, 2018, 2019 × 2, 2020), and one at MobileHCI 2009. Aside from academic publications, Xing-Dong’s work attracts major public interest via news coverage from a variety of media outlets with different mediums, including TV (e.g., Discovery Daily Planet), print (e.g., The Wall Street Journal, Forbes), and Internet News (e.g., MIT Technology Review, New Scientist)."},{date:"2021-01-15",name:"Ken Nakagaki",affiliation:"MIT Media Lab",url:"https://www.ken-nakagaki.com/",title:"'Mechanical Shells' for Actuated Tangible UIs - Hybrid Architecture of Active and Passive Machines for Interaction Design",abstract:"Research on actuated and shape-changing Tangible User Interfaces (TUIs) in the field of HCI has been explored widely in the past few decades to enrich interaction with digital information in physical and dynamic ways. In this effort, various types of generic devices of actuated TUIs have been investigated including pin-based shape displays, actuated curve interfaces, and swarm user interfaces. While these approaches are intended to be dynamically reconfigurable to offer generic interactivity, each hardware is inherently limited to the fixed configurations. How can we further expand the versatility of the actuated TUIs for fully expanding their capability for tangible interactions and motion / shape representations?\n\nIn my talk, I propose a ‘mechanical shell’, a design concept for actuated TUIs with modular interchangeable components that extends and converts the shape, motion, and interactivity of the hardware. By doing so, compared with the actuated TUI itself, each mechanical shell would bring much more specialized and customized interactivity, while, as the whole architecture, the system can adapt to much more versatile interactions. I present two research instances that demonstrate this concept based-on pin-based shape display and swarm user interface, and introduce proof-of-concept implementation as well as a range of applications. By introducing the novel interaction architecture, my research envisions the future of the physical environment where active and passive machines exist together for enriching tangible and embodied interactions.",bio:"Ken is an interaction designer and HCI researcher from Japan. Currently, he is a Ph.D. Candidate of Tangible Media Group, MIT Media Lab. He is interested in developing interfaces that combine digital information or computational aids into daily physical tools and materials, to develop novel physical and perceptual experiences. His research has been presented in top HCI conferences (ACM CHI, UIST, TEI, etc), and demonstrated in various exhibitions and awards including Ars Electronica, A' Design Award, and Japan Media Arts Festival."}],"content/output/publications/dis-2018-ta.json":{date:"2018-06",title:"Bod-IDE: An Augmented Reality Sandbox for eFashion Garments",authors:["Kevin Ta","Ehud Sharlin","Lora Oehlberg"],series:"DIS 2018",doi:"https://doi.org/10.1145/3197391.3205408",keywords:"augmented reality, electronic fashion, creativity support tool",pages:5,abstract:"Electronic fashion (eFashion) garments use technology to augment the human body with wearable interaction. In developing ideas, eFashion designers need to prototype the role and behavior of the interactive garment in context; however, current wearable prototyping toolkits require semi-permanent construction with physical materials that cannot easily be altered. We present Bod-IDE, an augmented reality 'mirror' that allows eFashion designers to create virtual interactive garment prototypes. Designers can quickly build, refine, and test on-the-body interactions without the need to connect or program electronics. By envisioning interaction with the body in mind, eFashion designers can focus more on reimagining the relationship between bodies, clothing, and technology.",dir:"content/output/publications",base:"dis-2018-ta.json",ext:".json",sourceBase:"dis-2018-ta.yaml",sourceExt:".yaml"},"content/output/work-in-progress/chi-2020-asha.json":{date:"2020-05",title:"Views from the Wheelchair: Understanding Interaction between Autonomous Vehicle and Pedestrians with Reduced Mobility",authors:["Ashratuz Zavin Asha","Christopher Smith","Lora Oehlberg","Sowmya Somanath","Ehud Sharlin"],series:"CHI EA 2020",doi:"https://doi.org/10.1145/3334480.3383041",keywords:"pedestrian with reduced mobility, autonomous vehicle",pages:8,video:"https://www.youtube.com/watch?v=JNc49desa44",abstract:"We are interested in the ways pedestrians will interact with autonomous vehicle (AV) in a future AV transportation ecosystem, when nonverbal cues from the driver such as eye movements, hand gestures, etc. are no longer provided. In this work, we examine a subset of this challenge: interaction between pedestrian with reduced mobility (PRM) and AV. This study explores interface designs between AVs and people in a wheelchair to help them interact with AVs by conducting a preliminary design study. We have assessed the data collected from the study using qualitative analysis and presented different findings on AV-PRM interactions. Our findings reflect on the importance of visual interfaces, changes to the wheelchair and the creative use of the street infrastructure.",dir:"content/output/work-in-progress",base:"chi-2020-asha.json",ext:".json",sourceBase:"chi-2020-asha.yaml",sourceExt:".yaml"},"content/output/publications/tvcg-2019-blascheck.json":{date:"2019-06",title:"Exploration Strategies for Discovery of Interactivity in Visualizations",authors:["Tanja Blascheck","Lindsay MacDonald Vermeulen","Jo Vermeulen","Charles Perin","Wesley Willett","Thomas Ertl","Sheelagh Carpendale"],series:"TVCG 2019",doi:"https://doi.org/10.1109/TVCG.2018.2802520",keywords:"discovery, visualization, open data, evaluation, eye tracking, interaction logs, think-aloud",video:"https://vimeo.com/289789025",pages:13,abstract:"We investigate how people discover the functionality of an interactive visualization that was designed for the general public. While interactive visualizations are increasingly available for public use, we still know little about how the general public discovers what they can do with these visualizations and what interactions are available. Developing a better understanding of this discovery process can help inform the design of visualizations for the general public, which in turn can help make data more accessible. To unpack this problem, we conducted a lab study in which participants were free to use their own methods to discover the functionality of a connected set of interactive visualizations of public energy data. We collected eye movement data and interaction logs as well as video and audio recordings. By analyzing this combined data, we extract exploration strategies that the participants employed to discover the functionality in these interactive visualizations. These exploration strategies illuminate possible design directions for improving the discoverability of a visualization's functionality.",dir:"content/output/publications",base:"tvcg-2019-blascheck.json",ext:".json",sourceBase:"tvcg-2019-blascheck.yaml",sourceExt:".yaml"},"content/output/publications/tei-2020-suzuki.json":{date:"2020-02",title:"LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces",authors:["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],series:"TEI 2020",doi:"https://dl.acm.org/doi/10.1145/3374920.3374941",keywords:"shape-changing interfaces, inflatables, large-scale interactions",pages:9,video:"https://www.youtube.com/watch?v=0LHeTkOMR84",abstract:"Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore interactions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust, making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various applications. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.",dir:"content/output/publications",base:"tei-2020-suzuki.json",ext:".json",sourceBase:"tei-2020-suzuki.yaml",sourceExt:".yaml"},"content/output/publications/uist-2019-suzuki.json":{date:"2019-10",title:"ShapeBots: Shape-changing Swarm Robots",authors:["Ryo Suzuki","Clement Zheng","Yasuaki Kakehi","Tom Yeh","Ellen Yi-Luen Do","Mark D. Gross","Daniel Leithinger"],series:"UIST 2019",keywords:"swarm user interfaces, shape-changing user interfaces",doi:"https://doi.org/10.1145/3332165.3347911",video:"https://www.youtube.com/watch?v=cwPaof0kKdM",pages:13,abstract:"We introduce shape-changing swarm robots. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.",dir:"content/output/publications",base:"uist-2019-suzuki.json",ext:".json",sourceBase:"uist-2019-suzuki.yaml",sourceExt:".yaml"},"content/output/publications/uist-2018-suzuki.json":{date:"2018-10",title:"Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation",authors:["Ryo Suzuki","Junichi Yamaoka","Daniel Leithinger","Tom Yeh","Mark D. Gross","Yoshihiro Kawahara","Yasuaki Kakehi"],series:"UIST 2018",doi:"https://doi.org/10.1145/3242587.3242659",keywords:"digital materials, dynamic 3D printing, shape displays",video:"https://www.youtube.com/watch?v=92eGI-gYYc4","video-2":"https://www.youtube.com/watch?v=7nPlr3O9xu8",talk:"https://www.youtube.com/watch?v=R3FRUtOIiCQ",pages:16,abstract:"This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.",dir:"content/output/publications",base:"uist-2018-suzuki.json",ext:".json",sourceBase:"uist-2018-suzuki.yaml",sourceExt:".yaml"},"content/output/people/nour-hammad.json":{name:"Nour Hammad",type:"undergrad",dir:"content/output/people",base:"nour-hammad.json",ext:".json",sourceBase:"nour-hammad.yaml",sourceExt:".yaml"},"content/output/work-in-progress/dis-2018-ta.json":{date:"2018-06",title:"Bod-IDE: An Augmented Reality Sandbox for eFashion Garments",authors:["Kevin Ta","Ehud Sharlin","Lora Oehlberg"],series:"DIS 2018",doi:"https://doi.org/10.1145/3197391.3205408",keywords:"augmented reality, electronic fashion, creativity support tool",pages:5,abstract:"Electronic fashion (eFashion) garments use technology to augment the human body with wearable interaction. In developing ideas, eFashion designers need to prototype the role and behavior of the interactive garment in context; however, current wearable prototyping toolkits require semi-permanent construction with physical materials that cannot easily be altered. We present Bod-IDE, an augmented reality 'mirror' that allows eFashion designers to create virtual interactive garment prototypes. Designers can quickly build, refine, and test on-the-body interactions without the need to connect or program electronics. By envisioning interaction with the body in mind, eFashion designers can focus more on reimagining the relationship between bodies, clothing, and technology.",dir:"content/output/work-in-progress",base:"dis-2018-ta.json",ext:".json",sourceBase:"dis-2018-ta.yaml",sourceExt:".yaml"},"content/output/publications/uist-2021-suzuki.json":{date:"2021-10",title:"HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots",authors:["Ryo Suzuki","Eyal Ofek","Mike Sinclair","Daniel Leithinger","Mar Gonzalez-Franco"],series:"UIST 2021",doi:"https://doi.org/10.1145/3472749.3474821",keywords:"virtual reality, encountered-type haptics, tabletop mobile robots, swarm user interfaces",video:"https://www.youtube.com/watch?v=HTiZgOESJyQ",pages:16,abstract:"HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic ap- proaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in- time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various ap- plications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.",dir:"content/output/publications",base:"uist-2021-suzuki.json",ext:".json",sourceBase:"uist-2021-suzuki.yaml",sourceExt:".yaml"},"content/output/publications/dis-2019-bressa.json":{date:"2019-06",title:"Sketching and Ideation Activities for Situated Visualization Design",authors:["Nathalie Bressa","Kendra Wannamaker","Henrik Korsgaard","Wesley Willett","Jo Vermeulen"],series:"DIS 2019",doi:"https://doi.org/10.1145/3322276.3322326",keywords:"ideation, design workshops, situated visualization, information visualization, small displays, sketching",abstract:"We report on findings from seven design workshops that used ideation and sketching activities to prototype new situated visualizations - representations of data that are displayed in proximity to the physical referents (such as people, objects, and locations) to which the data is related. Designing situated visualizations requires a fine-grained understanding of the context in which the visualizations are placed, as well as an exploration of different options for placement and form factors, which existing methods for visualization design do not account for. Focusing on small displays as a target platform, we reflect on our experiences of using a diverse range of sketching activities, materials, and prompts. Based on these observations, we identify challenges and opportunities for sketching and ideating situated visualizations. We also outline the space of design activities for situated visualization and highlight promising methods for both designers and researchers.",dir:"content/output/publications",base:"dis-2019-bressa.json",ext:".json",sourceBase:"dis-2019-bressa.yaml",sourceExt:".yaml"},"content/output/vimeo.json":{182971005:"https://i.vimeocdn.com/video/592033369_640.webp",230834366:"https://i.vimeocdn.com/video/651490206_640.webp",275404995:"https://i.vimeocdn.com/video/707686230_640.webp",289789025:"https://i.vimeocdn.com/video/725516359_640.webp",360483702:"https://i.vimeocdn.com/video/814665539_640.webp",368703151:"https://i.vimeocdn.com/video/825448765_640.webp",dir:"content/output",base:"vimeo.json",ext:".json",sourceBase:"vimeo.yaml",sourceExt:".yaml"}},sourceFileArray:["content/booktitles.yaml","content/facility.yaml","content/labs.yaml","content/news.yaml","content/people/adnan-karim.yaml","content/people/anthony-chen.yaml","content/people/anthony-tang.yaml","content/people/april-zhang.yaml","content/people/ashratuz-zavin-asha.yaml","content/people/bon-adriel-aseniero.yaml","content/people/brennan-jones.yaml","content/people/carl-gutwin.yaml","content/people/carman-neustaedter.yaml","content/people/carmen-hull.yaml","content/people/charlotte-tang.yaml","content/people/christopher-rodriguez.yaml","content/people/christopher-smith.yaml","content/people/colin-auyeung.yaml","content/people/dane-bertram.yaml","content/people/darcy-norman.yaml","content/people/david-ledo.yaml","content/people/donald-cox.yaml","content/people/doug-schaeffer.yaml","content/people/edward-tse.yaml","content/people/ehud-sharlin.yaml","content/people/georgina-freeman.yaml","content/people/grace-ferguson.yaml","content/people/gregor-mcewan.yaml","content/people/harrison-chen.yaml","content/people/helen-ai-he.yaml","content/people/james-tam.yaml","content/people/jessi-stark.yaml","content/people/jiannan-li.yaml","content/people/karthik-mahadevan.yaml","content/people/kathryn-blair.yaml","content/people/kathryn-elliot-rounding.yaml","content/people/kaynen-mitchell.yaml","content/people/kendra-wannamaker.yaml","content/people/kimberly-tee.yaml","content/people/kurtis-danyluk.yaml","content/people/linda-tauscher.yaml","content/people/lora-oehlberg.yaml","content/people/manjot-khangura.yaml","content/people/manuel-rodriguez.yaml","content/people/marcus-friedel.yaml","content/people/mark-roseman.yaml","content/people/martin-feick.yaml","content/people/matthew-dunlap.yaml","content/people/miaosen-wang.yaml","content/people/michael-hung.yaml","content/people/micheal-boyle.yaml","content/people/micheal-nunes.yaml","content/people/micheal-rounding.yaml","content/people/nathalie-bressa.yaml","content/people/neil-chulpongsatorn.yaml","content/people/nicolai-marquardt.yaml","content/people/nour-hammad.yaml","content/people/paul-saulnier.yaml","content/people/roberta-cabral-mota.yaml","content/people/roberto-diaz-marino.yaml","content/people/ryo-suzuki.yaml","content/people/sabrina-lakhdhir.yaml","content/people/samin-farajian.yaml","content/people/sasha-ivanov.yaml","content/people/saul-greenberg.yaml","content/people/setareh-manesh.yaml","content/people/shaun-kaasten.yaml","content/people/sheelagh-carpendale.yaml","content/people/shivesh-jadon.yaml","content/people/soren-knudsen.yaml","content/people/sowmya-somanath.yaml","content/people/stephanie-smale.yaml","content/people/sydney-pratte.yaml","content/people/teddy-seyed.yaml","content/people/terrance-mok.yaml","content/people/theodore-ogrady.yaml","content/people/tian-xia.yaml","content/people/tim-au-yeung.yaml","content/people/wesley-willett.yaml","content/people/william-wright.yaml","content/people/yibo-sun.yaml","content/people/zachary-mckendrick.yaml","content/publications/assets-2017-suzuki.yaml","content/publications/chi-2015-aseniero.yaml","content/publications/chi-2015-jones.yaml","content/publications/chi-2015-oehlberg.yaml","content/publications/chi-2015-willett.yaml","content/publications/chi-2017-aoki.yaml","content/publications/chi-2017-hull.yaml","content/publications/chi-2017-ledo.yaml","content/publications/chi-2017-somanath.yaml","content/publications/chi-2018-dillman.yaml","content/publications/chi-2018-feick.yaml","content/publications/chi-2018-heshmat.yaml","content/publications/chi-2018-ledo.yaml","content/publications/chi-2018-mahadevan.yaml","content/publications/chi-2018-neustaedter.yaml","content/publications/chi-2018-oh.yaml","content/publications/chi-2018-suzuki.yaml","content/publications/chi-2018-wuertz.yaml","content/publications/chi-2019-danyluk.yaml","content/publications/chi-2020-anjani.yaml","content/publications/chi-2020-goffin.yaml","content/publications/chi-2020-hou.yaml","content/publications/chi-2020-suzuki.yaml","content/publications/chi-2021-danyluk.yaml","content/publications/chi-2021-ens.yaml","content/publications/chi-2021-hammad.yaml","content/publications/cnc-2019-hammad.yaml","content/publications/cupum-2021-rout.yaml","content/publications/dis-2016-jones.yaml","content/publications/dis-2017-mok.yaml","content/publications/dis-2018-mikalauskas.yaml","content/publications/dis-2018-pham.yaml","content/publications/dis-2018-ta.yaml","content/publications/dis-2019-bressa.yaml","content/publications/dis-2019-ledo.yaml","content/publications/dis-2019-mahadevan.yaml","content/publications/dis-2019-nakayama.yaml","content/publications/dis-2019-seyed.yaml","content/publications/dis-2021-wannamaker.yaml","content/publications/gi-2021-mactavish.yaml","content/publications/hri-2018-feick.yaml","content/publications/imwut-2020-wang.yaml","content/publications/iros-2020-hedayati.yaml","content/publications/mobilehci-2015-ledo.yaml","content/publications/mobilehci-2019-hung.yaml","content/publications/nime-2020-ko.yaml","content/publications/sui-2017-li.yaml","content/publications/tei-2016-somanath.yaml","content/publications/tei-2019-mikalauskas.yaml","content/publications/tei-2019-tolley.yaml","content/publications/tei-2019-wun.yaml","content/publications/tei-2020-suzuki.yaml","content/publications/tvcg-2016-lopez.yaml","content/publications/tvcg-2017-goffin.yaml","content/publications/tvcg-2017-willett.yaml","content/publications/tvcg-2019-blascheck.yaml","content/publications/tvcg-2019-walny.yaml","content/publications/uist-2018-suzuki.yaml","content/publications/uist-2019-suzuki.yaml","content/publications/uist-2020-suzuki.yaml","content/publications/uist-2020-yixian.yaml","content/publications/uist-2021-suzuki.yaml","content/publications/vr-2019-satriadi.yaml","content/seminar.yaml","content/vimeo.yaml","content/work-in-progress/chi-2020-asha.yaml","content/work-in-progress/dis-2018-ta.yaml"]}},"78zJ":function(e){e.exports={date:"2017-01",title:"An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents",authors:["Pascal Goffin","Jeremy Boy","Wesley Willett","Petra Isenberg"],series:"TVCG 2017",doi:"https://doi.org/10.1109/TVCG.2016.2618797",keywords:"word-scale visualization, word-scale graphic, text visualization, sparklines, authoring tool, information visualization",pages:13,video:"https://vimeo.com/230834366",abstract:"We contribute an investigation of the design and function of word-scale graphics and visualizations embedded in text documents. Word-scale graphics include both data-driven representations such as word-scale visualizations and sparklines, and non-data-driven visual marks. Their design, function, and use has so far received little research attention. We present the results of an open ended exploratory study with nine graphic designers. The study resulted in a rich collection of different types of graphics, data provenance, and relationships between text, graphics, and data. Based on this corpus, we present a systematic overview of word-scale graphic designs, and examine how designers used them. We also discuss the designers' goals in creating their graphics, and characterize how they used word-scale graphics to visualize data, add emphasis, and create alternative narratives. Building on these examples, we discuss implications for the design of authoring tools for word-scale graphics and visualizations, and explore how new authoring environments could make it easier for designers to integrate them into documents.",dir:"content/output/publications",base:"tvcg-2017-goffin.json",ext:".json",sourceBase:"tvcg-2017-goffin.yaml",sourceExt:".yaml"}},"8gHz":function(e,t,i){var a=i("5K7Z"),n=i("eaoh"),o=i("UWiX")("species");e.exports=function(e,t){var i,s=a(e).constructor;return void 0===s||null==(i=a(s)[o])?t:n(i)}},"8iia":function(e,t,i){var a=i("QMMT"),n=i("RRc/");e.exports=function(e){return function(){if(a(this)!=e)throw TypeError(e+"#toJSON isn't generic");return n(this)}}},"9ed3":function(e){e.exports={date:"2020-09",title:"PufferBot: Actuated Expandable Structures for Aerial Robots",authors:["Hooman Hedayati","Ryo Suzuki","Daniel Leithinger","Daniel Szafir"],series:"IROS 2020",video:"https://www.youtube.com/watch?v=XtPepCxWcBg",pages:6,abstract:"We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drone’s propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique charac- terizations which may be useful in different situations. Finally, we present three motivating scenarios in which PufferBot might be useful beyond existing static propeller guard structures.",dir:"content/output/publications",base:"iros-2020-hedayati.json",ext:".json",sourceBase:"iros-2020-hedayati.yaml",sourceExt:".yaml"}},AFJX:function(e){e.exports={date:"2015-04",title:"Mechanics of Camera Work in Mobile Video Collaboration",authors:["Brennan Jones","Anna Witcraft","Scott Bateman","Carman Neustaedter","Anthony Tang"],series:"CHI 2015",doi:"https://doi.org/10.1145/2702123.2702345",keywords:"video communication, collaboration, mobile computing, handheld devices, cscw",pages:10,video:"https://www.youtube.com/watch?v=V133YGkLxC8",abstract:"Mobile video conferencing, where one or more participants are moving about in the real world, enables entirely new interaction scenarios (e.g., asking for help to construct or repair an object, or showing a physical location). While we have a good understanding of the challenges of video conferencing in office or home environments, we do not fully understand the mechanics of camera work-how people use mobile devices to communicate with one another-during mobile video calls. To provide an understanding of what people do in mobile video collaboration, we conducted an observational study where pairs of participants completed tasks using a mobile video conferencing system. Our analysis suggests that people use the camera view deliberately to support their interactions-for example, to convey a message or to ask questions-but the limited field of view, and the lack of camera control can make it a frustrating experience.",dir:"content/output/publications",base:"chi-2015-jones.json",ext:".json",sourceBase:"chi-2015-jones.yaml",sourceExt:".yaml"}},B9jh:function(e,t,i){"use strict";var a=i("Wu5q"),n=i("n3ko");e.exports=i("raTm")("Set",function(e){return function(){return e(this,arguments.length>0?arguments[0]:void 0)}},{add:function(e){return a.def(n(this,"Set"),e=0===e?0:e,e)}},a)},Blo1:function(e){e.exports={date:"2021-05",title:"Grand Challenges in Immersive Analytics",authors:["Barrett Ens","Benjamin Bach","Maxime Cordeil","Ulrich Engelke","Marcos Serrano","Wesley Willett","Arnaud Prouzeau","Christoph Anthes","Wolfgang Büschel","Cody Dunne","Tim Dwyer","Jens Grubert","Jason H. Haga","Nurit Kishenbaum","Dylan Kobayashi","Tica Lin","Monsurat Olaosebikan","Fabian Pointecker","David Saffo","Nazmus Saquib","Dieter Schmalsteig","Danielle Albers Szafir","Matthew Whitlock","Yalong Yang"],series:"CHI 2021",doi:null,keywords:"Immersive analytics, grand research challenges, data visualisation, augmented reality, virtual reality",pages:17,video:null,abstract:"Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.",dir:"content/output/publications",base:"chi-2021-ens.json",ext:".json",sourceBase:"chi-2021-ens.yaml",sourceExt:".yaml"}},C2SN:function(e,t,i){var a=i("93I4"),n=i("kAMH"),o=i("UWiX")("species");e.exports=function(e){var t;return n(e)&&("function"!=typeof(t=e.constructor)||t!==Array&&!n(t.prototype)||(t=void 0),a(t)&&null===(t=t[o])&&(t=void 0)),void 0===t?Array:t}},CCdR:function(e){e.exports={date:"2017-10",title:"FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers",authors:["Ryo Suzuki","Abigale Stangl","Mark D. Gross","Tom Yeh"],series:"ASSETS 2020",doi:"https://doi.org/10.1145/3132525.3132548",keywords:"visual impairment, dynamic tactile markers, tangible interfaces, interactive tactile graphics",video:"https://www.youtube.com/watch?v=VbwIZ9V6i_g",pages:10,abstract:"For people with visual impairments, tactile graphics are an important means to learn and explore information. However, raised line tactile graphics created with traditional materials such as embossing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dynamic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily reconfigured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, feature identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as education and data exploration.",dir:"content/output/publications",base:"assets-2017-suzuki.json",ext:".json",sourceBase:"assets-2017-suzuki.yaml",sourceExt:".yaml"}},DjNc:function(e){e.exports={date:"2020-06",title:"Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff",authors:["Jagoda Walny","Christian Frisson","Mieka West","Doris Kosminsky","Søren Knudsen","Sheelagh Carpendale","Wesley Willett"],series:"TVCG 2020",doi:"https://doi.org/10.1109/TVCG.2019.2934538",keywords:"Information visualization, design handoff, data mapping, design process",pages:10,video:"https://vimeo.com/360483702",talk:"https://vimeo.com/368703151",abstract:"Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.",dir:"content/output/publications",base:"tvcg-2020-walny.json",ext:".json",sourceBase:"tvcg-2020-walny.yaml",sourceExt:".yaml"}},EXMj:function(e,t){e.exports=function(e,t,i,a){if(!(e instanceof t)||void 0!==a&&a in e)throw TypeError(i+": incorrect invocation!");return e}},EnQq:function(e){e.exports={date:"2018-04",title:"Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration",authors:["Martin Feick","Terrance Tin Hoi Mok","Anthony Tang","Lora Oehlberg","Ehud Sharlin"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3173855",keywords:"cscw, remote collaboration, object-focused collaboration, physical telepresence, collaborative physical tasks",pages:13,award:"Honorable Mention",video:"https://www.youtube.com/watch?v=sfxTHsPJWHY",abstract:"Remote collaborators working together on physical objects have difficulty building a shared understanding of what each person is talking about. Conventional video chat systems are insufficient for many situations because they present a single view of the object in a flattened image. To understand how this limited perspective affects collaboration, we designed the Remote Manipulator (ReMa), which can reproduce orientation manipulations on a proxy object at a remote site. We conducted two studies with ReMa, with two main findings. First, a shared perspective is more effective and preferred compared to the opposing perspective offered by conventional video chat systems. Second, the physical proxy and video chat complement one another in a combined system: people used the physical proxy to understand objects, and used video chat to perform gestures and confirm remote actions.",dir:"content/output/publications",base:"chi-2018-feick.json",ext:".json",sourceBase:"chi-2018-feick.yaml",sourceExt:".yaml"}},EnfY:function(e){e.exports={date:"2018-04",title:"A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality",authors:["Kody R. Dillman","Terrance Mok","Anthony Tang","Lora Oehlberg","Alex Mitchell"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3173714",keywords:"game design, guidance, interaction cues, augmented reality",pages:12,talk:"https://www.youtube.com/watch?v=3FoZStToALQ",abstract:"Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences.",dir:"content/output/publications",base:"chi-2018-dillman.json",ext:".json",sourceBase:"chi-2018-dillman.yaml",sourceExt:".yaml"}},ErCr:function(e){e.exports={date:"2021-05",title:"Homecoming: Exploring Returns to Long-Term Single Player Games",authors:["Noor Hammad","Owen Brierley","Zachary McKendrick","Sowmya Somanath","Patrick Finn","Jessica Hammer","Ehud Sharlin"],series:"CHI 2021",doi:"https://doi.org/10.1145/3411764.3445357",keywords:"long-term single player game, autobiographical design, pivot point",pages:13,video:null,abstract:"We present an autobiographical design journey exploring the experience of returning to long-term single player games. Continuing progress from a previously saved game, particularly when substantial time has passed, is an understudied area in games research. To begin our exploration in this domain, we investigated what the return experience is like first-hand. By returning to four long-term single player games played extensively in the past, we revealed a phenomenon we call The Pivot Point, a ‘eureka’ moment in return gameplay. The pivot point anchors our design explorations, where we created prototypes to leverage the pivot point in reconnecting with the experience. These return experiences and subsequent prototyping iterations inform our understanding of how to design better returns to gameplay, which can benefit both producers and consumers of long-term single player games.",dir:"content/output/publications",base:"chi-2021-hammad.json",ext:".json",sourceBase:"chi-2021-hammad.yaml",sourceExt:".yaml"}},HFN6:function(e){e.exports={date:"2020-10",title:"ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World",authors:["Yan Yixian","Kazuki Takashima","Anthony Tang","Takayuki Tanno","Kazuyuki Fujita","Yoshifumi Kitamura"],series:"UIST 2020",doi:"https://doi.org/10.1145/3379337.3415859",keywords:"encountered-type haptic devices, immersive experience",video:"https://www.youtube.com/watch?v=j2iSNDkBxAY",pages:13,abstract:"We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls---autonomous robotic encounter-type haptic wall-shaped props---that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user's movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of autonomous mobile reusable props, which we view as an important design direction for haptics in VR.",dir:"content/output/publications",base:"uist-2020-yixian.json",ext:".json",sourceBase:"uist-2020-yixian.yaml",sourceExt:".yaml"}},HFsU:function(e){e.exports={date:"2021-05",title:"Perspective Charts",authors:["Mia MacTavish","Katayoon Etemad","Faramarz Samavati","Wesley Willett"],series:"GI 2021",doi:"http://hdl.handle.net/1880/113671",keywords:"information visualization",video:"https://www.youtube.com/watch?v=Sp4Vt8mMhCs&list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-",pages:10,abstract:"We introduce three novel data visualizations, called perspective charts, based on the concept of size constancy in linear perspective projection. Bar charts are a popular and commonly used tool for the interpretation of datasets, however, representing datasets with multi-scale variation is challenging in a bar chart due to limitations in viewing space. Each of our designs focuses on the static representation of datasets with large ranges with respect to important variations in the data. Through a user study, we measure the effectiveness of our designs for representing these datasets in comparison to traditional methods, such as a standard bar chart or a broken-axis bar chart, and state-of-the-art methods, such as a scale-stack bar chart. The evaluation reveals that our designs allow pieces of data to be visually compared at a level of accuracy similar to traditional visualizations. Our designs demonstrate advantages when compared to state-of-the-art visualizations designed to represent datasets with large outliers.",dir:"content/output/publications",base:"gi-2021-mactavish.json",ext:".json",sourceBase:"gi-2021-mactavish.yaml",sourceExt:".yaml"}},IClC:function(e,t,i){"use strict";var a=function(e){if(e&&e.__esModule)return e;var t={};if(null!=e)for(var i in e)Object.hasOwnProperty.call(e,i)&&(t[i]=e[i]);return t.default=e,t};Object.defineProperty(t,"__esModule",{value:!0});var n=a(i("q1tI"));t.HeadManagerContext=n.createContext(null)},IhcT:function(e){e.exports={date:"2017-05",title:"Building with Data: Architectural Models as Inspiration for Data Physicalization",authors:["Carmen Hull","Wesley Willett"],series:"CHI 2017",doi:"https://doi.org/10.1145/3025453.3025850",keywords:"design process, architectural models, data physicalization, embodied interaction, data visualization",pages:12,video:"https://www.youtube.com/watch?v=bkqLNgYIXek",abstract:"In this paper we analyze the role of physical scale models in the architectural design process and apply insights from architecture for the creation and use of data physicalizations. Based on a survey of the architecture literature on model making and ten interviews with practicing architects, we describe the role of physical models as a tool for exploration and communication. From these observations, we identify trends in the use of physical models in architecture, which have the potential to inform the design of data physicalizations. We identify four functions of architectural modeling that can be directly adapted for use in the process of building rich data models. Finally, we discuss how the visualization community can apply observations from architecture to the design of new data physicalizations.",dir:"content/output/publications",base:"chi-2017-hull.json",ext:".json",sourceBase:"chi-2017-hull.yaml",sourceExt:".yaml"}},"JMW+":function(e,t,i){"use strict";var a,n,o,s,r=i("uOPS"),c=i("5T2Y"),l=i("2GTP"),u=i("QMMT"),p=i("Y7ZC"),h=i("93I4"),d=i("eaoh"),m=i("EXMj"),g=i("oioR"),f=i("8gHz"),y=i("QXhf").set,v=i("q6LJ")(),b=i("ZW5q"),w=i("RDmV"),j=i("vBP9"),k=i("zXhZ"),x=c.TypeError,z=c.process,S=z&&z.versions,E=S&&S.v8||"",C=c.Promise,I="process"==u(z),A=function(){},_=n=b.f,T=!!function(){try{var e=C.resolve(1),t=(e.constructor={})[i("UWiX")("species")]=function(e){e(A,A)};return(I||"function"==typeof PromiseRejectionEvent)&&e.then(A)instanceof t&&0!==E.indexOf("6.6")&&-1===j.indexOf("Chrome/66")}catch(a){}}(),W=function(e){var t;return!(!h(e)||"function"!=typeof(t=e.then))&&t},B=function(e,t){if(!e._n){e._n=!0;var i=e._c;v(function(){for(var a=e._v,n=1==e._s,o=0,s=function(t){var i,o,s,r=n?t.ok:t.fail,c=t.resolve,l=t.reject,u=t.domain;try{r?(n||(2==e._h&&R(e),e._h=1),!0===r?i=a:(u&&u.enter(),i=r(a),u&&(u.exit(),s=!0)),i===t.promise?l(x("Promise-chain cycle")):(o=W(i))?o.call(i,c,l):c(i)):l(a)}catch(p){u&&!s&&u.exit(),l(p)}};i.length>o;)s(i[o++]);e._c=[],e._n=!1,t&&!e._h&&D(e)})}},D=function(e){y.call(c,function(){var t,i,a,n=e._v,o=M(e);if(o&&(t=w(function(){I?z.emit("unhandledRejection",n,e):(i=c.onunhandledrejection)?i({promise:e,reason:n}):(a=c.console)&&a.error&&a.error("Unhandled promise rejection",n)}),e._h=I||M(e)?2:1),e._a=void 0,o&&t.e)throw t.v})},M=function(e){return 1!==e._h&&0===(e._a||e._c).length},R=function(e){y.call(c,function(){var t;I?z.emit("rejectionHandled",e):(t=c.onrejectionhandled)&&t({promise:e,reason:e._v})})},P=function(e){var t=this;t._d||(t._d=!0,(t=t._w||t)._v=e,t._s=2,t._a||(t._a=t._c.slice()),B(t,!0))},L=function(e){var t,i=this;if(!i._d){i._d=!0,i=i._w||i;try{if(i===e)throw x("Promise can't be resolved itself");(t=W(e))?v(function(){var a={_w:i,_d:!1};try{t.call(e,l(L,a,1),l(P,a,1))}catch(n){P.call(a,n)}}):(i._v=e,i._s=1,B(i,!1))}catch(a){P.call({_w:i,_d:!1},a)}}};T||(C=function(e){m(this,C,"Promise","_h"),d(e),a.call(this);try{e(l(L,this,1),l(P,this,1))}catch(t){P.call(this,t)}},(a=function(e){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1}).prototype=i("XJU/")(C.prototype,{then:function(e,t){var i=_(f(this,C));return i.ok="function"!=typeof e||e,i.fail="function"==typeof t&&t,i.domain=I?z.domain:void 0,this._c.push(i),this._a&&this._a.push(i),this._s&&B(this,!1),i.promise},catch:function(e){return this.then(void 0,e)}}),o=function(){var e=new a;this.promise=e,this.resolve=l(L,e,1),this.reject=l(P,e,1)},b.f=_=function(e){return e===C||e===s?new o(e):n(e)}),p(p.G+p.W+p.F*!T,{Promise:C}),i("RfKB")(C,"Promise"),i("TJWN")("Promise"),s=i("WEpk").Promise,p(p.S+p.F*!T,"Promise",{reject:function(e){var t=_(this);return(0,t.reject)(e),t.promise}}),p(p.S+p.F*(r||!T),"Promise",{resolve:function(e){return k(r&&this===s?C:this,e)}}),p(p.S+p.F*!(T&&i("TuGD")(function(e){C.all(e).catch(A)})),"Promise",{all:function(e){var t=this,i=_(t),a=i.resolve,n=i.reject,o=w(function(){var i=[],o=0,s=1;g(e,!1,function(e){var r=o++,c=!1;i.push(void 0),s++,t.resolve(e).then(function(e){c||(c=!0,i[r]=e,--s||a(i))},n)}),--s||a(i)});return o.e&&n(o.v),i.promise},race:function(e){var t=this,i=_(t),a=i.reject,n=w(function(){g(e,!1,function(e){t.resolve(e).then(i.resolve,a)})});return n.e&&a(n.v),i.promise}})},JW94:function(e){e.exports={date:"2018-10",title:"Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation",authors:["Ryo Suzuki","Junichi Yamaoka","Daniel Leithinger","Tom Yeh","Mark D. Gross","Yoshihiro Kawahara","Yasuaki Kakehi"],series:"UIST 2018",doi:"https://doi.org/10.1145/3242587.3242659",keywords:"digital materials, dynamic 3D printing, shape displays",video:"https://www.youtube.com/watch?v=92eGI-gYYc4","video-2":"https://www.youtube.com/watch?v=7nPlr3O9xu8",talk:"https://www.youtube.com/watch?v=R3FRUtOIiCQ",pages:16,abstract:"This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.",dir:"content/output/publications",base:"uist-2018-suzuki.json",ext:".json",sourceBase:"uist-2018-suzuki.yaml",sourceExt:".yaml"}},KI45:function(e,t){e.exports=function(e){return e&&e.__esModule?e:{default:e}}},KMRZ:function(e){e.exports={date:"2019-06",title:"Sketching and Ideation Activities for Situated Visualization Design",authors:["Nathalie Bressa","Kendra Wannamaker","Henrik Korsgaard","Wesley Willett","Jo Vermeulen"],series:"DIS 2019",doi:"https://doi.org/10.1145/3322276.3322326",keywords:"ideation, design workshops, situated visualization, information visualization, small displays, sketching",abstract:"We report on findings from seven design workshops that used ideation and sketching activities to prototype new situated visualizations - representations of data that are displayed in proximity to the physical referents (such as people, objects, and locations) to which the data is related. Designing situated visualizations requires a fine-grained understanding of the context in which the visualizations are placed, as well as an exploration of different options for placement and form factors, which existing methods for visualization design do not account for. Focusing on small displays as a target platform, we reflect on our experiences of using a diverse range of sketching activities, materials, and prompts. Based on these observations, we identify challenges and opportunities for sketching and ideating situated visualizations. We also outline the space of design activities for situated visualization and highlight promising methods for both designers and researchers.",dir:"content/output/publications",base:"dis-2019-bressa.json",ext:".json",sourceBase:"dis-2019-bressa.yaml",sourceExt:".yaml"}},KjYe:function(e){e.exports={date:"2019-06",title:"MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction",authors:["Ryosuke Nakayama","Ryo Suzuki","Satoshi Nakamaru","Ryuma Niiyama","Yoshihiro Kawahara","Yasuaki Kakehi"],series:"DIS 2019",doi:"https://doi.org/10.1145/3322276.3322337",keywords:"shape-changing interfaces, programming by demonstration, soft robots, pneumatic actuation, tangible interactions",award:"Best Paper",video:"https://www.youtube.com/watch?v=ZkCcazfFD-M",pages:11,abstract:"We introduce MorphIO, entirely soft sensing and actuation modules for programming by demonstration of soft robots and shape-changing interfaces.MorphIO's hardware consists of a soft pneumatic actuator containing a conductive sponge sensor.This allows both input and output of three-dimensional deformation of a soft material.Leveraging this capability, MorphIO enables a user to record and later playback physical motion of programmable shape-changing materials.In addition, the modular design of MorphIO's unit allows the user to construct various shapes and topologies through magnetic connection.We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects.Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.",dir:"content/output/publications",base:"dis-2019-nakayama.json",ext:".json",sourceBase:"dis-2019-nakayama.yaml",sourceExt:".yaml"}},LEa2:function(e){e.exports={date:"2018-06",title:"Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design",authors:["Tran Pham","Jo Vermeulen","Anthony Tang","Lindsay MacDonald Vermeulen"],series:"DIS 2018",doi:"https://doi.org/10.1145/3196709.3196719",keywords:"augmented reality, gestures, gesture elicitation, hololens",pages:14,abstract:"Because gesture design for augmented reality (AR) remains idiosyncratic, people cannot necessarily use gestures learned in one AR application in another. To design discoverable gestures, we need to understand what gestures people expect to use. We explore how the scale of AR affects the gestures people expect to use to interact with 3D holograms. Using an elicitation study, we asked participants to generate gestures in response to holographic task referents, where we varied the scale of holograms from desktop-scale to room-scale objects. We found that the scale of objects and scenes in the AR experience moderates the generated gestures. Most gestures were informed by physical interaction, and when people interacted from a distance, they sought a good perspective on the target object before and during the interaction. These results suggest that gesture designers need to account for scale, and should not simply reuse gestures across different hologram sizes.",dir:"content/output/publications",base:"dis-2018-pham.json",ext:".json",sourceBase:"dis-2018-pham.yaml",sourceExt:".yaml"}},LQiU:function(e){e.exports={date:"2021-10",title:"HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots",authors:["Ryo Suzuki","Eyal Ofek","Mike Sinclair","Daniel Leithinger","Mar Gonzalez-Franco"],series:"UIST 2021",doi:"https://doi.org/10.1145/3472749.3474821",keywords:"virtual reality, encountered-type haptics, tabletop mobile robots, swarm user interfaces",video:"https://www.youtube.com/watch?v=HTiZgOESJyQ",pages:16,abstract:"HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic ap- proaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in- time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various ap- plications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.",dir:"content/output/publications",base:"uist-2021-hapicbots.json",ext:".json",sourceBase:"uist-2021-hapicbots.yaml",sourceExt:".yaml"}},"Ln+l":function(e){e.exports={date:"2018-04",title:"Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing",authors:["Yasamin Heshmat","Brennan Jones","Xiaoxuan Xiong","Carman Neustaedter","Anthony Tang","Bernhard E. Riecke","Lillian Yang"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3173933",keywords:"video communication, telepresence robots, leisure activities, social presence, geocaching",pages:13,abstract:"People often enjoy sharing outdoor activities together such as walking and hiking. However, when family and friends are separated by distance it can be difficult if not impossible to share such activities. We explore this design space by investigating the benefits and challenges of using a telepresence robot to support outdoor leisure activities. In our study, participants participated in the outdoor activity of geocaching where one person geocached with the help of a remote partner via a telepresence robot. We compared a wide field of view (WFOV) camera to a 360° camera. Results show the benefits of having a physical embodiment and a sense of immersion with the 360° view. Yet challenges related to a lack of environmental awareness, safety issues, and privacy concerns resulting from bystander interactions. These findings illustrate the need to design telepresence robots with the environment and public in mind to provide an enhanced sensory experience while balancing safety and privacy issues resulting from being amongst the general public.",dir:"content/output/publications",base:"chi-2018-heshmat.json",ext:".json",sourceBase:"chi-2018-heshmat.yaml",sourceExt:".yaml"}},LvDl:function(e,t,i){(function(e,a){var n;(function(){var o,s=200,r="Unsupported core-js use. Try https://npms.io/search?q=ponyfill.",c="Expected a function",l="__lodash_hash_undefined__",u=500,p="__lodash_placeholder__",h=1,d=2,m=4,g=1,f=2,y=1,v=2,b=4,w=8,j=16,k=32,x=64,z=128,S=256,E=512,C=30,I="...",A=800,_=16,T=1,W=2,B=1/0,D=9007199254740991,M=1.7976931348623157e308,R=NaN,P=4294967295,L=P-1,U=P>>>1,O=[["ary",z],["bind",y],["bindKey",v],["curry",w],["curryRight",j],["flip",E],["partial",k],["partialRight",x],["rearg",S]],H="[object Arguments]",V="[object Array]",N="[object AsyncFunction]",q="[object Boolean]",F="[object Date]",Y="[object DOMException]",G="[object Error]",K="[object Function]",J="[object GeneratorFunction]",Z="[object Map]",X="[object Number]",Q="[object Null]",$="[object Object]",ee="[object Proxy]",te="[object RegExp]",ie="[object Set]",ae="[object String]",ne="[object Symbol]",oe="[object Undefined]",se="[object WeakMap]",re="[object WeakSet]",ce="[object ArrayBuffer]",le="[object DataView]",ue="[object Float32Array]",pe="[object Float64Array]",he="[object Int8Array]",de="[object Int16Array]",me="[object Int32Array]",ge="[object Uint8Array]",fe="[object Uint8ClampedArray]",ye="[object Uint16Array]",ve="[object Uint32Array]",be=/\b__p \+= '';/g,we=/\b(__p \+=) '' \+/g,je=/(__e\(.*?\)|\b__t\)) \+\n'';/g,ke=/&(?:amp|lt|gt|quot|#39);/g,xe=/[&<>"']/g,ze=RegExp(ke.source),Se=RegExp(xe.source),Ee=/<%-([\s\S]+?)%>/g,Ce=/<%([\s\S]+?)%>/g,Ie=/<%=([\s\S]+?)%>/g,Ae=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,_e=/^\w*$/,Te=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,We=/[\\^$.*+?()[\]{}|]/g,Be=RegExp(We.source),De=/^\s+|\s+$/g,Me=/^\s+/,Re=/\s+$/,Pe=/\{(?:\n\/\* \[wrapped with .+\] \*\/)?\n?/,Le=/\{\n\/\* \[wrapped with (.+)\] \*/,Ue=/,? & /,Oe=/[^\x00-\x2f\x3a-\x40\x5b-\x60\x7b-\x7f]+/g,He=/\\(\\)?/g,Ve=/\$\{([^\\}]*(?:\\.[^\\}]*)*)\}/g,Ne=/\w*$/,qe=/^[-+]0x[0-9a-f]+$/i,Fe=/^0b[01]+$/i,Ye=/^\[object .+?Constructor\]$/,Ge=/^0o[0-7]+$/i,Ke=/^(?:0|[1-9]\d*)$/,Je=/[\xc0-\xd6\xd8-\xf6\xf8-\xff\u0100-\u017f]/g,Ze=/($^)/,Xe=/['\n\r\u2028\u2029\\]/g,Qe="\\u0300-\\u036f\\ufe20-\\ufe2f\\u20d0-\\u20ff",$e="\\xac\\xb1\\xd7\\xf7\\x00-\\x2f\\x3a-\\x40\\x5b-\\x60\\x7b-\\xbf\\u2000-\\u206f \\t\\x0b\\f\\xa0\\ufeff\\n\\r\\u2028\\u2029\\u1680\\u180e\\u2000\\u2001\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a\\u202f\\u205f\\u3000",et="[\\ud800-\\udfff]",tt="["+$e+"]",it="["+Qe+"]",at="\\d+",nt="[\\u2700-\\u27bf]",ot="[a-z\\xdf-\\xf6\\xf8-\\xff]",st="[^\\ud800-\\udfff"+$e+at+"\\u2700-\\u27bfa-z\\xdf-\\xf6\\xf8-\\xffA-Z\\xc0-\\xd6\\xd8-\\xde]",rt="\\ud83c[\\udffb-\\udfff]",ct="[^\\ud800-\\udfff]",lt="(?:\\ud83c[\\udde6-\\uddff]){2}",ut="[\\ud800-\\udbff][\\udc00-\\udfff]",pt="[A-Z\\xc0-\\xd6\\xd8-\\xde]",ht="(?:"+ot+"|"+st+")",dt="(?:"+pt+"|"+st+")",mt="(?:"+it+"|"+rt+")"+"?",gt="[\\ufe0e\\ufe0f]?"+mt+("(?:\\u200d(?:"+[ct,lt,ut].join("|")+")[\\ufe0e\\ufe0f]?"+mt+")*"),ft="(?:"+[nt,lt,ut].join("|")+")"+gt,yt="(?:"+[ct+it+"?",it,lt,ut,et].join("|")+")",vt=RegExp("['’]","g"),bt=RegExp(it,"g"),wt=RegExp(rt+"(?="+rt+")|"+yt+gt,"g"),jt=RegExp([pt+"?"+ot+"+(?:['’](?:d|ll|m|re|s|t|ve))?(?="+[tt,pt,"$"].join("|")+")",dt+"+(?:['’](?:D|LL|M|RE|S|T|VE))?(?="+[tt,pt+ht,"$"].join("|")+")",pt+"?"+ht+"+(?:['’](?:d|ll|m|re|s|t|ve))?",pt+"+(?:['’](?:D|LL|M|RE|S|T|VE))?","\\d*(?:1ST|2ND|3RD|(?![123])\\dTH)(?=\\b|[a-z_])","\\d*(?:1st|2nd|3rd|(?![123])\\dth)(?=\\b|[A-Z_])",at,ft].join("|"),"g"),kt=RegExp("[\\u200d\\ud800-\\udfff"+Qe+"\\ufe0e\\ufe0f]"),xt=/[a-z][A-Z]|[A-Z]{2}[a-z]|[0-9][a-zA-Z]|[a-zA-Z][0-9]|[^a-zA-Z0-9 ]/,zt=["Array","Buffer","DataView","Date","Error","Float32Array","Float64Array","Function","Int8Array","Int16Array","Int32Array","Map","Math","Object","Promise","RegExp","Set","String","Symbol","TypeError","Uint8Array","Uint8ClampedArray","Uint16Array","Uint32Array","WeakMap","_","clearTimeout","isFinite","parseInt","setTimeout"],St=-1,Et={};Et[ue]=Et[pe]=Et[he]=Et[de]=Et[me]=Et[ge]=Et[fe]=Et[ye]=Et[ve]=!0,Et[H]=Et[V]=Et[ce]=Et[q]=Et[le]=Et[F]=Et[G]=Et[K]=Et[Z]=Et[X]=Et[$]=Et[te]=Et[ie]=Et[ae]=Et[se]=!1;var Ct={};Ct[H]=Ct[V]=Ct[ce]=Ct[le]=Ct[q]=Ct[F]=Ct[ue]=Ct[pe]=Ct[he]=Ct[de]=Ct[me]=Ct[Z]=Ct[X]=Ct[$]=Ct[te]=Ct[ie]=Ct[ae]=Ct[ne]=Ct[ge]=Ct[fe]=Ct[ye]=Ct[ve]=!0,Ct[G]=Ct[K]=Ct[se]=!1;var It={"\\":"\\","'":"'","\n":"n","\r":"r","\u2028":"u2028","\u2029":"u2029"},At=parseFloat,_t=parseInt,Tt="object"==typeof e&&e&&e.Object===Object&&e,Wt="object"==typeof self&&self&&self.Object===Object&&self,Bt=Tt||Wt||Function("return this")(),Dt=t&&!t.nodeType&&t,Mt=Dt&&"object"==typeof a&&a&&!a.nodeType&&a,Rt=Mt&&Mt.exports===Dt,Pt=Rt&&Tt.process,Lt=function(){try{var e=Mt&&Mt.require&&Mt.require("util").types;return e||Pt&&Pt.binding&&Pt.binding("util")}catch(t){}}(),Ut=Lt&&Lt.isArrayBuffer,Ot=Lt&&Lt.isDate,Ht=Lt&&Lt.isMap,Vt=Lt&&Lt.isRegExp,Nt=Lt&&Lt.isSet,qt=Lt&&Lt.isTypedArray;function Ft(e,t,i){switch(i.length){case 0:return e.call(t);case 1:return e.call(t,i[0]);case 2:return e.call(t,i[0],i[1]);case 3:return e.call(t,i[0],i[1],i[2])}return e.apply(t,i)}function Yt(e,t,i,a){for(var n=-1,o=null==e?0:e.length;++n<o;){var s=e[n];t(a,s,i(s),e)}return a}function Gt(e,t){for(var i=-1,a=null==e?0:e.length;++i<a&&!1!==t(e[i],i,e););return e}function Kt(e,t){for(var i=null==e?0:e.length;i--&&!1!==t(e[i],i,e););return e}function Jt(e,t){for(var i=-1,a=null==e?0:e.length;++i<a;)if(!t(e[i],i,e))return!1;return!0}function Zt(e,t){for(var i=-1,a=null==e?0:e.length,n=0,o=[];++i<a;){var s=e[i];t(s,i,e)&&(o[n++]=s)}return o}function Xt(e,t){return!!(null==e?0:e.length)&&ri(e,t,0)>-1}function Qt(e,t,i){for(var a=-1,n=null==e?0:e.length;++a<n;)if(i(t,e[a]))return!0;return!1}function $t(e,t){for(var i=-1,a=null==e?0:e.length,n=Array(a);++i<a;)n[i]=t(e[i],i,e);return n}function ei(e,t){for(var i=-1,a=t.length,n=e.length;++i<a;)e[n+i]=t[i];return e}function ti(e,t,i,a){var n=-1,o=null==e?0:e.length;for(a&&o&&(i=e[++n]);++n<o;)i=t(i,e[n],n,e);return i}function ii(e,t,i,a){var n=null==e?0:e.length;for(a&&n&&(i=e[--n]);n--;)i=t(i,e[n],n,e);return i}function ai(e,t){for(var i=-1,a=null==e?0:e.length;++i<a;)if(t(e[i],i,e))return!0;return!1}var ni=pi("length");function oi(e,t,i){var a;return i(e,function(e,i,n){if(t(e,i,n))return a=i,!1}),a}function si(e,t,i,a){for(var n=e.length,o=i+(a?1:-1);a?o--:++o<n;)if(t(e[o],o,e))return o;return-1}function ri(e,t,i){return t==t?function(e,t,i){var a=i-1,n=e.length;for(;++a<n;)if(e[a]===t)return a;return-1}(e,t,i):si(e,li,i)}function ci(e,t,i,a){for(var n=i-1,o=e.length;++n<o;)if(a(e[n],t))return n;return-1}function li(e){return e!=e}function ui(e,t){var i=null==e?0:e.length;return i?mi(e,t)/i:R}function pi(e){return function(t){return null==t?o:t[e]}}function hi(e){return function(t){return null==e?o:e[t]}}function di(e,t,i,a,n){return n(e,function(e,n,o){i=a?(a=!1,e):t(i,e,n,o)}),i}function mi(e,t){for(var i,a=-1,n=e.length;++a<n;){var s=t(e[a]);s!==o&&(i=i===o?s:i+s)}return i}function gi(e,t){for(var i=-1,a=Array(e);++i<e;)a[i]=t(i);return a}function fi(e){return function(t){return e(t)}}function yi(e,t){return $t(t,function(t){return e[t]})}function vi(e,t){return e.has(t)}function bi(e,t){for(var i=-1,a=e.length;++i<a&&ri(t,e[i],0)>-1;);return i}function wi(e,t){for(var i=e.length;i--&&ri(t,e[i],0)>-1;);return i}var ji=hi({"À":"A","Á":"A","Â":"A","Ã":"A","Ä":"A","Å":"A","à":"a","á":"a","â":"a","ã":"a","ä":"a","å":"a","Ç":"C","ç":"c","Ð":"D","ð":"d","È":"E","É":"E","Ê":"E","Ë":"E","è":"e","é":"e","ê":"e","ë":"e","Ì":"I","Í":"I","Î":"I","Ï":"I","ì":"i","í":"i","î":"i","ï":"i","Ñ":"N","ñ":"n","Ò":"O","Ó":"O","Ô":"O","Õ":"O","Ö":"O","Ø":"O","ò":"o","ó":"o","ô":"o","õ":"o","ö":"o","ø":"o","Ù":"U","Ú":"U","Û":"U","Ü":"U","ù":"u","ú":"u","û":"u","ü":"u","Ý":"Y","ý":"y","ÿ":"y","Æ":"Ae","æ":"ae","Þ":"Th","þ":"th","ß":"ss","Ā":"A","Ă":"A","Ą":"A","ā":"a","ă":"a","ą":"a","Ć":"C","Ĉ":"C","Ċ":"C","Č":"C","ć":"c","ĉ":"c","ċ":"c","č":"c","Ď":"D","Đ":"D","ď":"d","đ":"d","Ē":"E","Ĕ":"E","Ė":"E","Ę":"E","Ě":"E","ē":"e","ĕ":"e","ė":"e","ę":"e","ě":"e","Ĝ":"G","Ğ":"G","Ġ":"G","Ģ":"G","ĝ":"g","ğ":"g","ġ":"g","ģ":"g","Ĥ":"H","Ħ":"H","ĥ":"h","ħ":"h","Ĩ":"I","Ī":"I","Ĭ":"I","Į":"I","İ":"I","ĩ":"i","ī":"i","ĭ":"i","į":"i","ı":"i","Ĵ":"J","ĵ":"j","Ķ":"K","ķ":"k","ĸ":"k","Ĺ":"L","Ļ":"L","Ľ":"L","Ŀ":"L","Ł":"L","ĺ":"l","ļ":"l","ľ":"l","ŀ":"l","ł":"l","Ń":"N","Ņ":"N","Ň":"N","Ŋ":"N","ń":"n","ņ":"n","ň":"n","ŋ":"n","Ō":"O","Ŏ":"O","Ő":"O","ō":"o","ŏ":"o","ő":"o","Ŕ":"R","Ŗ":"R","Ř":"R","ŕ":"r","ŗ":"r","ř":"r","Ś":"S","Ŝ":"S","Ş":"S","Š":"S","ś":"s","ŝ":"s","ş":"s","š":"s","Ţ":"T","Ť":"T","Ŧ":"T","ţ":"t","ť":"t","ŧ":"t","Ũ":"U","Ū":"U","Ŭ":"U","Ů":"U","Ű":"U","Ų":"U","ũ":"u","ū":"u","ŭ":"u","ů":"u","ű":"u","ų":"u","Ŵ":"W","ŵ":"w","Ŷ":"Y","ŷ":"y","Ÿ":"Y","Ź":"Z","Ż":"Z","Ž":"Z","ź":"z","ż":"z","ž":"z","Ĳ":"IJ","ĳ":"ij","Œ":"Oe","œ":"oe","ŉ":"'n","ſ":"s"}),ki=hi({"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&quot;","'":"&#39;"});function xi(e){return"\\"+It[e]}function zi(e){return kt.test(e)}function Si(e){var t=-1,i=Array(e.size);return e.forEach(function(e,a){i[++t]=[a,e]}),i}function Ei(e,t){return function(i){return e(t(i))}}function Ci(e,t){for(var i=-1,a=e.length,n=0,o=[];++i<a;){var s=e[i];s!==t&&s!==p||(e[i]=p,o[n++]=i)}return o}function Ii(e){var t=-1,i=Array(e.size);return e.forEach(function(e){i[++t]=e}),i}function Ai(e){var t=-1,i=Array(e.size);return e.forEach(function(e){i[++t]=[e,e]}),i}function _i(e){return zi(e)?function(e){var t=wt.lastIndex=0;for(;wt.test(e);)++t;return t}(e):ni(e)}function Ti(e){return zi(e)?function(e){return e.match(wt)||[]}(e):function(e){return e.split("")}(e)}var Wi=hi({"&amp;":"&","&lt;":"<","&gt;":">","&quot;":'"',"&#39;":"'"});var Bi=function e(t){var i,a=(t=null==t?Bt:Bi.defaults(Bt.Object(),t,Bi.pick(Bt,zt))).Array,n=t.Date,Qe=t.Error,$e=t.Function,et=t.Math,tt=t.Object,it=t.RegExp,at=t.String,nt=t.TypeError,ot=a.prototype,st=$e.prototype,rt=tt.prototype,ct=t["__core-js_shared__"],lt=st.toString,ut=rt.hasOwnProperty,pt=0,ht=(i=/[^.]+$/.exec(ct&&ct.keys&&ct.keys.IE_PROTO||""))?"Symbol(src)_1."+i:"",dt=rt.toString,mt=lt.call(tt),gt=Bt._,ft=it("^"+lt.call(ut).replace(We,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$"),yt=Rt?t.Buffer:o,wt=t.Symbol,kt=t.Uint8Array,It=yt?yt.allocUnsafe:o,Tt=Ei(tt.getPrototypeOf,tt),Wt=tt.create,Dt=rt.propertyIsEnumerable,Mt=ot.splice,Pt=wt?wt.isConcatSpreadable:o,Lt=wt?wt.iterator:o,ni=wt?wt.toStringTag:o,hi=function(){try{var e=Uo(tt,"defineProperty");return e({},"",{}),e}catch(t){}}(),Di=t.clearTimeout!==Bt.clearTimeout&&t.clearTimeout,Mi=n&&n.now!==Bt.Date.now&&n.now,Ri=t.setTimeout!==Bt.setTimeout&&t.setTimeout,Pi=et.ceil,Li=et.floor,Ui=tt.getOwnPropertySymbols,Oi=yt?yt.isBuffer:o,Hi=t.isFinite,Vi=ot.join,Ni=Ei(tt.keys,tt),qi=et.max,Fi=et.min,Yi=n.now,Gi=t.parseInt,Ki=et.random,Ji=ot.reverse,Zi=Uo(t,"DataView"),Xi=Uo(t,"Map"),Qi=Uo(t,"Promise"),$i=Uo(t,"Set"),ea=Uo(t,"WeakMap"),ta=Uo(tt,"create"),ia=ea&&new ea,aa={},na=ps(Zi),oa=ps(Xi),sa=ps(Qi),ra=ps($i),ca=ps(ea),la=wt?wt.prototype:o,ua=la?la.valueOf:o,pa=la?la.toString:o;function ha(e){if(Ir(e)&&!yr(e)&&!(e instanceof fa)){if(e instanceof ga)return e;if(ut.call(e,"__wrapped__"))return hs(e)}return new ga(e)}var da=function(){function e(){}return function(t){if(!Cr(t))return{};if(Wt)return Wt(t);e.prototype=t;var i=new e;return e.prototype=o,i}}();function ma(){}function ga(e,t){this.__wrapped__=e,this.__actions__=[],this.__chain__=!!t,this.__index__=0,this.__values__=o}function fa(e){this.__wrapped__=e,this.__actions__=[],this.__dir__=1,this.__filtered__=!1,this.__iteratees__=[],this.__takeCount__=P,this.__views__=[]}function ya(e){var t=-1,i=null==e?0:e.length;for(this.clear();++t<i;){var a=e[t];this.set(a[0],a[1])}}function va(e){var t=-1,i=null==e?0:e.length;for(this.clear();++t<i;){var a=e[t];this.set(a[0],a[1])}}function ba(e){var t=-1,i=null==e?0:e.length;for(this.clear();++t<i;){var a=e[t];this.set(a[0],a[1])}}function wa(e){var t=-1,i=null==e?0:e.length;for(this.__data__=new ba;++t<i;)this.add(e[t])}function ja(e){var t=this.__data__=new va(e);this.size=t.size}function ka(e,t){var i=yr(e),a=!i&&fr(e),n=!i&&!a&&jr(e),o=!i&&!a&&!n&&Rr(e),s=i||a||n||o,r=s?gi(e.length,at):[],c=r.length;for(var l in e)!t&&!ut.call(e,l)||s&&("length"==l||n&&("offset"==l||"parent"==l)||o&&("buffer"==l||"byteLength"==l||"byteOffset"==l)||Yo(l,c))||r.push(l);return r}function xa(e){var t=e.length;return t?e[jn(0,t-1)]:o}function za(e,t){return cs(io(e),Ba(t,0,e.length))}function Sa(e){return cs(io(e))}function Ea(e,t,i){(i===o||dr(e[t],i))&&(i!==o||t in e)||Ta(e,t,i)}function Ca(e,t,i){var a=e[t];ut.call(e,t)&&dr(a,i)&&(i!==o||t in e)||Ta(e,t,i)}function Ia(e,t){for(var i=e.length;i--;)if(dr(e[i][0],t))return i;return-1}function Aa(e,t,i,a){return La(e,function(e,n,o){t(a,e,i(e),o)}),a}function _a(e,t){return e&&ao(t,nc(t),e)}function Ta(e,t,i){"__proto__"==t&&hi?hi(e,t,{configurable:!0,enumerable:!0,value:i,writable:!0}):e[t]=i}function Wa(e,t){for(var i=-1,n=t.length,s=a(n),r=null==e;++i<n;)s[i]=r?o:$r(e,t[i]);return s}function Ba(e,t,i){return e==e&&(i!==o&&(e=e<=i?e:i),t!==o&&(e=e>=t?e:t)),e}function Da(e,t,i,a,n,s){var r,c=t&h,l=t&d,u=t&m;if(i&&(r=n?i(e,a,n,s):i(e)),r!==o)return r;if(!Cr(e))return e;var p=yr(e);if(p){if(r=function(e){var t=e.length,i=new e.constructor(t);return t&&"string"==typeof e[0]&&ut.call(e,"index")&&(i.index=e.index,i.input=e.input),i}(e),!c)return io(e,r)}else{var g=Vo(e),f=g==K||g==J;if(jr(e))return Zn(e,c);if(g==$||g==H||f&&!n){if(r=l||f?{}:qo(e),!c)return l?function(e,t){return ao(e,Ho(e),t)}(e,function(e,t){return e&&ao(t,oc(t),e)}(r,e)):function(e,t){return ao(e,Oo(e),t)}(e,_a(r,e))}else{if(!Ct[g])return n?e:{};r=function(e,t,i){var a,n,o,s=e.constructor;switch(t){case ce:return Xn(e);case q:case F:return new s(+e);case le:return function(e,t){var i=t?Xn(e.buffer):e.buffer;return new e.constructor(i,e.byteOffset,e.byteLength)}(e,i);case ue:case pe:case he:case de:case me:case ge:case fe:case ye:case ve:return Qn(e,i);case Z:return new s;case X:case ae:return new s(e);case te:return(o=new(n=e).constructor(n.source,Ne.exec(n))).lastIndex=n.lastIndex,o;case ie:return new s;case ne:return a=e,ua?tt(ua.call(a)):{}}}(e,g,c)}}s||(s=new ja);var y=s.get(e);if(y)return y;s.set(e,r),Br(e)?e.forEach(function(a){r.add(Da(a,t,i,a,e,s))}):Ar(e)&&e.forEach(function(a,n){r.set(n,Da(a,t,i,n,e,s))});var v=p?o:(u?l?Wo:To:l?oc:nc)(e);return Gt(v||e,function(a,n){v&&(a=e[n=a]),Ca(r,n,Da(a,t,i,n,e,s))}),r}function Ma(e,t,i){var a=i.length;if(null==e)return!a;for(e=tt(e);a--;){var n=i[a],s=t[n],r=e[n];if(r===o&&!(n in e)||!s(r))return!1}return!0}function Ra(e,t,i){if("function"!=typeof e)throw new nt(c);return ns(function(){e.apply(o,i)},t)}function Pa(e,t,i,a){var n=-1,o=Xt,r=!0,c=e.length,l=[],u=t.length;if(!c)return l;i&&(t=$t(t,fi(i))),a?(o=Qt,r=!1):t.length>=s&&(o=vi,r=!1,t=new wa(t));e:for(;++n<c;){var p=e[n],h=null==i?p:i(p);if(p=a||0!==p?p:0,r&&h==h){for(var d=u;d--;)if(t[d]===h)continue e;l.push(p)}else o(t,h,a)||l.push(p)}return l}ha.templateSettings={escape:Ee,evaluate:Ce,interpolate:Ie,variable:"",imports:{_:ha}},ha.prototype=ma.prototype,ha.prototype.constructor=ha,ga.prototype=da(ma.prototype),ga.prototype.constructor=ga,fa.prototype=da(ma.prototype),fa.prototype.constructor=fa,ya.prototype.clear=function(){this.__data__=ta?ta(null):{},this.size=0},ya.prototype.delete=function(e){var t=this.has(e)&&delete this.__data__[e];return this.size-=t?1:0,t},ya.prototype.get=function(e){var t=this.__data__;if(ta){var i=t[e];return i===l?o:i}return ut.call(t,e)?t[e]:o},ya.prototype.has=function(e){var t=this.__data__;return ta?t[e]!==o:ut.call(t,e)},ya.prototype.set=function(e,t){var i=this.__data__;return this.size+=this.has(e)?0:1,i[e]=ta&&t===o?l:t,this},va.prototype.clear=function(){this.__data__=[],this.size=0},va.prototype.delete=function(e){var t=this.__data__,i=Ia(t,e);return!(i<0||(i==t.length-1?t.pop():Mt.call(t,i,1),--this.size,0))},va.prototype.get=function(e){var t=this.__data__,i=Ia(t,e);return i<0?o:t[i][1]},va.prototype.has=function(e){return Ia(this.__data__,e)>-1},va.prototype.set=function(e,t){var i=this.__data__,a=Ia(i,e);return a<0?(++this.size,i.push([e,t])):i[a][1]=t,this},ba.prototype.clear=function(){this.size=0,this.__data__={hash:new ya,map:new(Xi||va),string:new ya}},ba.prototype.delete=function(e){var t=Po(this,e).delete(e);return this.size-=t?1:0,t},ba.prototype.get=function(e){return Po(this,e).get(e)},ba.prototype.has=function(e){return Po(this,e).has(e)},ba.prototype.set=function(e,t){var i=Po(this,e),a=i.size;return i.set(e,t),this.size+=i.size==a?0:1,this},wa.prototype.add=wa.prototype.push=function(e){return this.__data__.set(e,l),this},wa.prototype.has=function(e){return this.__data__.has(e)},ja.prototype.clear=function(){this.__data__=new va,this.size=0},ja.prototype.delete=function(e){var t=this.__data__,i=t.delete(e);return this.size=t.size,i},ja.prototype.get=function(e){return this.__data__.get(e)},ja.prototype.has=function(e){return this.__data__.has(e)},ja.prototype.set=function(e,t){var i=this.__data__;if(i instanceof va){var a=i.__data__;if(!Xi||a.length<s-1)return a.push([e,t]),this.size=++i.size,this;i=this.__data__=new ba(a)}return i.set(e,t),this.size=i.size,this};var La=so(Ya),Ua=so(Ga,!0);function Oa(e,t){var i=!0;return La(e,function(e,a,n){return i=!!t(e,a,n)}),i}function Ha(e,t,i){for(var a=-1,n=e.length;++a<n;){var s=e[a],r=t(s);if(null!=r&&(c===o?r==r&&!Mr(r):i(r,c)))var c=r,l=s}return l}function Va(e,t){var i=[];return La(e,function(e,a,n){t(e,a,n)&&i.push(e)}),i}function Na(e,t,i,a,n){var o=-1,s=e.length;for(i||(i=Fo),n||(n=[]);++o<s;){var r=e[o];t>0&&i(r)?t>1?Na(r,t-1,i,a,n):ei(n,r):a||(n[n.length]=r)}return n}var qa=ro(),Fa=ro(!0);function Ya(e,t){return e&&qa(e,t,nc)}function Ga(e,t){return e&&Fa(e,t,nc)}function Ka(e,t){return Zt(t,function(t){return zr(e[t])})}function Ja(e,t){for(var i=0,a=(t=Yn(t,e)).length;null!=e&&i<a;)e=e[us(t[i++])];return i&&i==a?e:o}function Za(e,t,i){var a=t(e);return yr(e)?a:ei(a,i(e))}function Xa(e){return null==e?e===o?oe:Q:ni&&ni in tt(e)?function(e){var t=ut.call(e,ni),i=e[ni];try{e[ni]=o;var a=!0}catch(s){}var n=dt.call(e);return a&&(t?e[ni]=i:delete e[ni]),n}(e):function(e){return dt.call(e)}(e)}function Qa(e,t){return e>t}function $a(e,t){return null!=e&&ut.call(e,t)}function en(e,t){return null!=e&&t in tt(e)}function tn(e,t,i){for(var n=i?Qt:Xt,s=e[0].length,r=e.length,c=r,l=a(r),u=1/0,p=[];c--;){var h=e[c];c&&t&&(h=$t(h,fi(t))),u=Fi(h.length,u),l[c]=!i&&(t||s>=120&&h.length>=120)?new wa(c&&h):o}h=e[0];var d=-1,m=l[0];e:for(;++d<s&&p.length<u;){var g=h[d],f=t?t(g):g;if(g=i||0!==g?g:0,!(m?vi(m,f):n(p,f,i))){for(c=r;--c;){var y=l[c];if(!(y?vi(y,f):n(e[c],f,i)))continue e}m&&m.push(f),p.push(g)}}return p}function an(e,t,i){var a=null==(e=ts(e,t=Yn(t,e)))?e:e[us(xs(t))];return null==a?o:Ft(a,e,i)}function nn(e){return Ir(e)&&Xa(e)==H}function on(e,t,i,a,n){return e===t||(null==e||null==t||!Ir(e)&&!Ir(t)?e!=e&&t!=t:function(e,t,i,a,n,s){var r=yr(e),c=yr(t),l=r?V:Vo(e),u=c?V:Vo(t),p=(l=l==H?$:l)==$,h=(u=u==H?$:u)==$,d=l==u;if(d&&jr(e)){if(!jr(t))return!1;r=!0,p=!1}if(d&&!p)return s||(s=new ja),r||Rr(e)?Ao(e,t,i,a,n,s):function(e,t,i,a,n,o,s){switch(i){case le:if(e.byteLength!=t.byteLength||e.byteOffset!=t.byteOffset)return!1;e=e.buffer,t=t.buffer;case ce:return!(e.byteLength!=t.byteLength||!o(new kt(e),new kt(t)));case q:case F:case X:return dr(+e,+t);case G:return e.name==t.name&&e.message==t.message;case te:case ae:return e==t+"";case Z:var r=Si;case ie:var c=a&g;if(r||(r=Ii),e.size!=t.size&&!c)return!1;var l=s.get(e);if(l)return l==t;a|=f,s.set(e,t);var u=Ao(r(e),r(t),a,n,o,s);return s.delete(e),u;case ne:if(ua)return ua.call(e)==ua.call(t)}return!1}(e,t,l,i,a,n,s);if(!(i&g)){var m=p&&ut.call(e,"__wrapped__"),y=h&&ut.call(t,"__wrapped__");if(m||y){var v=m?e.value():e,b=y?t.value():t;return s||(s=new ja),n(v,b,i,a,s)}}return!!d&&(s||(s=new ja),function(e,t,i,a,n,s){var r=i&g,c=To(e),l=c.length,u=To(t).length;if(l!=u&&!r)return!1;for(var p=l;p--;){var h=c[p];if(!(r?h in t:ut.call(t,h)))return!1}var d=s.get(e),m=s.get(t);if(d&&m)return d==t&&m==e;var f=!0;s.set(e,t),s.set(t,e);for(var y=r;++p<l;){h=c[p];var v=e[h],b=t[h];if(a)var w=r?a(b,v,h,t,e,s):a(v,b,h,e,t,s);if(!(w===o?v===b||n(v,b,i,a,s):w)){f=!1;break}y||(y="constructor"==h)}if(f&&!y){var j=e.constructor,k=t.constructor;j!=k&&"constructor"in e&&"constructor"in t&&!("function"==typeof j&&j instanceof j&&"function"==typeof k&&k instanceof k)&&(f=!1)}return s.delete(e),s.delete(t),f}(e,t,i,a,n,s))}(e,t,i,a,on,n))}function sn(e,t,i,a){var n=i.length,s=n,r=!a;if(null==e)return!s;for(e=tt(e);n--;){var c=i[n];if(r&&c[2]?c[1]!==e[c[0]]:!(c[0]in e))return!1}for(;++n<s;){var l=(c=i[n])[0],u=e[l],p=c[1];if(r&&c[2]){if(u===o&&!(l in e))return!1}else{var h=new ja;if(a)var d=a(u,p,l,e,t,h);if(!(d===o?on(p,u,g|f,a,h):d))return!1}}return!0}function rn(e){return!(!Cr(e)||(t=e,ht&&ht in t))&&(zr(e)?ft:Ye).test(ps(e));var t}function cn(e){return"function"==typeof e?e:null==e?_c:"object"==typeof e?yr(e)?mn(e[0],e[1]):dn(e):Uc(e)}function ln(e){if(!Xo(e))return Ni(e);var t=[];for(var i in tt(e))ut.call(e,i)&&"constructor"!=i&&t.push(i);return t}function un(e){if(!Cr(e))return function(e){var t=[];if(null!=e)for(var i in tt(e))t.push(i);return t}(e);var t=Xo(e),i=[];for(var a in e)("constructor"!=a||!t&&ut.call(e,a))&&i.push(a);return i}function pn(e,t){return e<t}function hn(e,t){var i=-1,n=br(e)?a(e.length):[];return La(e,function(e,a,o){n[++i]=t(e,a,o)}),n}function dn(e){var t=Lo(e);return 1==t.length&&t[0][2]?$o(t[0][0],t[0][1]):function(i){return i===e||sn(i,e,t)}}function mn(e,t){return Ko(e)&&Qo(t)?$o(us(e),t):function(i){var a=$r(i,e);return a===o&&a===t?ec(i,e):on(t,a,g|f)}}function gn(e,t,i,a,n){e!==t&&qa(t,function(s,r){if(n||(n=new ja),Cr(s))!function(e,t,i,a,n,s,r){var c=is(e,i),l=is(t,i),u=r.get(l);if(u)Ea(e,i,u);else{var p=s?s(c,l,i+"",e,t,r):o,h=p===o;if(h){var d=yr(l),m=!d&&jr(l),g=!d&&!m&&Rr(l);p=l,d||m||g?yr(c)?p=c:wr(c)?p=io(c):m?(h=!1,p=Zn(l,!0)):g?(h=!1,p=Qn(l,!0)):p=[]:Tr(l)||fr(l)?(p=c,fr(c)?p=qr(c):Cr(c)&&!zr(c)||(p=qo(l))):h=!1}h&&(r.set(l,p),n(p,l,a,s,r),r.delete(l)),Ea(e,i,p)}}(e,t,r,i,gn,a,n);else{var c=a?a(is(e,r),s,r+"",e,t,n):o;c===o&&(c=s),Ea(e,r,c)}},oc)}function fn(e,t){var i=e.length;if(i)return Yo(t+=t<0?i:0,i)?e[t]:o}function yn(e,t,i){t=t.length?$t(t,function(e){return yr(e)?function(t){return Ja(t,1===e.length?e[0]:e)}:e}):[_c];var a=-1;return t=$t(t,fi(Ro())),function(e,t){var i=e.length;for(e.sort(t);i--;)e[i]=e[i].value;return e}(hn(e,function(e,i,n){return{criteria:$t(t,function(t){return t(e)}),index:++a,value:e}}),function(e,t){return function(e,t,i){for(var a=-1,n=e.criteria,o=t.criteria,s=n.length,r=i.length;++a<s;){var c=$n(n[a],o[a]);if(c){if(a>=r)return c;var l=i[a];return c*("desc"==l?-1:1)}}return e.index-t.index}(e,t,i)})}function vn(e,t,i){for(var a=-1,n=t.length,o={};++a<n;){var s=t[a],r=Ja(e,s);i(r,s)&&En(o,Yn(s,e),r)}return o}function bn(e,t,i,a){var n=a?ci:ri,o=-1,s=t.length,r=e;for(e===t&&(t=io(t)),i&&(r=$t(e,fi(i)));++o<s;)for(var c=0,l=t[o],u=i?i(l):l;(c=n(r,u,c,a))>-1;)r!==e&&Mt.call(r,c,1),Mt.call(e,c,1);return e}function wn(e,t){for(var i=e?t.length:0,a=i-1;i--;){var n=t[i];if(i==a||n!==o){var o=n;Yo(n)?Mt.call(e,n,1):Ln(e,n)}}return e}function jn(e,t){return e+Li(Ki()*(t-e+1))}function kn(e,t){var i="";if(!e||t<1||t>D)return i;do{t%2&&(i+=e),(t=Li(t/2))&&(e+=e)}while(t);return i}function xn(e,t){return os(es(e,t,_c),e+"")}function zn(e){return xa(dc(e))}function Sn(e,t){var i=dc(e);return cs(i,Ba(t,0,i.length))}function En(e,t,i,a){if(!Cr(e))return e;for(var n=-1,s=(t=Yn(t,e)).length,r=s-1,c=e;null!=c&&++n<s;){var l=us(t[n]),u=i;if("__proto__"===l||"constructor"===l||"prototype"===l)return e;if(n!=r){var p=c[l];(u=a?a(p,l,c):o)===o&&(u=Cr(p)?p:Yo(t[n+1])?[]:{})}Ca(c,l,u),c=c[l]}return e}var Cn=ia?function(e,t){return ia.set(e,t),e}:_c,In=hi?function(e,t){return hi(e,"toString",{configurable:!0,enumerable:!1,value:Cc(t),writable:!0})}:_c;function An(e){return cs(dc(e))}function _n(e,t,i){var n=-1,o=e.length;t<0&&(t=-t>o?0:o+t),(i=i>o?o:i)<0&&(i+=o),o=t>i?0:i-t>>>0,t>>>=0;for(var s=a(o);++n<o;)s[n]=e[n+t];return s}function Tn(e,t){var i;return La(e,function(e,a,n){return!(i=t(e,a,n))}),!!i}function Wn(e,t,i){var a=0,n=null==e?a:e.length;if("number"==typeof t&&t==t&&n<=U){for(;a<n;){var o=a+n>>>1,s=e[o];null!==s&&!Mr(s)&&(i?s<=t:s<t)?a=o+1:n=o}return n}return Bn(e,t,_c,i)}function Bn(e,t,i,a){var n=0,s=null==e?0:e.length;if(0===s)return 0;for(var r=(t=i(t))!=t,c=null===t,l=Mr(t),u=t===o;n<s;){var p=Li((n+s)/2),h=i(e[p]),d=h!==o,m=null===h,g=h==h,f=Mr(h);if(r)var y=a||g;else y=u?g&&(a||d):c?g&&d&&(a||!m):l?g&&d&&!m&&(a||!f):!m&&!f&&(a?h<=t:h<t);y?n=p+1:s=p}return Fi(s,L)}function Dn(e,t){for(var i=-1,a=e.length,n=0,o=[];++i<a;){var s=e[i],r=t?t(s):s;if(!i||!dr(r,c)){var c=r;o[n++]=0===s?0:s}}return o}function Mn(e){return"number"==typeof e?e:Mr(e)?R:+e}function Rn(e){if("string"==typeof e)return e;if(yr(e))return $t(e,Rn)+"";if(Mr(e))return pa?pa.call(e):"";var t=e+"";return"0"==t&&1/e==-B?"-0":t}function Pn(e,t,i){var a=-1,n=Xt,o=e.length,r=!0,c=[],l=c;if(i)r=!1,n=Qt;else if(o>=s){var u=t?null:xo(e);if(u)return Ii(u);r=!1,n=vi,l=new wa}else l=t?[]:c;e:for(;++a<o;){var p=e[a],h=t?t(p):p;if(p=i||0!==p?p:0,r&&h==h){for(var d=l.length;d--;)if(l[d]===h)continue e;t&&l.push(h),c.push(p)}else n(l,h,i)||(l!==c&&l.push(h),c.push(p))}return c}function Ln(e,t){return null==(e=ts(e,t=Yn(t,e)))||delete e[us(xs(t))]}function Un(e,t,i,a){return En(e,t,i(Ja(e,t)),a)}function On(e,t,i,a){for(var n=e.length,o=a?n:-1;(a?o--:++o<n)&&t(e[o],o,e););return i?_n(e,a?0:o,a?o+1:n):_n(e,a?o+1:0,a?n:o)}function Hn(e,t){var i=e;return i instanceof fa&&(i=i.value()),ti(t,function(e,t){return t.func.apply(t.thisArg,ei([e],t.args))},i)}function Vn(e,t,i){var n=e.length;if(n<2)return n?Pn(e[0]):[];for(var o=-1,s=a(n);++o<n;)for(var r=e[o],c=-1;++c<n;)c!=o&&(s[o]=Pa(s[o]||r,e[c],t,i));return Pn(Na(s,1),t,i)}function Nn(e,t,i){for(var a=-1,n=e.length,s=t.length,r={};++a<n;){var c=a<s?t[a]:o;i(r,e[a],c)}return r}function qn(e){return wr(e)?e:[]}function Fn(e){return"function"==typeof e?e:_c}function Yn(e,t){return yr(e)?e:Ko(e,t)?[e]:ls(Fr(e))}var Gn=xn;function Kn(e,t,i){var a=e.length;return i=i===o?a:i,!t&&i>=a?e:_n(e,t,i)}var Jn=Di||function(e){return Bt.clearTimeout(e)};function Zn(e,t){if(t)return e.slice();var i=e.length,a=It?It(i):new e.constructor(i);return e.copy(a),a}function Xn(e){var t=new e.constructor(e.byteLength);return new kt(t).set(new kt(e)),t}function Qn(e,t){var i=t?Xn(e.buffer):e.buffer;return new e.constructor(i,e.byteOffset,e.length)}function $n(e,t){if(e!==t){var i=e!==o,a=null===e,n=e==e,s=Mr(e),r=t!==o,c=null===t,l=t==t,u=Mr(t);if(!c&&!u&&!s&&e>t||s&&r&&l&&!c&&!u||a&&r&&l||!i&&l||!n)return 1;if(!a&&!s&&!u&&e<t||u&&i&&n&&!a&&!s||c&&i&&n||!r&&n||!l)return-1}return 0}function eo(e,t,i,n){for(var o=-1,s=e.length,r=i.length,c=-1,l=t.length,u=qi(s-r,0),p=a(l+u),h=!n;++c<l;)p[c]=t[c];for(;++o<r;)(h||o<s)&&(p[i[o]]=e[o]);for(;u--;)p[c++]=e[o++];return p}function to(e,t,i,n){for(var o=-1,s=e.length,r=-1,c=i.length,l=-1,u=t.length,p=qi(s-c,0),h=a(p+u),d=!n;++o<p;)h[o]=e[o];for(var m=o;++l<u;)h[m+l]=t[l];for(;++r<c;)(d||o<s)&&(h[m+i[r]]=e[o++]);return h}function io(e,t){var i=-1,n=e.length;for(t||(t=a(n));++i<n;)t[i]=e[i];return t}function ao(e,t,i,a){var n=!i;i||(i={});for(var s=-1,r=t.length;++s<r;){var c=t[s],l=a?a(i[c],e[c],c,i,e):o;l===o&&(l=e[c]),n?Ta(i,c,l):Ca(i,c,l)}return i}function no(e,t){return function(i,a){var n=yr(i)?Yt:Aa,o=t?t():{};return n(i,e,Ro(a,2),o)}}function oo(e){return xn(function(t,i){var a=-1,n=i.length,s=n>1?i[n-1]:o,r=n>2?i[2]:o;for(s=e.length>3&&"function"==typeof s?(n--,s):o,r&&Go(i[0],i[1],r)&&(s=n<3?o:s,n=1),t=tt(t);++a<n;){var c=i[a];c&&e(t,c,a,s)}return t})}function so(e,t){return function(i,a){if(null==i)return i;if(!br(i))return e(i,a);for(var n=i.length,o=t?n:-1,s=tt(i);(t?o--:++o<n)&&!1!==a(s[o],o,s););return i}}function ro(e){return function(t,i,a){for(var n=-1,o=tt(t),s=a(t),r=s.length;r--;){var c=s[e?r:++n];if(!1===i(o[c],c,o))break}return t}}function co(e){return function(t){var i=zi(t=Fr(t))?Ti(t):o,a=i?i[0]:t.charAt(0),n=i?Kn(i,1).join(""):t.slice(1);return a[e]()+n}}function lo(e){return function(t){return ti(zc(fc(t).replace(vt,"")),e,"")}}function uo(e){return function(){var t=arguments;switch(t.length){case 0:return new e;case 1:return new e(t[0]);case 2:return new e(t[0],t[1]);case 3:return new e(t[0],t[1],t[2]);case 4:return new e(t[0],t[1],t[2],t[3]);case 5:return new e(t[0],t[1],t[2],t[3],t[4]);case 6:return new e(t[0],t[1],t[2],t[3],t[4],t[5]);case 7:return new e(t[0],t[1],t[2],t[3],t[4],t[5],t[6])}var i=da(e.prototype),a=e.apply(i,t);return Cr(a)?a:i}}function po(e){return function(t,i,a){var n=tt(t);if(!br(t)){var s=Ro(i,3);t=nc(t),i=function(e){return s(n[e],e,n)}}var r=e(t,i,a);return r>-1?n[s?t[r]:r]:o}}function ho(e){return _o(function(t){var i=t.length,a=i,n=ga.prototype.thru;for(e&&t.reverse();a--;){var s=t[a];if("function"!=typeof s)throw new nt(c);if(n&&!r&&"wrapper"==Do(s))var r=new ga([],!0)}for(a=r?a:i;++a<i;){var l=Do(s=t[a]),u="wrapper"==l?Bo(s):o;r=u&&Jo(u[0])&&u[1]==(z|w|k|S)&&!u[4].length&&1==u[9]?r[Do(u[0])].apply(r,u[3]):1==s.length&&Jo(s)?r[l]():r.thru(s)}return function(){var e=arguments,a=e[0];if(r&&1==e.length&&yr(a))return r.plant(a).value();for(var n=0,o=i?t[n].apply(this,e):a;++n<i;)o=t[n].call(this,o);return o}})}function mo(e,t,i,n,s,r,c,l,u,p){var h=t&z,d=t&y,m=t&v,g=t&(w|j),f=t&E,b=m?o:uo(e);return function y(){for(var v=arguments.length,w=a(v),j=v;j--;)w[j]=arguments[j];if(g)var k=Mo(y),x=function(e,t){for(var i=e.length,a=0;i--;)e[i]===t&&++a;return a}(w,k);if(n&&(w=eo(w,n,s,g)),r&&(w=to(w,r,c,g)),v-=x,g&&v<p){var z=Ci(w,k);return jo(e,t,mo,y.placeholder,i,w,z,l,u,p-v)}var S=d?i:this,E=m?S[e]:e;return v=w.length,l?w=function(e,t){for(var i=e.length,a=Fi(t.length,i),n=io(e);a--;){var s=t[a];e[a]=Yo(s,i)?n[s]:o}return e}(w,l):f&&v>1&&w.reverse(),h&&u<v&&(w.length=u),this&&this!==Bt&&this instanceof y&&(E=b||uo(E)),E.apply(S,w)}}function go(e,t){return function(i,a){return function(e,t,i,a){return Ya(e,function(e,n,o){t(a,i(e),n,o)}),a}(i,e,t(a),{})}}function fo(e,t){return function(i,a){var n;if(i===o&&a===o)return t;if(i!==o&&(n=i),a!==o){if(n===o)return a;"string"==typeof i||"string"==typeof a?(i=Rn(i),a=Rn(a)):(i=Mn(i),a=Mn(a)),n=e(i,a)}return n}}function yo(e){return _o(function(t){return t=$t(t,fi(Ro())),xn(function(i){var a=this;return e(t,function(e){return Ft(e,a,i)})})})}function vo(e,t){var i=(t=t===o?" ":Rn(t)).length;if(i<2)return i?kn(t,e):t;var a=kn(t,Pi(e/_i(t)));return zi(t)?Kn(Ti(a),0,e).join(""):a.slice(0,e)}function bo(e){return function(t,i,n){return n&&"number"!=typeof n&&Go(t,i,n)&&(i=n=o),t=Or(t),i===o?(i=t,t=0):i=Or(i),function(e,t,i,n){for(var o=-1,s=qi(Pi((t-e)/(i||1)),0),r=a(s);s--;)r[n?s:++o]=e,e+=i;return r}(t,i,n=n===o?t<i?1:-1:Or(n),e)}}function wo(e){return function(t,i){return"string"==typeof t&&"string"==typeof i||(t=Nr(t),i=Nr(i)),e(t,i)}}function jo(e,t,i,a,n,s,r,c,l,u){var p=t&w;t|=p?k:x,(t&=~(p?x:k))&b||(t&=~(y|v));var h=[e,t,n,p?s:o,p?r:o,p?o:s,p?o:r,c,l,u],d=i.apply(o,h);return Jo(e)&&as(d,h),d.placeholder=a,ss(d,e,t)}function ko(e){var t=et[e];return function(e,i){if(e=Nr(e),(i=null==i?0:Fi(Hr(i),292))&&Hi(e)){var a=(Fr(e)+"e").split("e");return+((a=(Fr(t(a[0]+"e"+(+a[1]+i)))+"e").split("e"))[0]+"e"+(+a[1]-i))}return t(e)}}var xo=$i&&1/Ii(new $i([,-0]))[1]==B?function(e){return new $i(e)}:Mc;function zo(e){return function(t){var i=Vo(t);return i==Z?Si(t):i==ie?Ai(t):function(e,t){return $t(t,function(t){return[t,e[t]]})}(t,e(t))}}function So(e,t,i,n,s,r,l,u){var h=t&v;if(!h&&"function"!=typeof e)throw new nt(c);var d=n?n.length:0;if(d||(t&=~(k|x),n=s=o),l=l===o?l:qi(Hr(l),0),u=u===o?u:Hr(u),d-=s?s.length:0,t&x){var m=n,g=s;n=s=o}var f=h?o:Bo(e),E=[e,t,i,n,s,m,g,r,l,u];if(f&&function(e,t){var i=e[1],a=t[1],n=i|a,o=n<(y|v|z),s=a==z&&i==w||a==z&&i==S&&e[7].length<=t[8]||a==(z|S)&&t[7].length<=t[8]&&i==w;if(!o&&!s)return e;a&y&&(e[2]=t[2],n|=i&y?0:b);var r=t[3];if(r){var c=e[3];e[3]=c?eo(c,r,t[4]):r,e[4]=c?Ci(e[3],p):t[4]}(r=t[5])&&(c=e[5],e[5]=c?to(c,r,t[6]):r,e[6]=c?Ci(e[5],p):t[6]),(r=t[7])&&(e[7]=r),a&z&&(e[8]=null==e[8]?t[8]:Fi(e[8],t[8])),null==e[9]&&(e[9]=t[9]),e[0]=t[0],e[1]=n}(E,f),e=E[0],t=E[1],i=E[2],n=E[3],s=E[4],!(u=E[9]=E[9]===o?h?0:e.length:qi(E[9]-d,0))&&t&(w|j)&&(t&=~(w|j)),t&&t!=y)C=t==w||t==j?function(e,t,i){var n=uo(e);return function s(){for(var r=arguments.length,c=a(r),l=r,u=Mo(s);l--;)c[l]=arguments[l];var p=r<3&&c[0]!==u&&c[r-1]!==u?[]:Ci(c,u);return(r-=p.length)<i?jo(e,t,mo,s.placeholder,o,c,p,o,o,i-r):Ft(this&&this!==Bt&&this instanceof s?n:e,this,c)}}(e,t,u):t!=k&&t!=(y|k)||s.length?mo.apply(o,E):function(e,t,i,n){var o=t&y,s=uo(e);return function t(){for(var r=-1,c=arguments.length,l=-1,u=n.length,p=a(u+c),h=this&&this!==Bt&&this instanceof t?s:e;++l<u;)p[l]=n[l];for(;c--;)p[l++]=arguments[++r];return Ft(h,o?i:this,p)}}(e,t,i,n);else var C=function(e,t,i){var a=t&y,n=uo(e);return function t(){return(this&&this!==Bt&&this instanceof t?n:e).apply(a?i:this,arguments)}}(e,t,i);return ss((f?Cn:as)(C,E),e,t)}function Eo(e,t,i,a){return e===o||dr(e,rt[i])&&!ut.call(a,i)?t:e}function Co(e,t,i,a,n,s){return Cr(e)&&Cr(t)&&(s.set(t,e),gn(e,t,o,Co,s),s.delete(t)),e}function Io(e){return Tr(e)?o:e}function Ao(e,t,i,a,n,s){var r=i&g,c=e.length,l=t.length;if(c!=l&&!(r&&l>c))return!1;var u=s.get(e),p=s.get(t);if(u&&p)return u==t&&p==e;var h=-1,d=!0,m=i&f?new wa:o;for(s.set(e,t),s.set(t,e);++h<c;){var y=e[h],v=t[h];if(a)var b=r?a(v,y,h,t,e,s):a(y,v,h,e,t,s);if(b!==o){if(b)continue;d=!1;break}if(m){if(!ai(t,function(e,t){if(!vi(m,t)&&(y===e||n(y,e,i,a,s)))return m.push(t)})){d=!1;break}}else if(y!==v&&!n(y,v,i,a,s)){d=!1;break}}return s.delete(e),s.delete(t),d}function _o(e){return os(es(e,o,vs),e+"")}function To(e){return Za(e,nc,Oo)}function Wo(e){return Za(e,oc,Ho)}var Bo=ia?function(e){return ia.get(e)}:Mc;function Do(e){for(var t=e.name+"",i=aa[t],a=ut.call(aa,t)?i.length:0;a--;){var n=i[a],o=n.func;if(null==o||o==e)return n.name}return t}function Mo(e){return(ut.call(ha,"placeholder")?ha:e).placeholder}function Ro(){var e=ha.iteratee||Tc;return e=e===Tc?cn:e,arguments.length?e(arguments[0],arguments[1]):e}function Po(e,t){var i,a,n=e.__data__;return("string"==(a=typeof(i=t))||"number"==a||"symbol"==a||"boolean"==a?"__proto__"!==i:null===i)?n["string"==typeof t?"string":"hash"]:n.map}function Lo(e){for(var t=nc(e),i=t.length;i--;){var a=t[i],n=e[a];t[i]=[a,n,Qo(n)]}return t}function Uo(e,t){var i=function(e,t){return null==e?o:e[t]}(e,t);return rn(i)?i:o}var Oo=Ui?function(e){return null==e?[]:(e=tt(e),Zt(Ui(e),function(t){return Dt.call(e,t)}))}:Vc,Ho=Ui?function(e){for(var t=[];e;)ei(t,Oo(e)),e=Tt(e);return t}:Vc,Vo=Xa;function No(e,t,i){for(var a=-1,n=(t=Yn(t,e)).length,o=!1;++a<n;){var s=us(t[a]);if(!(o=null!=e&&i(e,s)))break;e=e[s]}return o||++a!=n?o:!!(n=null==e?0:e.length)&&Er(n)&&Yo(s,n)&&(yr(e)||fr(e))}function qo(e){return"function"!=typeof e.constructor||Xo(e)?{}:da(Tt(e))}function Fo(e){return yr(e)||fr(e)||!!(Pt&&e&&e[Pt])}function Yo(e,t){var i=typeof e;return!!(t=null==t?D:t)&&("number"==i||"symbol"!=i&&Ke.test(e))&&e>-1&&e%1==0&&e<t}function Go(e,t,i){if(!Cr(i))return!1;var a=typeof t;return!!("number"==a?br(i)&&Yo(t,i.length):"string"==a&&t in i)&&dr(i[t],e)}function Ko(e,t){if(yr(e))return!1;var i=typeof e;return!("number"!=i&&"symbol"!=i&&"boolean"!=i&&null!=e&&!Mr(e))||_e.test(e)||!Ae.test(e)||null!=t&&e in tt(t)}function Jo(e){var t=Do(e),i=ha[t];if("function"!=typeof i||!(t in fa.prototype))return!1;if(e===i)return!0;var a=Bo(i);return!!a&&e===a[0]}(Zi&&Vo(new Zi(new ArrayBuffer(1)))!=le||Xi&&Vo(new Xi)!=Z||Qi&&"[object Promise]"!=Vo(Qi.resolve())||$i&&Vo(new $i)!=ie||ea&&Vo(new ea)!=se)&&(Vo=function(e){var t=Xa(e),i=t==$?e.constructor:o,a=i?ps(i):"";if(a)switch(a){case na:return le;case oa:return Z;case sa:return"[object Promise]";case ra:return ie;case ca:return se}return t});var Zo=ct?zr:Nc;function Xo(e){var t=e&&e.constructor;return e===("function"==typeof t&&t.prototype||rt)}function Qo(e){return e==e&&!Cr(e)}function $o(e,t){return function(i){return null!=i&&i[e]===t&&(t!==o||e in tt(i))}}function es(e,t,i){return t=qi(t===o?e.length-1:t,0),function(){for(var n=arguments,o=-1,s=qi(n.length-t,0),r=a(s);++o<s;)r[o]=n[t+o];o=-1;for(var c=a(t+1);++o<t;)c[o]=n[o];return c[t]=i(r),Ft(e,this,c)}}function ts(e,t){return t.length<2?e:Ja(e,_n(t,0,-1))}function is(e,t){if(("constructor"!==t||"function"!=typeof e[t])&&"__proto__"!=t)return e[t]}var as=rs(Cn),ns=Ri||function(e,t){return Bt.setTimeout(e,t)},os=rs(In);function ss(e,t,i){var a=t+"";return os(e,function(e,t){var i=t.length;if(!i)return e;var a=i-1;return t[a]=(i>1?"& ":"")+t[a],t=t.join(i>2?", ":" "),e.replace(Pe,"{\n/* [wrapped with "+t+"] */\n")}(a,function(e,t){return Gt(O,function(i){var a="_."+i[0];t&i[1]&&!Xt(e,a)&&e.push(a)}),e.sort()}(function(e){var t=e.match(Le);return t?t[1].split(Ue):[]}(a),i)))}function rs(e){var t=0,i=0;return function(){var a=Yi(),n=_-(a-i);if(i=a,n>0){if(++t>=A)return arguments[0]}else t=0;return e.apply(o,arguments)}}function cs(e,t){var i=-1,a=e.length,n=a-1;for(t=t===o?a:t;++i<t;){var s=jn(i,n),r=e[s];e[s]=e[i],e[i]=r}return e.length=t,e}var ls=function(e){var t=rr(e,function(e){return i.size===u&&i.clear(),e}),i=t.cache;return t}(function(e){var t=[];return 46===e.charCodeAt(0)&&t.push(""),e.replace(Te,function(e,i,a,n){t.push(a?n.replace(He,"$1"):i||e)}),t});function us(e){if("string"==typeof e||Mr(e))return e;var t=e+"";return"0"==t&&1/e==-B?"-0":t}function ps(e){if(null!=e){try{return lt.call(e)}catch(t){}try{return e+""}catch(t){}}return""}function hs(e){if(e instanceof fa)return e.clone();var t=new ga(e.__wrapped__,e.__chain__);return t.__actions__=io(e.__actions__),t.__index__=e.__index__,t.__values__=e.__values__,t}var ds=xn(function(e,t){return wr(e)?Pa(e,Na(t,1,wr,!0)):[]}),ms=xn(function(e,t){var i=xs(t);return wr(i)&&(i=o),wr(e)?Pa(e,Na(t,1,wr,!0),Ro(i,2)):[]}),gs=xn(function(e,t){var i=xs(t);return wr(i)&&(i=o),wr(e)?Pa(e,Na(t,1,wr,!0),o,i):[]});function fs(e,t,i){var a=null==e?0:e.length;if(!a)return-1;var n=null==i?0:Hr(i);return n<0&&(n=qi(a+n,0)),si(e,Ro(t,3),n)}function ys(e,t,i){var a=null==e?0:e.length;if(!a)return-1;var n=a-1;return i!==o&&(n=Hr(i),n=i<0?qi(a+n,0):Fi(n,a-1)),si(e,Ro(t,3),n,!0)}function vs(e){return null!=e&&e.length?Na(e,1):[]}function bs(e){return e&&e.length?e[0]:o}var ws=xn(function(e){var t=$t(e,qn);return t.length&&t[0]===e[0]?tn(t):[]}),js=xn(function(e){var t=xs(e),i=$t(e,qn);return t===xs(i)?t=o:i.pop(),i.length&&i[0]===e[0]?tn(i,Ro(t,2)):[]}),ks=xn(function(e){var t=xs(e),i=$t(e,qn);return(t="function"==typeof t?t:o)&&i.pop(),i.length&&i[0]===e[0]?tn(i,o,t):[]});function xs(e){var t=null==e?0:e.length;return t?e[t-1]:o}var zs=xn(Ss);function Ss(e,t){return e&&e.length&&t&&t.length?bn(e,t):e}var Es=_o(function(e,t){var i=null==e?0:e.length,a=Wa(e,t);return wn(e,$t(t,function(e){return Yo(e,i)?+e:e}).sort($n)),a});function Cs(e){return null==e?e:Ji.call(e)}var Is=xn(function(e){return Pn(Na(e,1,wr,!0))}),As=xn(function(e){var t=xs(e);return wr(t)&&(t=o),Pn(Na(e,1,wr,!0),Ro(t,2))}),_s=xn(function(e){var t=xs(e);return t="function"==typeof t?t:o,Pn(Na(e,1,wr,!0),o,t)});function Ts(e){if(!e||!e.length)return[];var t=0;return e=Zt(e,function(e){if(wr(e))return t=qi(e.length,t),!0}),gi(t,function(t){return $t(e,pi(t))})}function Ws(e,t){if(!e||!e.length)return[];var i=Ts(e);return null==t?i:$t(i,function(e){return Ft(t,o,e)})}var Bs=xn(function(e,t){return wr(e)?Pa(e,t):[]}),Ds=xn(function(e){return Vn(Zt(e,wr))}),Ms=xn(function(e){var t=xs(e);return wr(t)&&(t=o),Vn(Zt(e,wr),Ro(t,2))}),Rs=xn(function(e){var t=xs(e);return t="function"==typeof t?t:o,Vn(Zt(e,wr),o,t)}),Ps=xn(Ts);var Ls=xn(function(e){var t=e.length,i=t>1?e[t-1]:o;return i="function"==typeof i?(e.pop(),i):o,Ws(e,i)});function Us(e){var t=ha(e);return t.__chain__=!0,t}function Os(e,t){return t(e)}var Hs=_o(function(e){var t=e.length,i=t?e[0]:0,a=this.__wrapped__,n=function(t){return Wa(t,e)};return!(t>1||this.__actions__.length)&&a instanceof fa&&Yo(i)?((a=a.slice(i,+i+(t?1:0))).__actions__.push({func:Os,args:[n],thisArg:o}),new ga(a,this.__chain__).thru(function(e){return t&&!e.length&&e.push(o),e})):this.thru(n)});var Vs=no(function(e,t,i){ut.call(e,i)?++e[i]:Ta(e,i,1)});var Ns=po(fs),qs=po(ys);function Fs(e,t){return(yr(e)?Gt:La)(e,Ro(t,3))}function Ys(e,t){return(yr(e)?Kt:Ua)(e,Ro(t,3))}var Gs=no(function(e,t,i){ut.call(e,i)?e[i].push(t):Ta(e,i,[t])});var Ks=xn(function(e,t,i){var n=-1,o="function"==typeof t,s=br(e)?a(e.length):[];return La(e,function(e){s[++n]=o?Ft(t,e,i):an(e,t,i)}),s}),Js=no(function(e,t,i){Ta(e,i,t)});function Zs(e,t){return(yr(e)?$t:hn)(e,Ro(t,3))}var Xs=no(function(e,t,i){e[i?0:1].push(t)},function(){return[[],[]]});var Qs=xn(function(e,t){if(null==e)return[];var i=t.length;return i>1&&Go(e,t[0],t[1])?t=[]:i>2&&Go(t[0],t[1],t[2])&&(t=[t[0]]),yn(e,Na(t,1),[])}),$s=Mi||function(){return Bt.Date.now()};function er(e,t,i){return t=i?o:t,t=e&&null==t?e.length:t,So(e,z,o,o,o,o,t)}function tr(e,t){var i;if("function"!=typeof t)throw new nt(c);return e=Hr(e),function(){return--e>0&&(i=t.apply(this,arguments)),e<=1&&(t=o),i}}var ir=xn(function(e,t,i){var a=y;if(i.length){var n=Ci(i,Mo(ir));a|=k}return So(e,a,t,i,n)}),ar=xn(function(e,t,i){var a=y|v;if(i.length){var n=Ci(i,Mo(ar));a|=k}return So(t,a,e,i,n)});function nr(e,t,i){var a,n,s,r,l,u,p=0,h=!1,d=!1,m=!0;if("function"!=typeof e)throw new nt(c);function g(t){var i=a,s=n;return a=n=o,p=t,r=e.apply(s,i)}function f(e){var i=e-u;return u===o||i>=t||i<0||d&&e-p>=s}function y(){var e=$s();if(f(e))return v(e);l=ns(y,function(e){var i=t-(e-u);return d?Fi(i,s-(e-p)):i}(e))}function v(e){return l=o,m&&a?g(e):(a=n=o,r)}function b(){var e=$s(),i=f(e);if(a=arguments,n=this,u=e,i){if(l===o)return function(e){return p=e,l=ns(y,t),h?g(e):r}(u);if(d)return Jn(l),l=ns(y,t),g(u)}return l===o&&(l=ns(y,t)),r}return t=Nr(t)||0,Cr(i)&&(h=!!i.leading,s=(d="maxWait"in i)?qi(Nr(i.maxWait)||0,t):s,m="trailing"in i?!!i.trailing:m),b.cancel=function(){l!==o&&Jn(l),p=0,a=u=n=l=o},b.flush=function(){return l===o?r:v($s())},b}var or=xn(function(e,t){return Ra(e,1,t)}),sr=xn(function(e,t,i){return Ra(e,Nr(t)||0,i)});function rr(e,t){if("function"!=typeof e||null!=t&&"function"!=typeof t)throw new nt(c);var i=function(){var a=arguments,n=t?t.apply(this,a):a[0],o=i.cache;if(o.has(n))return o.get(n);var s=e.apply(this,a);return i.cache=o.set(n,s)||o,s};return i.cache=new(rr.Cache||ba),i}function cr(e){if("function"!=typeof e)throw new nt(c);return function(){var t=arguments;switch(t.length){case 0:return!e.call(this);case 1:return!e.call(this,t[0]);case 2:return!e.call(this,t[0],t[1]);case 3:return!e.call(this,t[0],t[1],t[2])}return!e.apply(this,t)}}rr.Cache=ba;var lr=Gn(function(e,t){var i=(t=1==t.length&&yr(t[0])?$t(t[0],fi(Ro())):$t(Na(t,1),fi(Ro()))).length;return xn(function(a){for(var n=-1,o=Fi(a.length,i);++n<o;)a[n]=t[n].call(this,a[n]);return Ft(e,this,a)})}),ur=xn(function(e,t){var i=Ci(t,Mo(ur));return So(e,k,o,t,i)}),pr=xn(function(e,t){var i=Ci(t,Mo(pr));return So(e,x,o,t,i)}),hr=_o(function(e,t){return So(e,S,o,o,o,t)});function dr(e,t){return e===t||e!=e&&t!=t}var mr=wo(Qa),gr=wo(function(e,t){return e>=t}),fr=nn(function(){return arguments}())?nn:function(e){return Ir(e)&&ut.call(e,"callee")&&!Dt.call(e,"callee")},yr=a.isArray,vr=Ut?fi(Ut):function(e){return Ir(e)&&Xa(e)==ce};function br(e){return null!=e&&Er(e.length)&&!zr(e)}function wr(e){return Ir(e)&&br(e)}var jr=Oi||Nc,kr=Ot?fi(Ot):function(e){return Ir(e)&&Xa(e)==F};function xr(e){if(!Ir(e))return!1;var t=Xa(e);return t==G||t==Y||"string"==typeof e.message&&"string"==typeof e.name&&!Tr(e)}function zr(e){if(!Cr(e))return!1;var t=Xa(e);return t==K||t==J||t==N||t==ee}function Sr(e){return"number"==typeof e&&e==Hr(e)}function Er(e){return"number"==typeof e&&e>-1&&e%1==0&&e<=D}function Cr(e){var t=typeof e;return null!=e&&("object"==t||"function"==t)}function Ir(e){return null!=e&&"object"==typeof e}var Ar=Ht?fi(Ht):function(e){return Ir(e)&&Vo(e)==Z};function _r(e){return"number"==typeof e||Ir(e)&&Xa(e)==X}function Tr(e){if(!Ir(e)||Xa(e)!=$)return!1;var t=Tt(e);if(null===t)return!0;var i=ut.call(t,"constructor")&&t.constructor;return"function"==typeof i&&i instanceof i&&lt.call(i)==mt}var Wr=Vt?fi(Vt):function(e){return Ir(e)&&Xa(e)==te};var Br=Nt?fi(Nt):function(e){return Ir(e)&&Vo(e)==ie};function Dr(e){return"string"==typeof e||!yr(e)&&Ir(e)&&Xa(e)==ae}function Mr(e){return"symbol"==typeof e||Ir(e)&&Xa(e)==ne}var Rr=qt?fi(qt):function(e){return Ir(e)&&Er(e.length)&&!!Et[Xa(e)]};var Pr=wo(pn),Lr=wo(function(e,t){return e<=t});function Ur(e){if(!e)return[];if(br(e))return Dr(e)?Ti(e):io(e);if(Lt&&e[Lt])return function(e){for(var t,i=[];!(t=e.next()).done;)i.push(t.value);return i}(e[Lt]());var t=Vo(e);return(t==Z?Si:t==ie?Ii:dc)(e)}function Or(e){return e?(e=Nr(e))===B||e===-B?(e<0?-1:1)*M:e==e?e:0:0===e?e:0}function Hr(e){var t=Or(e),i=t%1;return t==t?i?t-i:t:0}function Vr(e){return e?Ba(Hr(e),0,P):0}function Nr(e){if("number"==typeof e)return e;if(Mr(e))return R;if(Cr(e)){var t="function"==typeof e.valueOf?e.valueOf():e;e=Cr(t)?t+"":t}if("string"!=typeof e)return 0===e?e:+e;e=e.replace(De,"");var i=Fe.test(e);return i||Ge.test(e)?_t(e.slice(2),i?2:8):qe.test(e)?R:+e}function qr(e){return ao(e,oc(e))}function Fr(e){return null==e?"":Rn(e)}var Yr=oo(function(e,t){if(Xo(t)||br(t))ao(t,nc(t),e);else for(var i in t)ut.call(t,i)&&Ca(e,i,t[i])}),Gr=oo(function(e,t){ao(t,oc(t),e)}),Kr=oo(function(e,t,i,a){ao(t,oc(t),e,a)}),Jr=oo(function(e,t,i,a){ao(t,nc(t),e,a)}),Zr=_o(Wa);var Xr=xn(function(e,t){e=tt(e);var i=-1,a=t.length,n=a>2?t[2]:o;for(n&&Go(t[0],t[1],n)&&(a=1);++i<a;)for(var s=t[i],r=oc(s),c=-1,l=r.length;++c<l;){var u=r[c],p=e[u];(p===o||dr(p,rt[u])&&!ut.call(e,u))&&(e[u]=s[u])}return e}),Qr=xn(function(e){return e.push(o,Co),Ft(rc,o,e)});function $r(e,t,i){var a=null==e?o:Ja(e,t);return a===o?i:a}function ec(e,t){return null!=e&&No(e,t,en)}var tc=go(function(e,t,i){null!=t&&"function"!=typeof t.toString&&(t=dt.call(t)),e[t]=i},Cc(_c)),ic=go(function(e,t,i){null!=t&&"function"!=typeof t.toString&&(t=dt.call(t)),ut.call(e,t)?e[t].push(i):e[t]=[i]},Ro),ac=xn(an);function nc(e){return br(e)?ka(e):ln(e)}function oc(e){return br(e)?ka(e,!0):un(e)}var sc=oo(function(e,t,i){gn(e,t,i)}),rc=oo(function(e,t,i,a){gn(e,t,i,a)}),cc=_o(function(e,t){var i={};if(null==e)return i;var a=!1;t=$t(t,function(t){return t=Yn(t,e),a||(a=t.length>1),t}),ao(e,Wo(e),i),a&&(i=Da(i,h|d|m,Io));for(var n=t.length;n--;)Ln(i,t[n]);return i});var lc=_o(function(e,t){return null==e?{}:function(e,t){return vn(e,t,function(t,i){return ec(e,i)})}(e,t)});function uc(e,t){if(null==e)return{};var i=$t(Wo(e),function(e){return[e]});return t=Ro(t),vn(e,i,function(e,i){return t(e,i[0])})}var pc=zo(nc),hc=zo(oc);function dc(e){return null==e?[]:yi(e,nc(e))}var mc=lo(function(e,t,i){return t=t.toLowerCase(),e+(i?gc(t):t)});function gc(e){return xc(Fr(e).toLowerCase())}function fc(e){return(e=Fr(e))&&e.replace(Je,ji).replace(bt,"")}var yc=lo(function(e,t,i){return e+(i?"-":"")+t.toLowerCase()}),vc=lo(function(e,t,i){return e+(i?" ":"")+t.toLowerCase()}),bc=co("toLowerCase");var wc=lo(function(e,t,i){return e+(i?"_":"")+t.toLowerCase()});var jc=lo(function(e,t,i){return e+(i?" ":"")+xc(t)});var kc=lo(function(e,t,i){return e+(i?" ":"")+t.toUpperCase()}),xc=co("toUpperCase");function zc(e,t,i){return e=Fr(e),(t=i?o:t)===o?function(e){return xt.test(e)}(e)?function(e){return e.match(jt)||[]}(e):function(e){return e.match(Oe)||[]}(e):e.match(t)||[]}var Sc=xn(function(e,t){try{return Ft(e,o,t)}catch(i){return xr(i)?i:new Qe(i)}}),Ec=_o(function(e,t){return Gt(t,function(t){t=us(t),Ta(e,t,ir(e[t],e))}),e});function Cc(e){return function(){return e}}var Ic=ho(),Ac=ho(!0);function _c(e){return e}function Tc(e){return cn("function"==typeof e?e:Da(e,h))}var Wc=xn(function(e,t){return function(i){return an(i,e,t)}}),Bc=xn(function(e,t){return function(i){return an(e,i,t)}});function Dc(e,t,i){var a=nc(t),n=Ka(t,a);null!=i||Cr(t)&&(n.length||!a.length)||(i=t,t=e,e=this,n=Ka(t,nc(t)));var o=!(Cr(i)&&"chain"in i&&!i.chain),s=zr(e);return Gt(n,function(i){var a=t[i];e[i]=a,s&&(e.prototype[i]=function(){var t=this.__chain__;if(o||t){var i=e(this.__wrapped__);return(i.__actions__=io(this.__actions__)).push({func:a,args:arguments,thisArg:e}),i.__chain__=t,i}return a.apply(e,ei([this.value()],arguments))})}),e}function Mc(){}var Rc=yo($t),Pc=yo(Jt),Lc=yo(ai);function Uc(e){return Ko(e)?pi(us(e)):function(e){return function(t){return Ja(t,e)}}(e)}var Oc=bo(),Hc=bo(!0);function Vc(){return[]}function Nc(){return!1}var qc=fo(function(e,t){return e+t},0),Fc=ko("ceil"),Yc=fo(function(e,t){return e/t},1),Gc=ko("floor");var Kc,Jc=fo(function(e,t){return e*t},1),Zc=ko("round"),Xc=fo(function(e,t){return e-t},0);return ha.after=function(e,t){if("function"!=typeof t)throw new nt(c);return e=Hr(e),function(){if(--e<1)return t.apply(this,arguments)}},ha.ary=er,ha.assign=Yr,ha.assignIn=Gr,ha.assignInWith=Kr,ha.assignWith=Jr,ha.at=Zr,ha.before=tr,ha.bind=ir,ha.bindAll=Ec,ha.bindKey=ar,ha.castArray=function(){if(!arguments.length)return[];var e=arguments[0];return yr(e)?e:[e]},ha.chain=Us,ha.chunk=function(e,t,i){t=(i?Go(e,t,i):t===o)?1:qi(Hr(t),0);var n=null==e?0:e.length;if(!n||t<1)return[];for(var s=0,r=0,c=a(Pi(n/t));s<n;)c[r++]=_n(e,s,s+=t);return c},ha.compact=function(e){for(var t=-1,i=null==e?0:e.length,a=0,n=[];++t<i;){var o=e[t];o&&(n[a++]=o)}return n},ha.concat=function(){var e=arguments.length;if(!e)return[];for(var t=a(e-1),i=arguments[0],n=e;n--;)t[n-1]=arguments[n];return ei(yr(i)?io(i):[i],Na(t,1))},ha.cond=function(e){var t=null==e?0:e.length,i=Ro();return e=t?$t(e,function(e){if("function"!=typeof e[1])throw new nt(c);return[i(e[0]),e[1]]}):[],xn(function(i){for(var a=-1;++a<t;){var n=e[a];if(Ft(n[0],this,i))return Ft(n[1],this,i)}})},ha.conforms=function(e){return function(e){var t=nc(e);return function(i){return Ma(i,e,t)}}(Da(e,h))},ha.constant=Cc,ha.countBy=Vs,ha.create=function(e,t){var i=da(e);return null==t?i:_a(i,t)},ha.curry=function e(t,i,a){var n=So(t,w,o,o,o,o,o,i=a?o:i);return n.placeholder=e.placeholder,n},ha.curryRight=function e(t,i,a){var n=So(t,j,o,o,o,o,o,i=a?o:i);return n.placeholder=e.placeholder,n},ha.debounce=nr,ha.defaults=Xr,ha.defaultsDeep=Qr,ha.defer=or,ha.delay=sr,ha.difference=ds,ha.differenceBy=ms,ha.differenceWith=gs,ha.drop=function(e,t,i){var a=null==e?0:e.length;return a?_n(e,(t=i||t===o?1:Hr(t))<0?0:t,a):[]},ha.dropRight=function(e,t,i){var a=null==e?0:e.length;return a?_n(e,0,(t=a-(t=i||t===o?1:Hr(t)))<0?0:t):[]},ha.dropRightWhile=function(e,t){return e&&e.length?On(e,Ro(t,3),!0,!0):[]},ha.dropWhile=function(e,t){return e&&e.length?On(e,Ro(t,3),!0):[]},ha.fill=function(e,t,i,a){var n=null==e?0:e.length;return n?(i&&"number"!=typeof i&&Go(e,t,i)&&(i=0,a=n),function(e,t,i,a){var n=e.length;for((i=Hr(i))<0&&(i=-i>n?0:n+i),(a=a===o||a>n?n:Hr(a))<0&&(a+=n),a=i>a?0:Vr(a);i<a;)e[i++]=t;return e}(e,t,i,a)):[]},ha.filter=function(e,t){return(yr(e)?Zt:Va)(e,Ro(t,3))},ha.flatMap=function(e,t){return Na(Zs(e,t),1)},ha.flatMapDeep=function(e,t){return Na(Zs(e,t),B)},ha.flatMapDepth=function(e,t,i){return i=i===o?1:Hr(i),Na(Zs(e,t),i)},ha.flatten=vs,ha.flattenDeep=function(e){return null!=e&&e.length?Na(e,B):[]},ha.flattenDepth=function(e,t){return null!=e&&e.length?Na(e,t=t===o?1:Hr(t)):[]},ha.flip=function(e){return So(e,E)},ha.flow=Ic,ha.flowRight=Ac,ha.fromPairs=function(e){for(var t=-1,i=null==e?0:e.length,a={};++t<i;){var n=e[t];a[n[0]]=n[1]}return a},ha.functions=function(e){return null==e?[]:Ka(e,nc(e))},ha.functionsIn=function(e){return null==e?[]:Ka(e,oc(e))},ha.groupBy=Gs,ha.initial=function(e){return null!=e&&e.length?_n(e,0,-1):[]},ha.intersection=ws,ha.intersectionBy=js,ha.intersectionWith=ks,ha.invert=tc,ha.invertBy=ic,ha.invokeMap=Ks,ha.iteratee=Tc,ha.keyBy=Js,ha.keys=nc,ha.keysIn=oc,ha.map=Zs,ha.mapKeys=function(e,t){var i={};return t=Ro(t,3),Ya(e,function(e,a,n){Ta(i,t(e,a,n),e)}),i},ha.mapValues=function(e,t){var i={};return t=Ro(t,3),Ya(e,function(e,a,n){Ta(i,a,t(e,a,n))}),i},ha.matches=function(e){return dn(Da(e,h))},ha.matchesProperty=function(e,t){return mn(e,Da(t,h))},ha.memoize=rr,ha.merge=sc,ha.mergeWith=rc,ha.method=Wc,ha.methodOf=Bc,ha.mixin=Dc,ha.negate=cr,ha.nthArg=function(e){return e=Hr(e),xn(function(t){return fn(t,e)})},ha.omit=cc,ha.omitBy=function(e,t){return uc(e,cr(Ro(t)))},ha.once=function(e){return tr(2,e)},ha.orderBy=function(e,t,i,a){return null==e?[]:(yr(t)||(t=null==t?[]:[t]),yr(i=a?o:i)||(i=null==i?[]:[i]),yn(e,t,i))},ha.over=Rc,ha.overArgs=lr,ha.overEvery=Pc,ha.overSome=Lc,ha.partial=ur,ha.partialRight=pr,ha.partition=Xs,ha.pick=lc,ha.pickBy=uc,ha.property=Uc,ha.propertyOf=function(e){return function(t){return null==e?o:Ja(e,t)}},ha.pull=zs,ha.pullAll=Ss,ha.pullAllBy=function(e,t,i){return e&&e.length&&t&&t.length?bn(e,t,Ro(i,2)):e},ha.pullAllWith=function(e,t,i){return e&&e.length&&t&&t.length?bn(e,t,o,i):e},ha.pullAt=Es,ha.range=Oc,ha.rangeRight=Hc,ha.rearg=hr,ha.reject=function(e,t){return(yr(e)?Zt:Va)(e,cr(Ro(t,3)))},ha.remove=function(e,t){var i=[];if(!e||!e.length)return i;var a=-1,n=[],o=e.length;for(t=Ro(t,3);++a<o;){var s=e[a];t(s,a,e)&&(i.push(s),n.push(a))}return wn(e,n),i},ha.rest=function(e,t){if("function"!=typeof e)throw new nt(c);return xn(e,t=t===o?t:Hr(t))},ha.reverse=Cs,ha.sampleSize=function(e,t,i){return t=(i?Go(e,t,i):t===o)?1:Hr(t),(yr(e)?za:Sn)(e,t)},ha.set=function(e,t,i){return null==e?e:En(e,t,i)},ha.setWith=function(e,t,i,a){return a="function"==typeof a?a:o,null==e?e:En(e,t,i,a)},ha.shuffle=function(e){return(yr(e)?Sa:An)(e)},ha.slice=function(e,t,i){var a=null==e?0:e.length;return a?(i&&"number"!=typeof i&&Go(e,t,i)?(t=0,i=a):(t=null==t?0:Hr(t),i=i===o?a:Hr(i)),_n(e,t,i)):[]},ha.sortBy=Qs,ha.sortedUniq=function(e){return e&&e.length?Dn(e):[]},ha.sortedUniqBy=function(e,t){return e&&e.length?Dn(e,Ro(t,2)):[]},ha.split=function(e,t,i){return i&&"number"!=typeof i&&Go(e,t,i)&&(t=i=o),(i=i===o?P:i>>>0)?(e=Fr(e))&&("string"==typeof t||null!=t&&!Wr(t))&&!(t=Rn(t))&&zi(e)?Kn(Ti(e),0,i):e.split(t,i):[]},ha.spread=function(e,t){if("function"!=typeof e)throw new nt(c);return t=null==t?0:qi(Hr(t),0),xn(function(i){var a=i[t],n=Kn(i,0,t);return a&&ei(n,a),Ft(e,this,n)})},ha.tail=function(e){var t=null==e?0:e.length;return t?_n(e,1,t):[]},ha.take=function(e,t,i){return e&&e.length?_n(e,0,(t=i||t===o?1:Hr(t))<0?0:t):[]},ha.takeRight=function(e,t,i){var a=null==e?0:e.length;return a?_n(e,(t=a-(t=i||t===o?1:Hr(t)))<0?0:t,a):[]},ha.takeRightWhile=function(e,t){return e&&e.length?On(e,Ro(t,3),!1,!0):[]},ha.takeWhile=function(e,t){return e&&e.length?On(e,Ro(t,3)):[]},ha.tap=function(e,t){return t(e),e},ha.throttle=function(e,t,i){var a=!0,n=!0;if("function"!=typeof e)throw new nt(c);return Cr(i)&&(a="leading"in i?!!i.leading:a,n="trailing"in i?!!i.trailing:n),nr(e,t,{leading:a,maxWait:t,trailing:n})},ha.thru=Os,ha.toArray=Ur,ha.toPairs=pc,ha.toPairsIn=hc,ha.toPath=function(e){return yr(e)?$t(e,us):Mr(e)?[e]:io(ls(Fr(e)))},ha.toPlainObject=qr,ha.transform=function(e,t,i){var a=yr(e),n=a||jr(e)||Rr(e);if(t=Ro(t,4),null==i){var o=e&&e.constructor;i=n?a?new o:[]:Cr(e)&&zr(o)?da(Tt(e)):{}}return(n?Gt:Ya)(e,function(e,a,n){return t(i,e,a,n)}),i},ha.unary=function(e){return er(e,1)},ha.union=Is,ha.unionBy=As,ha.unionWith=_s,ha.uniq=function(e){return e&&e.length?Pn(e):[]},ha.uniqBy=function(e,t){return e&&e.length?Pn(e,Ro(t,2)):[]},ha.uniqWith=function(e,t){return t="function"==typeof t?t:o,e&&e.length?Pn(e,o,t):[]},ha.unset=function(e,t){return null==e||Ln(e,t)},ha.unzip=Ts,ha.unzipWith=Ws,ha.update=function(e,t,i){return null==e?e:Un(e,t,Fn(i))},ha.updateWith=function(e,t,i,a){return a="function"==typeof a?a:o,null==e?e:Un(e,t,Fn(i),a)},ha.values=dc,ha.valuesIn=function(e){return null==e?[]:yi(e,oc(e))},ha.without=Bs,ha.words=zc,ha.wrap=function(e,t){return ur(Fn(t),e)},ha.xor=Ds,ha.xorBy=Ms,ha.xorWith=Rs,ha.zip=Ps,ha.zipObject=function(e,t){return Nn(e||[],t||[],Ca)},ha.zipObjectDeep=function(e,t){return Nn(e||[],t||[],En)},ha.zipWith=Ls,ha.entries=pc,ha.entriesIn=hc,ha.extend=Gr,ha.extendWith=Kr,Dc(ha,ha),ha.add=qc,ha.attempt=Sc,ha.camelCase=mc,ha.capitalize=gc,ha.ceil=Fc,ha.clamp=function(e,t,i){return i===o&&(i=t,t=o),i!==o&&(i=(i=Nr(i))==i?i:0),t!==o&&(t=(t=Nr(t))==t?t:0),Ba(Nr(e),t,i)},ha.clone=function(e){return Da(e,m)},ha.cloneDeep=function(e){return Da(e,h|m)},ha.cloneDeepWith=function(e,t){return Da(e,h|m,t="function"==typeof t?t:o)},ha.cloneWith=function(e,t){return Da(e,m,t="function"==typeof t?t:o)},ha.conformsTo=function(e,t){return null==t||Ma(e,t,nc(t))},ha.deburr=fc,ha.defaultTo=function(e,t){return null==e||e!=e?t:e},ha.divide=Yc,ha.endsWith=function(e,t,i){e=Fr(e),t=Rn(t);var a=e.length,n=i=i===o?a:Ba(Hr(i),0,a);return(i-=t.length)>=0&&e.slice(i,n)==t},ha.eq=dr,ha.escape=function(e){return(e=Fr(e))&&Se.test(e)?e.replace(xe,ki):e},ha.escapeRegExp=function(e){return(e=Fr(e))&&Be.test(e)?e.replace(We,"\\$&"):e},ha.every=function(e,t,i){var a=yr(e)?Jt:Oa;return i&&Go(e,t,i)&&(t=o),a(e,Ro(t,3))},ha.find=Ns,ha.findIndex=fs,ha.findKey=function(e,t){return oi(e,Ro(t,3),Ya)},ha.findLast=qs,ha.findLastIndex=ys,ha.findLastKey=function(e,t){return oi(e,Ro(t,3),Ga)},ha.floor=Gc,ha.forEach=Fs,ha.forEachRight=Ys,ha.forIn=function(e,t){return null==e?e:qa(e,Ro(t,3),oc)},ha.forInRight=function(e,t){return null==e?e:Fa(e,Ro(t,3),oc)},ha.forOwn=function(e,t){return e&&Ya(e,Ro(t,3))},ha.forOwnRight=function(e,t){return e&&Ga(e,Ro(t,3))},ha.get=$r,ha.gt=mr,ha.gte=gr,ha.has=function(e,t){return null!=e&&No(e,t,$a)},ha.hasIn=ec,ha.head=bs,ha.identity=_c,ha.includes=function(e,t,i,a){e=br(e)?e:dc(e),i=i&&!a?Hr(i):0;var n=e.length;return i<0&&(i=qi(n+i,0)),Dr(e)?i<=n&&e.indexOf(t,i)>-1:!!n&&ri(e,t,i)>-1},ha.indexOf=function(e,t,i){var a=null==e?0:e.length;if(!a)return-1;var n=null==i?0:Hr(i);return n<0&&(n=qi(a+n,0)),ri(e,t,n)},ha.inRange=function(e,t,i){return t=Or(t),i===o?(i=t,t=0):i=Or(i),function(e,t,i){return e>=Fi(t,i)&&e<qi(t,i)}(e=Nr(e),t,i)},ha.invoke=ac,ha.isArguments=fr,ha.isArray=yr,ha.isArrayBuffer=vr,ha.isArrayLike=br,ha.isArrayLikeObject=wr,ha.isBoolean=function(e){return!0===e||!1===e||Ir(e)&&Xa(e)==q},ha.isBuffer=jr,ha.isDate=kr,ha.isElement=function(e){return Ir(e)&&1===e.nodeType&&!Tr(e)},ha.isEmpty=function(e){if(null==e)return!0;if(br(e)&&(yr(e)||"string"==typeof e||"function"==typeof e.splice||jr(e)||Rr(e)||fr(e)))return!e.length;var t=Vo(e);if(t==Z||t==ie)return!e.size;if(Xo(e))return!ln(e).length;for(var i in e)if(ut.call(e,i))return!1;return!0},ha.isEqual=function(e,t){return on(e,t)},ha.isEqualWith=function(e,t,i){var a=(i="function"==typeof i?i:o)?i(e,t):o;return a===o?on(e,t,o,i):!!a},ha.isError=xr,ha.isFinite=function(e){return"number"==typeof e&&Hi(e)},ha.isFunction=zr,ha.isInteger=Sr,ha.isLength=Er,ha.isMap=Ar,ha.isMatch=function(e,t){return e===t||sn(e,t,Lo(t))},ha.isMatchWith=function(e,t,i){return i="function"==typeof i?i:o,sn(e,t,Lo(t),i)},ha.isNaN=function(e){return _r(e)&&e!=+e},ha.isNative=function(e){if(Zo(e))throw new Qe(r);return rn(e)},ha.isNil=function(e){return null==e},ha.isNull=function(e){return null===e},ha.isNumber=_r,ha.isObject=Cr,ha.isObjectLike=Ir,ha.isPlainObject=Tr,ha.isRegExp=Wr,ha.isSafeInteger=function(e){return Sr(e)&&e>=-D&&e<=D},ha.isSet=Br,ha.isString=Dr,ha.isSymbol=Mr,ha.isTypedArray=Rr,ha.isUndefined=function(e){return e===o},ha.isWeakMap=function(e){return Ir(e)&&Vo(e)==se},ha.isWeakSet=function(e){return Ir(e)&&Xa(e)==re},ha.join=function(e,t){return null==e?"":Vi.call(e,t)},ha.kebabCase=yc,ha.last=xs,ha.lastIndexOf=function(e,t,i){var a=null==e?0:e.length;if(!a)return-1;var n=a;return i!==o&&(n=(n=Hr(i))<0?qi(a+n,0):Fi(n,a-1)),t==t?function(e,t,i){for(var a=i+1;a--;)if(e[a]===t)return a;return a}(e,t,n):si(e,li,n,!0)},ha.lowerCase=vc,ha.lowerFirst=bc,ha.lt=Pr,ha.lte=Lr,ha.max=function(e){return e&&e.length?Ha(e,_c,Qa):o},ha.maxBy=function(e,t){return e&&e.length?Ha(e,Ro(t,2),Qa):o},ha.mean=function(e){return ui(e,_c)},ha.meanBy=function(e,t){return ui(e,Ro(t,2))},ha.min=function(e){return e&&e.length?Ha(e,_c,pn):o},ha.minBy=function(e,t){return e&&e.length?Ha(e,Ro(t,2),pn):o},ha.stubArray=Vc,ha.stubFalse=Nc,ha.stubObject=function(){return{}},ha.stubString=function(){return""},ha.stubTrue=function(){return!0},ha.multiply=Jc,ha.nth=function(e,t){return e&&e.length?fn(e,Hr(t)):o},ha.noConflict=function(){return Bt._===this&&(Bt._=gt),this},ha.noop=Mc,ha.now=$s,ha.pad=function(e,t,i){e=Fr(e);var a=(t=Hr(t))?_i(e):0;if(!t||a>=t)return e;var n=(t-a)/2;return vo(Li(n),i)+e+vo(Pi(n),i)},ha.padEnd=function(e,t,i){e=Fr(e);var a=(t=Hr(t))?_i(e):0;return t&&a<t?e+vo(t-a,i):e},ha.padStart=function(e,t,i){e=Fr(e);var a=(t=Hr(t))?_i(e):0;return t&&a<t?vo(t-a,i)+e:e},ha.parseInt=function(e,t,i){return i||null==t?t=0:t&&(t=+t),Gi(Fr(e).replace(Me,""),t||0)},ha.random=function(e,t,i){if(i&&"boolean"!=typeof i&&Go(e,t,i)&&(t=i=o),i===o&&("boolean"==typeof t?(i=t,t=o):"boolean"==typeof e&&(i=e,e=o)),e===o&&t===o?(e=0,t=1):(e=Or(e),t===o?(t=e,e=0):t=Or(t)),e>t){var a=e;e=t,t=a}if(i||e%1||t%1){var n=Ki();return Fi(e+n*(t-e+At("1e-"+((n+"").length-1))),t)}return jn(e,t)},ha.reduce=function(e,t,i){var a=yr(e)?ti:di,n=arguments.length<3;return a(e,Ro(t,4),i,n,La)},ha.reduceRight=function(e,t,i){var a=yr(e)?ii:di,n=arguments.length<3;return a(e,Ro(t,4),i,n,Ua)},ha.repeat=function(e,t,i){return t=(i?Go(e,t,i):t===o)?1:Hr(t),kn(Fr(e),t)},ha.replace=function(){var e=arguments,t=Fr(e[0]);return e.length<3?t:t.replace(e[1],e[2])},ha.result=function(e,t,i){var a=-1,n=(t=Yn(t,e)).length;for(n||(n=1,e=o);++a<n;){var s=null==e?o:e[us(t[a])];s===o&&(a=n,s=i),e=zr(s)?s.call(e):s}return e},ha.round=Zc,ha.runInContext=e,ha.sample=function(e){return(yr(e)?xa:zn)(e)},ha.size=function(e){if(null==e)return 0;if(br(e))return Dr(e)?_i(e):e.length;var t=Vo(e);return t==Z||t==ie?e.size:ln(e).length},ha.snakeCase=wc,ha.some=function(e,t,i){var a=yr(e)?ai:Tn;return i&&Go(e,t,i)&&(t=o),a(e,Ro(t,3))},ha.sortedIndex=function(e,t){return Wn(e,t)},ha.sortedIndexBy=function(e,t,i){return Bn(e,t,Ro(i,2))},ha.sortedIndexOf=function(e,t){var i=null==e?0:e.length;if(i){var a=Wn(e,t);if(a<i&&dr(e[a],t))return a}return-1},ha.sortedLastIndex=function(e,t){return Wn(e,t,!0)},ha.sortedLastIndexBy=function(e,t,i){return Bn(e,t,Ro(i,2),!0)},ha.sortedLastIndexOf=function(e,t){if(null!=e&&e.length){var i=Wn(e,t,!0)-1;if(dr(e[i],t))return i}return-1},ha.startCase=jc,ha.startsWith=function(e,t,i){return e=Fr(e),i=null==i?0:Ba(Hr(i),0,e.length),t=Rn(t),e.slice(i,i+t.length)==t},ha.subtract=Xc,ha.sum=function(e){return e&&e.length?mi(e,_c):0},ha.sumBy=function(e,t){return e&&e.length?mi(e,Ro(t,2)):0},ha.template=function(e,t,i){var a=ha.templateSettings;i&&Go(e,t,i)&&(t=o),e=Fr(e),t=Kr({},t,a,Eo);var n,s,r=Kr({},t.imports,a.imports,Eo),c=nc(r),l=yi(r,c),u=0,p=t.interpolate||Ze,h="__p += '",d=it((t.escape||Ze).source+"|"+p.source+"|"+(p===Ie?Ve:Ze).source+"|"+(t.evaluate||Ze).source+"|$","g"),m="//# sourceURL="+(ut.call(t,"sourceURL")?(t.sourceURL+"").replace(/\s/g," "):"lodash.templateSources["+ ++St+"]")+"\n";e.replace(d,function(t,i,a,o,r,c){return a||(a=o),h+=e.slice(u,c).replace(Xe,xi),i&&(n=!0,h+="' +\n__e("+i+") +\n'"),r&&(s=!0,h+="';\n"+r+";\n__p += '"),a&&(h+="' +\n((__t = ("+a+")) == null ? '' : __t) +\n'"),u=c+t.length,t}),h+="';\n";var g=ut.call(t,"variable")&&t.variable;g||(h="with (obj) {\n"+h+"\n}\n"),h=(s?h.replace(be,""):h).replace(we,"$1").replace(je,"$1;"),h="function("+(g||"obj")+") {\n"+(g?"":"obj || (obj = {});\n")+"var __t, __p = ''"+(n?", __e = _.escape":"")+(s?", __j = Array.prototype.join;\nfunction print() { __p += __j.call(arguments, '') }\n":";\n")+h+"return __p\n}";var f=Sc(function(){return $e(c,m+"return "+h).apply(o,l)});if(f.source=h,xr(f))throw f;return f},ha.times=function(e,t){if((e=Hr(e))<1||e>D)return[];var i=P,a=Fi(e,P);t=Ro(t),e-=P;for(var n=gi(a,t);++i<e;)t(i);return n},ha.toFinite=Or,ha.toInteger=Hr,ha.toLength=Vr,ha.toLower=function(e){return Fr(e).toLowerCase()},ha.toNumber=Nr,ha.toSafeInteger=function(e){return e?Ba(Hr(e),-D,D):0===e?e:0},ha.toString=Fr,ha.toUpper=function(e){return Fr(e).toUpperCase()},ha.trim=function(e,t,i){if((e=Fr(e))&&(i||t===o))return e.replace(De,"");if(!e||!(t=Rn(t)))return e;var a=Ti(e),n=Ti(t);return Kn(a,bi(a,n),wi(a,n)+1).join("")},ha.trimEnd=function(e,t,i){if((e=Fr(e))&&(i||t===o))return e.replace(Re,"");if(!e||!(t=Rn(t)))return e;var a=Ti(e);return Kn(a,0,wi(a,Ti(t))+1).join("")},ha.trimStart=function(e,t,i){if((e=Fr(e))&&(i||t===o))return e.replace(Me,"");if(!e||!(t=Rn(t)))return e;var a=Ti(e);return Kn(a,bi(a,Ti(t))).join("")},ha.truncate=function(e,t){var i=C,a=I;if(Cr(t)){var n="separator"in t?t.separator:n;i="length"in t?Hr(t.length):i,a="omission"in t?Rn(t.omission):a}var s=(e=Fr(e)).length;if(zi(e)){var r=Ti(e);s=r.length}if(i>=s)return e;var c=i-_i(a);if(c<1)return a;var l=r?Kn(r,0,c).join(""):e.slice(0,c);if(n===o)return l+a;if(r&&(c+=l.length-c),Wr(n)){if(e.slice(c).search(n)){var u,p=l;for(n.global||(n=it(n.source,Fr(Ne.exec(n))+"g")),n.lastIndex=0;u=n.exec(p);)var h=u.index;l=l.slice(0,h===o?c:h)}}else if(e.indexOf(Rn(n),c)!=c){var d=l.lastIndexOf(n);d>-1&&(l=l.slice(0,d))}return l+a},ha.unescape=function(e){return(e=Fr(e))&&ze.test(e)?e.replace(ke,Wi):e},ha.uniqueId=function(e){var t=++pt;return Fr(e)+t},ha.upperCase=kc,ha.upperFirst=xc,ha.each=Fs,ha.eachRight=Ys,ha.first=bs,Dc(ha,(Kc={},Ya(ha,function(e,t){ut.call(ha.prototype,t)||(Kc[t]=e)}),Kc),{chain:!1}),ha.VERSION="4.17.19",Gt(["bind","bindKey","curry","curryRight","partial","partialRight"],function(e){ha[e].placeholder=ha}),Gt(["drop","take"],function(e,t){fa.prototype[e]=function(i){i=i===o?1:qi(Hr(i),0);var a=this.__filtered__&&!t?new fa(this):this.clone();return a.__filtered__?a.__takeCount__=Fi(i,a.__takeCount__):a.__views__.push({size:Fi(i,P),type:e+(a.__dir__<0?"Right":"")}),a},fa.prototype[e+"Right"]=function(t){return this.reverse()[e](t).reverse()}}),Gt(["filter","map","takeWhile"],function(e,t){var i=t+1,a=i==T||3==i;fa.prototype[e]=function(e){var t=this.clone();return t.__iteratees__.push({iteratee:Ro(e,3),type:i}),t.__filtered__=t.__filtered__||a,t}}),Gt(["head","last"],function(e,t){var i="take"+(t?"Right":"");fa.prototype[e]=function(){return this[i](1).value()[0]}}),Gt(["initial","tail"],function(e,t){var i="drop"+(t?"":"Right");fa.prototype[e]=function(){return this.__filtered__?new fa(this):this[i](1)}}),fa.prototype.compact=function(){return this.filter(_c)},fa.prototype.find=function(e){return this.filter(e).head()},fa.prototype.findLast=function(e){return this.reverse().find(e)},fa.prototype.invokeMap=xn(function(e,t){return"function"==typeof e?new fa(this):this.map(function(i){return an(i,e,t)})}),fa.prototype.reject=function(e){return this.filter(cr(Ro(e)))},fa.prototype.slice=function(e,t){e=Hr(e);var i=this;return i.__filtered__&&(e>0||t<0)?new fa(i):(e<0?i=i.takeRight(-e):e&&(i=i.drop(e)),t!==o&&(i=(t=Hr(t))<0?i.dropRight(-t):i.take(t-e)),i)},fa.prototype.takeRightWhile=function(e){return this.reverse().takeWhile(e).reverse()},fa.prototype.toArray=function(){return this.take(P)},Ya(fa.prototype,function(e,t){var i=/^(?:filter|find|map|reject)|While$/.test(t),a=/^(?:head|last)$/.test(t),n=ha[a?"take"+("last"==t?"Right":""):t],s=a||/^find/.test(t);n&&(ha.prototype[t]=function(){var t=this.__wrapped__,r=a?[1]:arguments,c=t instanceof fa,l=r[0],u=c||yr(t),p=function(e){var t=n.apply(ha,ei([e],r));return a&&h?t[0]:t};u&&i&&"function"==typeof l&&1!=l.length&&(c=u=!1);var h=this.__chain__,d=!!this.__actions__.length,m=s&&!h,g=c&&!d;if(!s&&u){t=g?t:new fa(this);var f=e.apply(t,r);return f.__actions__.push({func:Os,args:[p],thisArg:o}),new ga(f,h)}return m&&g?e.apply(this,r):(f=this.thru(p),m?a?f.value()[0]:f.value():f)})}),Gt(["pop","push","shift","sort","splice","unshift"],function(e){var t=ot[e],i=/^(?:push|sort|unshift)$/.test(e)?"tap":"thru",a=/^(?:pop|shift)$/.test(e);ha.prototype[e]=function(){var e=arguments;if(a&&!this.__chain__){var n=this.value();return t.apply(yr(n)?n:[],e)}return this[i](function(i){return t.apply(yr(i)?i:[],e)})}}),Ya(fa.prototype,function(e,t){var i=ha[t];if(i){var a=i.name+"";ut.call(aa,a)||(aa[a]=[]),aa[a].push({name:t,func:i})}}),aa[mo(o,v).name]=[{name:"wrapper",func:o}],fa.prototype.clone=function(){var e=new fa(this.__wrapped__);return e.__actions__=io(this.__actions__),e.__dir__=this.__dir__,e.__filtered__=this.__filtered__,e.__iteratees__=io(this.__iteratees__),e.__takeCount__=this.__takeCount__,e.__views__=io(this.__views__),e},fa.prototype.reverse=function(){if(this.__filtered__){var e=new fa(this);e.__dir__=-1,e.__filtered__=!0}else(e=this.clone()).__dir__*=-1;return e},fa.prototype.value=function(){var e=this.__wrapped__.value(),t=this.__dir__,i=yr(e),a=t<0,n=i?e.length:0,o=function(e,t,i){for(var a=-1,n=i.length;++a<n;){var o=i[a],s=o.size;switch(o.type){case"drop":e+=s;break;case"dropRight":t-=s;break;case"take":t=Fi(t,e+s);break;case"takeRight":e=qi(e,t-s)}}return{start:e,end:t}}(0,n,this.__views__),s=o.start,r=o.end,c=r-s,l=a?r:s-1,u=this.__iteratees__,p=u.length,h=0,d=Fi(c,this.__takeCount__);if(!i||!a&&n==c&&d==c)return Hn(e,this.__actions__);var m=[];e:for(;c--&&h<d;){for(var g=-1,f=e[l+=t];++g<p;){var y=u[g],v=y.iteratee,b=y.type,w=v(f);if(b==W)f=w;else if(!w){if(b==T)continue e;break e}}m[h++]=f}return m},ha.prototype.at=Hs,ha.prototype.chain=function(){return Us(this)},ha.prototype.commit=function(){return new ga(this.value(),this.__chain__)},ha.prototype.next=function(){this.__values__===o&&(this.__values__=Ur(this.value()));var e=this.__index__>=this.__values__.length;return{done:e,value:e?o:this.__values__[this.__index__++]}},ha.prototype.plant=function(e){for(var t,i=this;i instanceof ma;){var a=hs(i);a.__index__=0,a.__values__=o,t?n.__wrapped__=a:t=a;var n=a;i=i.__wrapped__}return n.__wrapped__=e,t},ha.prototype.reverse=function(){var e=this.__wrapped__;if(e instanceof fa){var t=e;return this.__actions__.length&&(t=new fa(this)),(t=t.reverse()).__actions__.push({func:Os,args:[Cs],thisArg:o}),new ga(t,this.__chain__)}return this.thru(Cs)},ha.prototype.toJSON=ha.prototype.valueOf=ha.prototype.value=function(){return Hn(this.__wrapped__,this.__actions__)},ha.prototype.first=ha.prototype.head,Lt&&(ha.prototype[Lt]=function(){return this}),ha}();Bt._=Bi,(n=function(){return Bi}.call(t,i,t,a))===o||(a.exports=n)}).call(this)}).call(this,i("yLpj"),i("YuTi")(e))},M695:function(e){e.exports={date:"2020-04",title:"Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations",authors:["Pascal Goffin","Tanja Blascheck","Petra Isenberg","Wesley Willett"],series:"CHI 2020",doi:"https://doi.org/10.1145/3313831.3376842",keywords:"glyphs, word-scale visualization, information visualization, interaction techniques, text visualization",pages:13,video:"https://www.youtube.com/watch?v=wPaVdSWM8hU",abstract:"We describe a design space of view manipulation interactions for small data-driven contextual visualizations (word-scale visualizations). These interaction techniques support an active reading experience and engage readers through exploration of embedded visualizations whose placement and content connect them to specific terms in a document. A reader could, for example, use our proposed interaction techniques to explore word-scale visualizations of stock market trends for companies listed in a market overview article. When readers wish to engage more deeply with the data, they can collect, arrange, compare, and navigate the document using the embedded word-scale visualizations, permitting more visualization-centric analyses. We support our design space with a concrete implementation, illustrate it with examples from three application domains, and report results from two experiments. The experiments show how view manipulation interactions helped readers examine embedded visualizations more quickly and with less scrolling and yielded qualitative feedback on usability and future opportunities.",dir:"content/output/publications",base:"chi-2020-goffin.json",ext:".json",sourceBase:"chi-2020-goffin.yaml",sourceExt:".yaml"}},M9HE:function(e){e.exports={date:"2019-06",title:"AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator",authors:["Karthik Mahadevan","Elaheh Sanoubari","Sowmya Somanath","James E. Young","Ehud Sharlin"],series:"DIS 2019",doi:"https://doi.org/10.1145/3322276.3322328",keywords:"mixed traffic, pedestrian simulator, autonomous vehicle-pedestrian interaction",pages:12,abstract:"AV-pedestrian interaction will impact pedestrian safety, etiquette, and overall acceptance of AV technology. Evaluating AV-pedestrian interaction is challenging given limited availability of AVs and safety concerns. These challenges are compounded by \"mixed traffic\" conditions: studying AV-pedestrian interaction will be difficult in traffic consisting of vehicles varying in autonomy level. We propose immersive pedestrian simulators as design tools to study AV-pedestrian interaction, allowing rapid prototyping and evaluation of future AV-pedestrian interfaces. We present OnFoot: a VR-based simulator that immerses participants in mixed traffic conditions and allows examination of their behavior while controlling vehicles' autonomy-level, traffic and street characteristics, behavior of other virtual pedestrians, and integration of novel AV-pedestrian interfaces. We validated OnFoot against prior simulators and Wizard-of-Oz studies, and conducted a user study, manipulating vehicles' autonomy level, interfaces, and pedestrian group behavior. Our findings highlight the potential to use VR simulators as powerful tools for AV-pedestrian interaction design in mixed traffic.",dir:"content/output/publications",base:"dis-2019-mahadevan.json",ext:".json",sourceBase:"dis-2019-mahadevan.yaml",sourceExt:".yaml"}},MI4q:function(e){e.exports={date:"2020-05",title:"RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots",authors:["Ryo Suzuki","Hooman Hedayati","Clement Zheng","James Bohn","Daniel Szafir","Ellen Yi-Luen Do","Mark D Gross","Daniel Leithinger"],series:"CHI 2020",doi:"https://doi.org/10.1145/3313831.3376523",keywords:"virtual reality, room-scale haptics, haptic interfaces, swarm robots",video:"https://www.youtube.com/watch?v=4OWU60gTOFE",pages:11,abstract:"RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.",dir:"content/output/publications",base:"chi-2020-suzuki.json",ext:".json",sourceBase:"chi-2020-suzuki.yaml",sourceExt:".yaml"}},MkNd:function(e){e.exports={date:"2019-06",title:"Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways",authors:["Teddy Seyed","Anthony Tang"],series:"DIS 2019",doi:"https://doi.org/10.1145/3322276.3322305",keywords:"fashion, haute couture, e-textiles, maker culture, fashion-tech, wearables, avant-garde, haute-tech couture, modular",award:"Honorable Mention",pages:13,abstract:"Drawing upon multiple disciplines, avant-garde fashion-tech teams push the boundaries between fashion and technology. Many are well trained in envisioning aesthetic qualities of garments, but few have formal training on designing and fabricating technologies themselves. We introduce Mannequette, a prototyping tool for fashion-tech garments that enables teams to experiment with interactive technologies at early stages of their design processes. Mannequette provides an abstraction of light-based outputs and sensor-based inputs for garments through a DJ mixer-like interface that allows for dynamic changes and recording/playback of visual effects. The base of Mannequette can also be incorporated into the final garment, where it is then connected to the final components. We conducted an 8-week deployment study with eight design teams who created new garments for a runway show. Our results revealed Mannequette allowed teams to repeatedly consider new design and technical options early in their creative processes, and to communicate more effectively across disciplinary backgrounds.",dir:"content/output/publications",base:"dis-2019-seyed.json",ext:".json",sourceBase:"dis-2019-seyed.yaml",sourceExt:".yaml"}},NysX:function(e){e.exports={date:"2018-06",title:"Bod-IDE: An Augmented Reality Sandbox for eFashion Garments",authors:["Kevin Ta","Ehud Sharlin","Lora Oehlberg"],series:"DIS 2018",doi:"https://doi.org/10.1145/3197391.3205408",keywords:"augmented reality, electronic fashion, creativity support tool",pages:5,abstract:"Electronic fashion (eFashion) garments use technology to augment the human body with wearable interaction. In developing ideas, eFashion designers need to prototype the role and behavior of the interactive garment in context; however, current wearable prototyping toolkits require semi-permanent construction with physical materials that cannot easily be altered. We present Bod-IDE, an augmented reality 'mirror' that allows eFashion designers to create virtual interactive garment prototypes. Designers can quickly build, refine, and test on-the-body interactions without the need to connect or program electronics. By envisioning interaction with the body in mind, eFashion designers can focus more on reimagining the relationship between bodies, clothing, and technology.",dir:"content/output/publications",base:"dis-2018-ta.json",ext:".json",sourceBase:"dis-2018-ta.yaml",sourceExt:".yaml"}},O40h:function(e,t,i){"use strict";i.r(t),i.d(t,"default",function(){return s});var a=i("eVuF"),n=i.n(a);function o(e,t,i,a,o,s,r){try{var c=e[s](r),l=c.value}catch(u){return void i(u)}c.done?t(l):n.a.resolve(l).then(a,o)}function s(e){return function(){var t=this,i=arguments;return new n.a(function(a,n){var s=e.apply(t,i);function r(e){o(s,a,n,r,c,"next",e)}function c(e){o(s,a,n,r,c,"throw",e)}r(void 0)})}}},OvZk:function(e){e.exports={date:"2018-06",title:"Improvising with an Audience-Controlled Robot Performer",authors:["Claire Mikalauskas","Tiffany Wun","Kevin Ta","Joshua Horacsek","Lora Oehlberg"],series:"DIS 2018",doi:"https://doi.org/10.1145/3196709.3196757",pages:10,keywords:"human-robot interaction, improvised theatre, creativity-support tools, crowdsourcing",video:"https://youtu.be/ORN9jljPncc",abstract:"In improvisational theatre (improv), actors perform unscripted scenes together, collectively creating a narrative. Audience suggestions introduce randomness and build audience engagement, but can be challenging to mediate at scale. We present Robot Improv Puppet Theatre (RIPT), which includes a performance robot (Pokey) who performs gestures and dialogue in short-form improv scenes based on audience input from a mobile interface. We evaluated RIPT in several initial informal performances, and in a rehearsal with seven professional improvisers. The improvisers noted how audience prompts can have a big impact on the scene - highlighting the delicate balance between ambiguity and constraints in improv. The open structure of RIPT performances allows for multiple interpretations of how to perform with Pokey, including one-on-one conversations or multi-performer scenes. While Pokey lacks key qualities of a good improviser, improvisers found his serendipitous dialogue and gestures particularly rewarding.",dir:"content/output/publications",base:"dis-2018-mikalauskas.json",ext:".json",sourceBase:"dis-2018-mikalauskas.yaml",sourceExt:".yaml"}},PBE1:function(e,t,i){"use strict";var a=i("Y7ZC"),n=i("WEpk"),o=i("5T2Y"),s=i("8gHz"),r=i("zXhZ");a(a.P+a.R,"Promise",{finally:function(e){var t=s(this,n.Promise||o.Promise),i="function"==typeof e;return this.then(i?function(i){return r(t,e()).then(function(){return i})}:e,i?function(i){return r(t,e()).then(function(){throw i})}:e)}})},"Q/yX":function(e,t,i){"use strict";var a=i("Y7ZC"),n=i("ZW5q"),o=i("RDmV");a(a.S,"Promise",{try:function(e){var t=n.f(this),i=o(e);return(i.e?t.reject:t.resolve)(i.v),t.promise}})},QXhf:function(e,t,i){var a,n,o,s=i("2GTP"),r=i("MCSJ"),c=i("MvwC"),l=i("Hsns"),u=i("5T2Y"),p=u.process,h=u.setImmediate,d=u.clearImmediate,m=u.MessageChannel,g=u.Dispatch,f=0,y={},v=function(){var e=+this;if(y.hasOwnProperty(e)){var t=y[e];delete y[e],t()}},b=function(e){v.call(e.data)};h&&d||(h=function(e){for(var t=[],i=1;arguments.length>i;)t.push(arguments[i++]);return y[++f]=function(){r("function"==typeof e?e:Function(e),t)},a(f),f},d=function(e){delete y[e]},"process"==i("a0xu")(p)?a=function(e){p.nextTick(s(v,e,1))}:g&&g.now?a=function(e){g.now(s(v,e,1))}:m?(o=(n=new m).port2,n.port1.onmessage=b,a=s(o.postMessage,o,1)):u.addEventListener&&"function"==typeof postMessage&&!u.importScripts?(a=function(e){u.postMessage(e+"","*")},u.addEventListener("message",b,!1)):a="onreadystatechange"in l("script")?function(e){c.appendChild(l("script")).onreadystatechange=function(){c.removeChild(this),v.call(e)}}:function(e){setTimeout(s(v,e,1),0)}),e.exports={set:h,clear:d}},QhjQ:function(e){e.exports={CHI:{booktitle:"Proceedings of the CHI Conference on Human Factors in Computing Systems",publisher:"ACM, New York, NY, USA"},"CHI EA":{booktitle:"Extended Abstracts of the CHI Conference on Human Factors in Computing Systems",publisher:"ACM, New York, NY, USA"},UIST:{booktitle:"Proceedings of the Annual ACM Symposium on User Interface Software and Technology",publisher:"ACM, New York, NY, USA"},IMWUT:{booktitle:"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",publisher:"ACM, New York, NY, USA"},DIS:{booktitle:"Proceedings of the ACM on Designing Interactive Systems Conference",publisher:"ACM, New York, NY, USA"},MobileHCI:{booktitle:"Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services",publisher:"ACM, New York, NY, USA"},TEI:{booktitle:"Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction",publisher:"ACM, New York, NY, USA"},HRI:{booktitle:"Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction",publisher:"ACM, New York, NY, USA"},VR:{booktitle:"Proceedings of the IEEE Conference on Virtual Reality and 3D User Interfaces",publisher:"IEEE, New York, NY, USA"},TVCG:{booktitle:"IEEE Transactions on Visualization and Computer Graphics",publisher:"IEEE, New York, NY, USA"},IROS:{booktitle:"Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems",publisher:"IEEE, New York, NY, USA"},"C&C":{booktitle:"Proceedings of the ACM on Creativity and Cognition",publisher:"ACM, New York, NY, USA"},dir:"content/output",base:"booktitles.json",ext:".json",sourceBase:"booktitles.yaml",sourceExt:".yaml"}},RDmV:function(e,t){e.exports=function(e){try{return{e:!1,v:e()}}catch(t){return{e:!0,v:t}}}},"RRc/":function(e,t,i){var a=i("oioR");e.exports=function(e,t){var i=[];return a(e,!1,i.push,i,t),i}},RawC:function(e){e.exports={date:"2019-03",title:"WindyWall: Exploring Creative Wind Simulations",authors:["David Tolley","Thi Ngoc Tram Nguyen","Anthony Tang","Nimesha Ranasinghe","Kensaku Kawauchi","Ching-Chiuan Yen"],series:"TEI 2019",doi:"https://doi.org/10.1145/3294109.3295624",keywords:"tactile/haptic interaction, multimodal interaction, novel actuators/displays",video:"https://www.youtube.com/watch?v=Tn11UmsOsTE",pages:10,abstract:'Wind simulations are typically one-off implementations for specific applications. We introduce WindyWall, a platform for creative design and exploration of wind simulations. WindyWall is a three-panel 90-fan array that encapsulates users with 270? of wind coverage. We describe the design and implementation of the array panels, discussing how the panels can be re-arranged, where various wind simulations can be realized as simple effects. To understand how people perceive "wind" generated from WindyWall, we conducted a pilot study of wind magnitude perception using different wind activation patterns from WindyWall. Our findings suggest that: horizontal wind activations are perceived more readily than vertical ones, and that people\'s perceptions of wind are highly variable-most individuals will rate airflow differently in subsequent exposures. Based on our findings, we discuss the importance of developing a method for characterizing wind simulations, and provide design directions for others using fan arrays to simulate wind.',dir:"content/output/publications",base:"tei-2019-tolley.json",ext:".json",sourceBase:"tei-2019-tolley.yaml",sourceExt:".yaml"}},S8m4:function(e,t){!function(t){"use strict";var i,a=Object.prototype,n=a.hasOwnProperty,o="function"==typeof Symbol?Symbol:{},s=o.iterator||"@@iterator",r=o.asyncIterator||"@@asyncIterator",c=o.toStringTag||"@@toStringTag",l="object"==typeof e,u=t.regeneratorRuntime;if(u)l&&(e.exports=u);else{(u=t.regeneratorRuntime=l?e.exports:{}).wrap=w;var p="suspendedStart",h="suspendedYield",d="executing",m="completed",g={},f={};f[s]=function(){return this};var y=Object.getPrototypeOf,v=y&&y(y(T([])));v&&v!==a&&n.call(v,s)&&(f=v);var b=z.prototype=k.prototype=Object.create(f);x.prototype=b.constructor=z,z.constructor=x,z[c]=x.displayName="GeneratorFunction",u.isGeneratorFunction=function(e){var t="function"==typeof e&&e.constructor;return!!t&&(t===x||"GeneratorFunction"===(t.displayName||t.name))},u.mark=function(e){return Object.setPrototypeOf?Object.setPrototypeOf(e,z):(e.__proto__=z,c in e||(e[c]="GeneratorFunction")),e.prototype=Object.create(b),e},u.awrap=function(e){return{__await:e}},S(E.prototype),E.prototype[r]=function(){return this},u.AsyncIterator=E,u.async=function(e,t,i,a){var n=new E(w(e,t,i,a));return u.isGeneratorFunction(t)?n:n.next().then(function(e){return e.done?e.value:n.next()})},S(b),b[c]="Generator",b[s]=function(){return this},b.toString=function(){return"[object Generator]"},u.keys=function(e){var t=[];for(var i in e)t.push(i);return t.reverse(),function i(){for(;t.length;){var a=t.pop();if(a in e)return i.value=a,i.done=!1,i}return i.done=!0,i}},u.values=T,_.prototype={constructor:_,reset:function(e){if(this.prev=0,this.next=0,this.sent=this._sent=i,this.done=!1,this.delegate=null,this.method="next",this.arg=i,this.tryEntries.forEach(A),!e)for(var t in this)"t"===t.charAt(0)&&n.call(this,t)&&!isNaN(+t.slice(1))&&(this[t]=i)},stop:function(){this.done=!0;var e=this.tryEntries[0].completion;if("throw"===e.type)throw e.arg;return this.rval},dispatchException:function(e){if(this.done)throw e;var t=this;function a(a,n){return r.type="throw",r.arg=e,t.next=a,n&&(t.method="next",t.arg=i),!!n}for(var o=this.tryEntries.length-1;o>=0;--o){var s=this.tryEntries[o],r=s.completion;if("root"===s.tryLoc)return a("end");if(s.tryLoc<=this.prev){var c=n.call(s,"catchLoc"),l=n.call(s,"finallyLoc");if(c&&l){if(this.prev<s.catchLoc)return a(s.catchLoc,!0);if(this.prev<s.finallyLoc)return a(s.finallyLoc)}else if(c){if(this.prev<s.catchLoc)return a(s.catchLoc,!0)}else{if(!l)throw new Error("try statement without catch or finally");if(this.prev<s.finallyLoc)return a(s.finallyLoc)}}}},abrupt:function(e,t){for(var i=this.tryEntries.length-1;i>=0;--i){var a=this.tryEntries[i];if(a.tryLoc<=this.prev&&n.call(a,"finallyLoc")&&this.prev<a.finallyLoc){var o=a;break}}o&&("break"===e||"continue"===e)&&o.tryLoc<=t&&t<=o.finallyLoc&&(o=null);var s=o?o.completion:{};return s.type=e,s.arg=t,o?(this.method="next",this.next=o.finallyLoc,g):this.complete(s)},complete:function(e,t){if("throw"===e.type)throw e.arg;return"break"===e.type||"continue"===e.type?this.next=e.arg:"return"===e.type?(this.rval=this.arg=e.arg,this.method="return",this.next="end"):"normal"===e.type&&t&&(this.next=t),g},finish:function(e){for(var t=this.tryEntries.length-1;t>=0;--t){var i=this.tryEntries[t];if(i.finallyLoc===e)return this.complete(i.completion,i.afterLoc),A(i),g}},catch:function(e){for(var t=this.tryEntries.length-1;t>=0;--t){var i=this.tryEntries[t];if(i.tryLoc===e){var a=i.completion;if("throw"===a.type){var n=a.arg;A(i)}return n}}throw new Error("illegal catch attempt")},delegateYield:function(e,t,a){return this.delegate={iterator:T(e),resultName:t,nextLoc:a},"next"===this.method&&(this.arg=i),g}}}function w(e,t,i,a){var n=t&&t.prototype instanceof k?t:k,o=Object.create(n.prototype),s=new _(a||[]);return o._invoke=function(e,t,i){var a=p;return function(n,o){if(a===d)throw new Error("Generator is already running");if(a===m){if("throw"===n)throw o;return W()}for(i.method=n,i.arg=o;;){var s=i.delegate;if(s){var r=C(s,i);if(r){if(r===g)continue;return r}}if("next"===i.method)i.sent=i._sent=i.arg;else if("throw"===i.method){if(a===p)throw a=m,i.arg;i.dispatchException(i.arg)}else"return"===i.method&&i.abrupt("return",i.arg);a=d;var c=j(e,t,i);if("normal"===c.type){if(a=i.done?m:h,c.arg===g)continue;return{value:c.arg,done:i.done}}"throw"===c.type&&(a=m,i.method="throw",i.arg=c.arg)}}}(e,i,s),o}function j(e,t,i){try{return{type:"normal",arg:e.call(t,i)}}catch(a){return{type:"throw",arg:a}}}function k(){}function x(){}function z(){}function S(e){["next","throw","return"].forEach(function(t){e[t]=function(e){return this._invoke(t,e)}})}function E(e){var t;this._invoke=function(i,a){function o(){return new Promise(function(t,o){!function t(i,a,o,s){var r=j(e[i],e,a);if("throw"!==r.type){var c=r.arg,l=c.value;return l&&"object"==typeof l&&n.call(l,"__await")?Promise.resolve(l.__await).then(function(e){t("next",e,o,s)},function(e){t("throw",e,o,s)}):Promise.resolve(l).then(function(e){c.value=e,o(c)},function(e){return t("throw",e,o,s)})}s(r.arg)}(i,a,t,o)})}return t=t?t.then(o,o):o()}}function C(e,t){var a=e.iterator[t.method];if(a===i){if(t.delegate=null,"throw"===t.method){if(e.iterator.return&&(t.method="return",t.arg=i,C(e,t),"throw"===t.method))return g;t.method="throw",t.arg=new TypeError("The iterator does not provide a 'throw' method")}return g}var n=j(a,e.iterator,t.arg);if("throw"===n.type)return t.method="throw",t.arg=n.arg,t.delegate=null,g;var o=n.arg;return o?o.done?(t[e.resultName]=o.value,t.next=e.nextLoc,"return"!==t.method&&(t.method="next",t.arg=i),t.delegate=null,g):o:(t.method="throw",t.arg=new TypeError("iterator result is not an object"),t.delegate=null,g)}function I(e){var t={tryLoc:e[0]};1 in e&&(t.catchLoc=e[1]),2 in e&&(t.finallyLoc=e[2],t.afterLoc=e[3]),this.tryEntries.push(t)}function A(e){var t=e.completion||{};t.type="normal",delete t.arg,e.completion=t}function _(e){this.tryEntries=[{tryLoc:"root"}],e.forEach(I,this),this.reset(!0)}function T(e){if(e){var t=e[s];if(t)return t.call(e);if("function"==typeof e.next)return e;if(!isNaN(e.length)){var a=-1,o=function t(){for(;++a<e.length;)if(n.call(e,a))return t.value=e[a],t.done=!1,t;return t.value=i,t.done=!0,t};return o.next=o}}return{next:W}}function W(){return{value:i,done:!0}}}(function(){return this||"object"==typeof self&&self}()||Function("return this")())},Si70:function(e){e.exports={date:"2018-04",title:"A Design Framework for Awareness Cues in Distributed Multiplayer Games",authors:["Jason Wuertz","Sultan A. Alharthi","William A. Hamilton","Scott Bateman","Carl Gutwin","Anthony Tang","Zachary O. Toups","Jessica Hammer"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3173817",keywords:"workspace awareness, situation awareness, game design, distributed multiplayer games, awareness cues",pages:14,abstract:"In the physical world, teammates develop situation awareness about each other's location, status, and actions through cues such as gaze direction and ambient noise. To support situation awareness, distributed multiplayer games provide awareness cues - information that games automatically make available to players to support cooperative gameplay. The design of awareness cues can be extremely complex, impacting how players experience games and work with teammates. Despite the importance of awareness cues, designers have little beyond experiential knowledge to guide their design. In this work, we describe a design framework for awareness cues, providing insight into what information they provide, how they communicate this information, and how design choices can impact play experience. Our research, based on a grounded theory analysis of current games, is the first to provide a characterization of awareness cues, providing a palette for game designers to improve design practice and a starting point for deeper research into collaborative play.",dir:"content/output/publications",base:"chi-2018-wuertz.json",ext:".json",sourceBase:"chi-2018-wuertz.yaml",sourceExt:".yaml"}},TJWN:function(e,t,i){"use strict";var a=i("5T2Y"),n=i("WEpk"),o=i("2faE"),s=i("jmDH"),r=i("UWiX")("species");e.exports=function(e){var t="function"==typeof n[e]?n[e]:a[e];s&&t&&!t[r]&&o.f(t,r,{configurable:!0,get:function(){return this}})}},TsNw:function(e){e.exports={date:"2020-02",title:"LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces",authors:["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],series:"TEI 2020",doi:"https://dl.acm.org/doi/10.1145/3374920.3374941",keywords:"shape-changing interfaces, inflatables, large-scale interactions",pages:9,video:"https://www.youtube.com/watch?v=0LHeTkOMR84",abstract:"Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore interactions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust, making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various applications. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.",dir:"content/output/publications",base:"tei-2020-suzuki.json",ext:".json",sourceBase:"tei-2020-suzuki.yaml",sourceExt:".yaml"}},U6lc:function(e){e.exports={date:"2019-03",title:"Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology",authors:["Claire Mikalauskas","April Viczko","Lora Oehlberg"],series:"TEI 2019",doi:"https://doi.org/10.1145/3294109.3295631",pages:9,keywords:"props, performer-controlled technology, improvisational theatre",abstract:"While improvised theatre (improv) is often performed on a bare stage, improvisers sometimes incorporate physical props to inspire new directions for a scene and to enrich their performance. A tech booth can improvise light and sound technical elements, but coordinating with improvisers' actions on-stage is challenging. Our goal is to inform the design of an augmented prop that lets improvisers tangibly control light and sound technical elements while performing. We interviewed five professional improvisers about their use of physical props in improv, and their expectations of a possible augmented prop that controls technical theatre elements. We propose a set of guidelines for the design of an augmented prop that fits with the existing world of unpredictable improvised performance.",dir:"content/output/publications",base:"tei-2019-mikalauskas.json",ext:".json",sourceBase:"tei-2019-mikalauskas.yaml",sourceExt:".yaml"}},UTp2:function(e){e.exports={date:"2021-06",title:"I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking",authors:["Kendra Wannamaker","Sandeep Kollannur","Marian Dörk","Wesley Willett"],series:"DIS 2021",doi:"http://hdl.handle.net/1880/113555",keywords:"information visualization, personal informatics, situated visualization",pages:10,talk:"https://www.youtube.com/watch?v=yhMKURtgFZ0",abstract:"We present I/O Bits, a prototype personal informatics system that explores the potential for user-driven and situated self-tracking. With simple tactile inputs and small e-paper visualizations, I/O Bits are dedicated physical devices that allow individuals to track and visualize different kinds of personal activities in-situ. This is in contrast to most self-tracking systems, which automate data collection, centralize information displays, or integrate into multi-purpose devices like smartwatches or mobile phones. We report findings from an e-paper visualization workshop and a prototype deployment where participants constructed their own I/O Bits and used them to track a range of personal data. Based on these experiences, we contribute insights and opportunities for situated and user-driven personal informatics.",dir:"content/output/publications",base:"dis-2021-wannamaker.json",ext:".json",sourceBase:"dis-2021-wannamaker.yaml",sourceExt:".yaml"}},"V+O7":function(e,t,i){i("aPfg")("Set")},V7Et:function(e,t,i){var a=i("2GTP"),n=i("M1xp"),o=i("JB68"),s=i("tEej"),r=i("v6xn");e.exports=function(e,t){var i=1==e,c=2==e,l=3==e,u=4==e,p=6==e,h=5==e||p,d=t||r;return function(t,r,m){for(var g,f,y=o(t),v=n(y),b=a(r,m,3),w=s(v.length),j=0,k=i?d(t,w):c?d(t,0):void 0;w>j;j++)if((h||j in v)&&(f=b(g=v[j],j,y),e))if(i)k[j]=f;else if(f)switch(e){case 3:return!0;case 5:return g;case 6:return j;case 2:k.push(g)}else if(u)return!1;return p?-1:l||u?u:k}}},VKFn:function(e,t,i){i("bBy9"),i("FlQf"),e.exports=i("ldVq")},Wu5q:function(e,t,i){"use strict";var a=i("2faE").f,n=i("oVml"),o=i("XJU/"),s=i("2GTP"),r=i("EXMj"),c=i("oioR"),l=i("MPFp"),u=i("UO39"),p=i("TJWN"),h=i("jmDH"),d=i("6/1s").fastKey,m=i("n3ko"),g=h?"_s":"size",f=function(e,t){var i,a=d(t);if("F"!==a)return e._i[a];for(i=e._f;i;i=i.n)if(i.k==t)return i};e.exports={getConstructor:function(e,t,i,l){var u=e(function(e,a){r(e,u,t,"_i"),e._t=t,e._i=n(null),e._f=void 0,e._l=void 0,e[g]=0,null!=a&&c(a,i,e[l],e)});return o(u.prototype,{clear:function(){for(var e=m(this,t),i=e._i,a=e._f;a;a=a.n)a.r=!0,a.p&&(a.p=a.p.n=void 0),delete i[a.i];e._f=e._l=void 0,e[g]=0},delete:function(e){var i=m(this,t),a=f(i,e);if(a){var n=a.n,o=a.p;delete i._i[a.i],a.r=!0,o&&(o.n=n),n&&(n.p=o),i._f==a&&(i._f=n),i._l==a&&(i._l=o),i[g]--}return!!a},forEach:function(e){m(this,t);for(var i,a=s(e,arguments.length>1?arguments[1]:void 0,3);i=i?i.n:this._f;)for(a(i.v,i.k,this);i&&i.r;)i=i.p},has:function(e){return!!f(m(this,t),e)}}),h&&a(u.prototype,"size",{get:function(){return m(this,t)[g]}}),u},def:function(e,t,i){var a,n,o=f(e,t);return o?o.v=i:(e._l=o={i:n=d(t,!0),k:t,v:i,p:a=e._l,n:void 0,r:!1},e._f||(e._f=o),a&&(a.n=o),e[g]++,"F"!==n&&(e._i[n]=o)),e},getEntry:f,setStrong:function(e,t,i){l(e,t,function(e,i){this._t=m(e,t),this._k=i,this._l=void 0},function(){for(var e=this._k,t=this._l;t&&t.r;)t=t.p;return this._t&&(this._l=t=t?t.n:this._t._f)?u(0,"keys"==e?t.k:"values"==e?t.v:[t.k,t.v]):(this._t=void 0,u(1))},i?"entries":"values",!i,!0),p(t)}}},WxVc:function(e){e.exports={date:"2021-02",title:"(Big) Data in Urban Design Practice: Supporting High-Level Design Tasks Using a Visualization of Human Movement Data from Smartphones",authors:["Angela Rout","Wesley Willett"],series:"Urban Informatics and Future Cities",publisher:"Springer",pages:"301-318",doi:"http://hdl.handle.net/1880/113114",video:"https://www.youtube.com/watch?v=Me8cU6RoCiA",keywords:"information visualization, smartphone data, GPS, data visualization, architecture, urban design, task-based framework, high-level tasks",abstract:"We present the SmartCampus visualization tool, representing spatiotemporal data of over 200 student pathways and restpoints on a university campus. Based on our experiences with SmartCampus, we also propose a task-based framework that de-scribes how practicing urban designers (specifically, architects) can use human movement data visualizations in their work. Although extensive amounts of location data are produced daily by smartphones, existing geospatial tools are not customized to specifically support high-level urban design tasks. To help identify opportunities in urban design for visualizing human movement data from devices such as smartphones, we used our SmartCampus prototype to facilitate a series of 3 participatory design sessions (3 participants), a targeted online survey (14 participants), and semi-structured interviews (6 participants) with architectural experts. Our findings showcase the need for location analysis tools tailored to concrete urban design practices, and also highlight opportunities for Smart City researchers interested in developing domain specific, visualization tools.",dir:"content/output/publications",base:"cupum-2021-rout.json",ext:".json",sourceBase:"cupum-2021-rout.yaml",sourceExt:".yaml"}},Wziy:function(e,t,i){"use strict";var a=function(e){return e&&e.__esModule?e:{default:e}};Object.defineProperty(t,"__esModule",{value:!0});var n=a(i("q1tI")),o=i("imt6");function s(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},t=e.enabled,i=void 0!==t&&t,a=e.hybrid,n=void 0!==a&&a,o=e.hasQuery;return i&&(!n||n&&(void 0!==o&&o))}t.isAmp=s,t.useAmp=function(){return s(n.default.useContext(o.AmpModeContext))},t.withAmp=function(e){var t=(arguments.length>1&&void 0!==arguments[1]?arguments[1]:{}).hybrid,i=void 0!==t&&t;function a(){var t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},a=n.default.useContext(o.AmpModeContext);return a.enabled=!0,a.hybrid=i,n.default.createElement(e,t)}return a.__nextAmpOnly=!i,a.getInitialProps=e.getInitialProps,a}},"XJU/":function(e,t,i){var a=i("NegM");e.exports=function(e,t,i){for(var n in t)i&&e[n]?e[n]=t[n]:a(e,n,t[n]);return e}},XNPm:function(e){e.exports={date:"2018-04",title:"Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction",authors:["Karthik Mahadevan","Sowmya Somanath","Ehud Sharlin"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3174003",keywords:"autonomous vehicle-pedestrian interaction, perceived awareness and intent in autonomous vehicles",pages:12,video:"https://www.youtube.com/watch?v=D_hhcGVREGA",talk:"https://www.youtube.com/watch?v=08OEKuz93dY",abstract:"Drivers use nonverbal cues such as vehicle speed, eye gaze, and hand gestures to communicate awareness and intent to pedestrians. Conversely, in autonomous vehicles, drivers can be distracted or absent, leaving pedestrians to infer awareness and intent from the vehicle alone. In this paper, we investigate the usefulness of interfaces (beyond vehicle movement) that explicitly communicate awareness and intent of autonomous vehicles to pedestrians, focusing on crosswalk scenarios. We conducted a preliminary study to gain insight on designing interfaces that communicate autonomous vehicle awareness and intent to pedestrians. Based on study outcomes, we developed four prototype interfaces and deployed them in studies involving a Segway and a car. We found interfaces communicating vehicle awareness and intent: (1) can help pedestrians attempting to cross; (2) are not limited to the vehicle and can exist in the environment; and (3) should use a combination of modalities such as visual, auditory, and physical.",dir:"content/output/publications",base:"chi-2018-mahadevan.json",ext:".json",sourceBase:"chi-2018-mahadevan.yaml",sourceExt:".yaml"}},Xj6D:function(e,t,i){"use strict";i.r(t);var a=i("XXOK"),n=i.n(a),o=i("p0XB"),s=i.n(o),r=i("XVgq"),c=i.n(r),l=i("Z7t5"),u=i.n(l),p=i("d04V"),h=i.n(p),d=i("pbKT"),m=i.n(d),g=i("UXZV"),f=i.n(g),y=i("0iUn"),v=i("sLSF"),b=i("Tit0"),w=i("MI3g"),j=i("a7VT"),k=i("q1tI"),x=i.n(k),z=i("LvDl"),S=i.n(z);function E(e,t){var i;if(void 0===u.a||null==e[c.a]){if(s()(e)||(i=function(e,t){if(!e)return;if("string"==typeof e)return C(e,t);var i=Object.prototype.toString.call(e).slice(8,-1);"Object"===i&&e.constructor&&(i=e.constructor.name);if("Map"===i||"Set"===i)return h()(e);if("Arguments"===i||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(i))return C(e,t)}(e))||t&&e&&"number"==typeof e.length){i&&(e=i);var a=0,o=function(){};return{s:o,n:function(){return a>=e.length?{done:!0}:{done:!1,value:e[a++]}},e:function(e){throw e},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,l=!0,p=!1;return{s:function(){i=n()(e)},n:function(){var e=i.next();return l=e.done,e},e:function(e){p=!0,r=e},f:function(){try{l||null==i.return||i.return()}finally{if(p)throw r}}}}function C(e,t){(null==t||t>e.length)&&(t=e.length);for(var i=0,a=new Array(t);i<t;i++)a[i]=e[i];return a}function I(e){var t=function(){if("undefined"==typeof Reflect||!m.a)return!1;if(m.a.sham)return!1;if("function"==typeof Proxy)return!0;try{return Date.prototype.toString.call(m()(Date,[],function(){})),!0}catch(e){return!1}}();return function(){var i,a=Object(j.default)(e);if(t){var n=Object(j.default)(this).constructor;i=m()(a,arguments,n)}else i=a.apply(this,arguments);return Object(w.default)(this,i)}}var A=function(e){Object(b.default)(i,e);var t=I(i);function i(e){var a;return Object(y.default)(this,i),(a=t.call(this,e)).props.publication?(a.publication=f()({},a.props.publication),a.people=f()([],a.props.people),a.namesId=f()({},a.props.namesId),a.names=a.people.map(function(e){return e.name}),a.publication.base&&(a.publication.id=a.publication.base.split(".json")[0]),a.getProceedings(),a.getVideoEmbed(),a.showFigures=!1,a.props.short||(a.getFigures(),a.figures[a.publication.id]&&(a.showFigures=!0)),a):Object(w.default)(a)}return Object(v.default)(i,[{key:"getProceedings",value:function(){var e=this.publication.series.slice(0,-5),t=this.publication.series.slice(-2);this.proceeding=this.props.booktitles[e],this.proceeding||(this.proceeding={}),this.proceeding.series="".concat(e," '").concat(t),this.publication.pages<4&&(this.proceeding.booktitle="Adjunct "+this.proceeding.booktitle)}},{key:"getVideoEmbed",value:function(){this.publication.video&&(this.publication.video.includes("youtube")&&(this.publication.embedId=this.publication.video.split("?v=")[1],this.publication.embed="https://www.youtube.com/embed/".concat(this.publication.embedId),this.publication.embedThumbnail="https://img.youtube.com/vi/".concat(this.publication.embedId,"/maxresdefault.jpg")),this.publication.video.includes("vimeo")&&(this.publication.embedId=this.publication.video.split("/")[3],this.publication.embed="https://player.vimeo.com/video/".concat(this.publication.embedId),this.publication.embedThumbnail=this.props.vimeo[this.publication.embedId])),this.publication.talk&&(this.publication.talk.includes("youtube")&&(this.publication.talkEmbedId=this.publication.talk.split("?v=")[1],this.publication.talkEmbed="https://www.youtube.com/embed/".concat(this.publication.talkEmbedId),this.publication.talkEmbedThumbnail="https://img.youtube.com/vi/".concat(this.publication.talkEmbedId,"/maxresdefault.jpg")),this.publication.talk.includes("vimeo")&&(this.publication.talkEmbedId=this.publication.talk.split("/")[3],this.publication.talkEmbed="https://player.vimeo.com/video/".concat(this.publication.talkEmbedId),this.publication.talkEmbedThumbnail=this.props.vimeo[this.publication.talkEmbedId]))}},{key:"getFigures",value:function(){var e=this.props.files.children.filter(function(e){return"images"===e.name})[0].children.filter(function(e){return"publications"===e.name})[0].children.filter(function(e){return"figures"===e.name})[0].children;this.figures={};var t,i=E(e);try{for(i.s();!(t=i.n()).done;){var a=t.value,n=a.name,o=a.children.map(function(e){return e.path});this.figures[n]=o}}catch(s){i.e(s)}finally{i.f()}}},{key:"render",value:function(){var e=this;return this.props.publication?x.a.createElement("div",{id:"publication"},x.a.createElement("div",{className:"block"},x.a.createElement("div",{id:"breadcrumb",className:"ui breadcrumb"},x.a.createElement("a",{className:"section",href:"/publications"},"Publications"),x.a.createElement("i",{className:"right angle icon divider"}),x.a.createElement("a",{className:"active section"},this.publication.series)),x.a.createElement("div",{className:"ui stackable grid",style:{marginTop:"10px"}},x.a.createElement("div",{className:"three wide column",style:{margin:"auto"}},x.a.createElement("img",{className:"cover",src:"/static/images/publications/cover/".concat(this.publication.id,".jpg")})),x.a.createElement("div",{className:"thirteen wide column"},this.props.short&&x.a.createElement("h1",null,x.a.createElement("a",{href:"/publications/".concat(this.publication.id),target:"_blank"},this.publication.title)),!this.props.short&&x.a.createElement("h1",null,this.publication.title),x.a.createElement("p",{className:"meta"},this.publication.authors.map(function(t){return e.names.includes(t)?x.a.createElement("a",{href:"/people/".concat(e.namesId[t]),key:t},x.a.createElement("img",{src:"/static/images/people/".concat(e.namesId[t],".jpg"),className:"ui circular spaced image mini-profile"}),x.a.createElement("strong",null,t)):x.a.createElement("span",{key:t},t)}).reduce(function(e,t){return[e," , ",t]})),x.a.createElement("p",null,x.a.createElement("a",{href:"/static/publications/".concat(this.publication.id,".pdf"),target:"_blank"},x.a.createElement("i",{className:"far fa-file-pdf fa-fw"}),"".concat(this.publication.id,".pdf")))))),this.publication.embed&&x.a.createElement("div",{className:"block"},x.a.createElement("div",{className:"video-container"},x.a.createElement("iframe",{className:"embed",width:"100%",height:"315",src:"".concat(this.publication.embed),srcDoc:"<style>*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}</style><a href=".concat(this.publication.embed,"?autoplay=1><img src=").concat(this.publication.embedThumbnail,"><span>▶</span></a>"),frameBorder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0,mozallowfullscreen:"true",msallowfullscreen:"true",oallowfullscreen:"true",webkitallowfullscreen:"true"}))),x.a.createElement("div",{className:"block"},x.a.createElement("h1",null,"Abstract"),x.a.createElement("p",null,this.publication.abstract),this.publication.keywords&&x.a.createElement("div",{className:"ui large basic labels"},"Keywords:  ",this.publication.keywords.split(", ").map(function(e){return x.a.createElement("span",{className:"ui brown basic label",key:e},S.a.startCase(e))}))),x.a.createElement("div",{className:"block"},x.a.createElement("h1",null,"Reference"),x.a.createElement("div",{className:"ui segment"},x.a.createElement("p",{style:{lineHeight:"160%"}},this.publication.authors.reduce(function(e,t){return[e,", ",t]}),". ",x.a.createElement("b",null,this.publication.title),". ",x.a.createElement("i",null,"In ".concat(this.proceeding.booktitle," (").concat(this.proceeding.series,")")),". ",this.proceeding.publisher,"  Page: 1-",this.publication.pages,".  DOI: ",x.a.createElement("a",{href:this.publication.doi,target:"_blank"},this.publication.doi)))),this.publication.talkEmbed&&x.a.createElement("div",{className:"block"},x.a.createElement("h1",null,"Talk"),x.a.createElement("div",{className:"video-container"},x.a.createElement("iframe",{className:"embed",width:"100%",height:"315",src:"".concat(this.publication.talkEmbed),srcDoc:"<style>*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}</style><a href=".concat(this.publication.talkEmbed,"?autoplay=1><img src=").concat(this.publication.talkEmbedThumbnail,"><span>▶</span></a>"),frameBorder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0,mozallowfullscreen:"true",msallowfullscreen:"true",oallowfullscreen:"true",webkitallowfullscreen:"true"}))),this.showFigures&&x.a.createElement("div",{className:"block"},x.a.createElement("h1",null,"Figures"),x.a.createElement("div",{id:"figure"},x.a.createElement("div",{className:"ui stackable three cards",style:{marginTop:"30px"}},this.figures[this.publication.id].map(function(e){return x.a.createElement("a",{className:"card",href:"/".concat(e),target:"_blank"},x.a.createElement("div",{className:"image"},x.a.createElement("img",{src:"/".concat(e)})))}))))):x.a.createElement("div",null)}}]),i}(x.a.Component);t.default=A},Xpnm:function(e){e.exports={date:"2018-05",title:"Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation",authors:["Ryo Suzuki","Jun Kato","Mark D. Gross","Tom Yeh"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3173773",keywords:"direct manipulation, tangible programming, swarm user interfaces, programming by demonstration",video:"https://www.youtube.com/watch?v=Gb7brajKCVE",pages:13,abstract:"We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high-level interface design. Inspired by current UI programming practices, we introduce a four-step workflow-create elements, abstract attributes, specify behaviors, and propagate changes-for Swarm UI programming. We propose a set of direct physical manipulation techniques to support each step in this workflow. To demonstrate these concepts, we developed Reactile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies-an in-class survey with 148 students and a lab interview with eight participants-confirm that our approach is intuitive and understandable for programming Swarm UIs.",dir:"content/output/publications",base:"chi-2018-suzuki.json",ext:".json",sourceBase:"chi-2018-suzuki.yaml",sourceExt:".yaml"}},YYe3:function(e,t,i){var a={"./assets-2017-suzuki.json":"CCdR","./chi-2015-aseniero.json":"eHLl","./chi-2015-jones.json":"AFJX","./chi-2015-oehlberg.json":"rQPd","./chi-2015-willett.json":"zRIs","./chi-2017-aoki.json":"jrLG","./chi-2017-hull.json":"IhcT","./chi-2017-ledo.json":"rPih","./chi-2017-somanath.json":"qFtH","./chi-2018-dillman.json":"EnfY","./chi-2018-feick.json":"EnQq","./chi-2018-heshmat.json":"Ln+l","./chi-2018-ledo.json":"tyxG","./chi-2018-mahadevan.json":"XNPm","./chi-2018-neustaedter.json":"yO3u","./chi-2018-oh.json":"5KnR","./chi-2018-suzuki.json":"Xpnm","./chi-2018-wuertz.json":"Si70","./chi-2019-danyluk.json":"v0jR","./chi-2020-anjani.json":"ZlYZ","./chi-2020-goffin.json":"M695","./chi-2020-hou.json":"+yqD","./chi-2020-suzuki.json":"MI4q","./chi-2021-danyluk.json":"pbez","./chi-2021-ens.json":"Blo1","./chi-2021-hammad.json":"ErCr","./cnc-2019-hammad.json":"ctjq","./cupum-2021-rout.json":"WxVc","./dis-2016-jones.json":"1kbU","./dis-2017-mok.json":"5rD/","./dis-2018-mikalauskas.json":"OvZk","./dis-2018-pham.json":"LEa2","./dis-2018-ta.json":"NysX","./dis-2019-bressa.json":"KMRZ","./dis-2019-ledo.json":"llnM","./dis-2019-mahadevan.json":"M9HE","./dis-2019-nakayama.json":"KjYe","./dis-2019-seyed.json":"MkNd","./dis-2021-wannamaker.json":"UTp2","./gi-2021-mactavish.json":"HFsU","./hri-2018-feick.json":"cJDD","./imwut-2020-wang.json":"hpIb","./iros-2020-hedayati.json":"9ed3","./mobilehci-2015-ledo.json":"eWNY","./mobilehci-2019-hung.json":"h56d","./nime-2020-ko.json":"+sZ5","./sui-2017-li.json":"qVJz","./tei-2016-somanath.json":"+/Wn","./tei-2019-mikalauskas.json":"U6lc","./tei-2019-tolley.json":"RawC","./tei-2019-wun.json":"k0FI","./tei-2020-suzuki.json":"TsNw","./tvcg-2016-lopez.json":"1D+r","./tvcg-2017-goffin.json":"78zJ","./tvcg-2017-willett.json":"aBZs","./tvcg-2019-blascheck.json":"xLid","./tvcg-2019-walny.json":"eEXe","./tvcg-2020-walny.json":"DjNc","./uist-2018-suzuki.json":"JW94","./uist-2019-suzuki.json":"k7FW","./uist-2020-suzuki.json":"3M7L","./uist-2020-yixian.json":"HFN6","./uist-2021-hapicbots.json":"LQiU","./uist-2021-suzuki.json":"3L7d","./vr-2019-satriadi.json":"slZ3"};function n(e){var t=o(e);return i(t)}function o(e){var t=a[e];if(!(t+1)){var i=new Error("Cannot find module '"+e+"'");throw i.code="MODULE_NOT_FOUND",i}return t}n.keys=function(){return Object.keys(a)},n.resolve=o,e.exports=n,n.id="YYe3"},ZW5q:function(e,t,i){"use strict";var a=i("eaoh");function n(e){var t,i;this.promise=new e(function(e,a){if(void 0!==t||void 0!==i)throw TypeError("Bad Promise constructor");t=e,i=a}),this.resolve=a(t),this.reject=a(i)}e.exports.f=function(e){return new n(e)}},ZlYZ:function(e){e.exports={date:"2020-04",title:"Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers",authors:["Laurensia Anjani","Terrance Mok","Anthony Tang","Lora Oehlberg","Wooi Boon Goh"],series:"CHI 2020",doi:"https://doi.org/10.1145/3313831.3376567",keywords:"video streams, mukbang",pages:13,talk:"https://www.youtube.com/watch?v=Dkp8A_em90M",abstract:"We present a mixed-methods study of viewers on their practices and motivations around watching mukbang — video streams of people eating large quantities of food. Viewers' experiences provide insight on future technologies for multisensorial video streams and technology-supported commensality (eating with others). We surveyed 104 viewers and interviewed 15 of them about their attitudes and reflections on their mukbang viewing habits, their physiological aspects of watching someone eat, and their perceived social relationship with mukbangers. Based on our findings, we propose design implications for remote commensality, and for synchronized multisensorial video streaming content.",dir:"content/output/publications",base:"chi-2020-anjani.json",ext:".json",sourceBase:"chi-2020-anjani.yaml",sourceExt:".yaml"}},aB1d:function(e){e.exports={182971005:"https://i.vimeocdn.com/video/592033369_640.webp",230834366:"https://i.vimeocdn.com/video/651490206_640.webp",275404995:"https://i.vimeocdn.com/video/707686230_640.webp",289789025:"https://i.vimeocdn.com/video/725516359_640.webp",360483702:"https://i.vimeocdn.com/video/814665539_640.webp",368703151:"https://i.vimeocdn.com/video/825448765_640.webp",dir:"content/output",base:"vimeo.json",ext:".json",sourceBase:"vimeo.yaml",sourceExt:".yaml"}},aBZs:function(e){e.exports={date:"2017-01",title:"Embedded Data Representations",authors:["Wesley Willett","Yvonne Jansen","Pierre Dragicevic"],series:"TVCG 2017",doi:"https://doi.org/10.1109/TVCG.2016.2598608",keywords:"information visualization, data physicalization, ambient displays, ubiquitous computing, augmented reality",pages:10,video:"https://vimeo.com/182971005",talk:"https://www.youtube.com/watch?v=ZS7lU60xChI",abstract:"We introduce embedded data representations, the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents – the real-world entities and spaces to which data corresponds – and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations, which display data in proximity to data referents, and embedded representations, which display data so that it spatially coincides with data referents. Drawing on examples from visualization, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and physicalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.",dir:"content/output/publications",base:"tvcg-2017-willett.json",ext:".json",sourceBase:"tvcg-2017-willett.yaml",sourceExt:".yaml"}},aPfg:function(e,t,i){"use strict";var a=i("Y7ZC"),n=i("eaoh"),o=i("2GTP"),s=i("oioR");e.exports=function(e){a(a.S,e,{from:function(e){var t,i,a,r,c=arguments[1];return n(this),(t=void 0!==c)&&n(c),null==e?new this:(i=[],t?(a=0,r=o(c,arguments[2],2),s(e,!1,function(e){i.push(r(e,a++))})):s(e,!1,i.push,i),new this(i))}})}},aW7e:function(e,t,i){i("wgeU"),i("FlQf"),i("bBy9"),i("JMW+"),i("PBE1"),i("Q/yX"),e.exports=i("WEpk").Promise},abLL:function(e,t,i){"use strict";i.r(t);var a=i("XXOK"),n=i.n(a),o=i("p0XB"),s=i.n(o),r=i("XVgq"),c=i.n(r),l=i("Z7t5"),u=i.n(l),p=i("d04V"),h=i.n(p),d=i("pbKT"),m=i.n(d),g=i("UXZV"),f=i.n(g),y=i("pLtp"),v=i.n(y),b=i("ln6h"),w=i.n(b),j=i("O40h"),k=i("0iUn"),x=i("sLSF"),z=i("Tit0"),S=i("MI3g"),E=i("a7VT"),C=i("q1tI"),I=i.n(C),A=(i("IujW"),i("6T/A")),_=i("QhjQ"),T=i("xjNh"),W=i("aB1d"),B=i("kU4l"),D=(i("m/Pd"),i("pZAB")),M=i("Xj6D"),R=i("yYy+");function P(e,t){var i;if(void 0===u.a||null==e[c.a]){if(s()(e)||(i=function(e,t){if(!e)return;if("string"==typeof e)return L(e,t);var i=Object.prototype.toString.call(e).slice(8,-1);"Object"===i&&e.constructor&&(i=e.constructor.name);if("Map"===i||"Set"===i)return h()(e);if("Arguments"===i||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(i))return L(e,t)}(e))||t&&e&&"number"==typeof e.length){i&&(e=i);var a=0,o=function(){};return{s:o,n:function(){return a>=e.length?{done:!0}:{done:!1,value:e[a++]}},e:function(e){throw e},f:o}}throw new TypeError("Invalid attempt to iterate non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}var r,l=!0,p=!1;return{s:function(){i=n()(e)},n:function(){var e=i.next();return l=e.done,e},e:function(e){p=!0,r=e},f:function(){try{l||null==i.return||i.return()}finally{if(p)throw r}}}}function L(e,t){(null==t||t>e.length)&&(t=e.length);for(var i=0,a=new Array(t);i<t;i++)a[i]=e[i];return a}function U(e){var t=function(){if("undefined"==typeof Reflect||!m.a)return!1;if(m.a.sham)return!1;if("function"==typeof Proxy)return!0;try{return Date.prototype.toString.call(m()(Date,[],function(){})),!0}catch(e){return!1}}();return function(){var i,a=Object(E.default)(e);if(t){var n=Object(E.default)(this).constructor;i=m()(a,arguments,n)}else i=a.apply(this,arguments);return Object(S.default)(this,i)}}var O=function(e){Object(z.default)(a,e);var t=U(a);function a(e){var n;Object(k.default)(this,a),n=t.call(this,e);var o=v()(A.fileMap).filter(function(e){return e.includes("people")});n.people=[];var s,r=P(o);try{for(r.s();!(s=r.n()).done;){var c=s.value,l=c.split("/")[3].replace(".json",""),u=f()(A.fileMap[c],{id:l});n.people.push(u)}}catch(m){r.e(m)}finally{r.f()}n.namesId={};var p,h=P(n.people);try{for(h.s();!(p=h.n()).done;){var d=p.value;n.namesId[d.name]=d.id}}catch(m){h.e(m)}finally{h.f()}return n.publication=i("YYe3")("./".concat(n.props.id,".json")),n}return Object(x.default)(a,null,[{key:"getInitialProps",value:function(){var e=Object(j.default)(w.a.mark(function e(t){var i;return w.a.wrap(function(e){for(;;)switch(e.prev=e.next){case 0:return i=t.query.id,e.abrupt("return",{id:i});case 2:case"end":return e.stop()}},e)}));return function(t){return e.apply(this,arguments)}}()}]),Object(x.default)(a,[{key:"render",value:function(){return I.a.createElement("div",null,I.a.createElement(B.default,{title:this.publication.title,description:this.publication.abstract,image:"/static/images/publications/cover/".concat(this.props.id,".jpg"),keywords:this.publication.keywords}),I.a.createElement(D.default,{current:"Publications"}),I.a.createElement("div",{className:"ui stackable grid"},I.a.createElement("div",{className:"one wide column"}),I.a.createElement("div",{className:"ten wide column centered",style:{marginTop:"30px"}},I.a.createElement(M.default,{publication:this.publication,namesId:this.namesId,people:this.people,booktitles:_,files:T,vimeo:W})),I.a.createElement("div",{className:"one wide column"})),I.a.createElement(R.default,null))}}]),a}(I.a.Component);t.default=O},cHUd:function(e,t,i){"use strict";var a=i("Y7ZC");e.exports=function(e){a(a.S,e,{of:function(){for(var e=arguments.length,t=new Array(e);e--;)t[e]=arguments[e];return new this(t)}})}},cJDD:function(e){e.exports={date:"2018-04",title:"The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration",authors:["Martin Feick","Lora Oehlberg","Anthony Tang","André Miede","Ehud Sharlin"],series:"HRI 2018",doi:"https://doi.org/10.1145/3173386.3176959",pages:2,keywords:"movement trajectory & velocity, remote collaboration, robot surrogate",abstract:"In this paper, we discuss the role of the movement trajectory and velocity enabled by our tele-robotic system (ReMa) for remote collaboration on physical tasks. Our system reproduces changes in object orientation and position at a remote location using a humanoid robotic arm. However, even minor kinematics differences between robot and human arm can result in awkward or exaggerated robot movements. As a result, user communication with the robotic system can become less efficient, less fluent and more time intensive.",dir:"content/output/publications",base:"hri-2018-feick.json",ext:".json",sourceBase:"hri-2018-feick.yaml",sourceExt:".yaml"}},ctjq:function(e){e.exports={date:"2019-06",title:"Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning",authors:["Nour Hammad","Elaheh Sanoubari","Patrick Finn","Sowmya Somanath","James E. Young","Ehud Sharlin"],year:2019,series:"C&C 2019",keywords:"interaction design, cyborgs, user experience, performing art techniques",video:"https://www.youtube.com/watch?v=HFH59__Fkok",doi:"https://doi.org/10.1145/3325480.3325508",abstract:"We present Mutation: performing arts based approach that can help decrease the cognitive load associated with cyborg transitioning. Cyborgs are human-machine hybrids with organic and mechatronic body parts that can be implanted or worn. The transition into and out of experiencing additional body parts is not fully understood. Our goal is to draw from performing arts techniques in order to help decrease the cognitive load associated with becoming and unbecoming a cyborg. Actors constantly shift between states, whether from one character to another, or from pre- to post- performance. We contribute a straightforward adaptation of classic performing art practices to cyborg transitioning, and a study where actors used these protocols in order to enter a cyborg state, perform as a cyborg, and then exit the cyborg state. Our work on Mutation suggests that classic performing art practices can be useful in cyborg transitioning, as well as in other technology augmented experiences.",dir:"content/output/publications",base:"cnc-2019-hammad.json",ext:".json",sourceBase:"cnc-2019-hammad.yaml",sourceExt:".yaml"}},dL40:function(e,t,i){var a=i("Y7ZC");a(a.P+a.R,"Set",{toJSON:i("8iia")("Set")})},dfwq:function(e,t,i){"use strict";i.r(t);var a=i("p0XB"),n=i.n(a);var o=i("d04V"),s=i.n(o),r=i("yLu3"),c=i.n(r);function l(e){return function(e){if(n()(e)){for(var t=0,i=new Array(e.length);t<e.length;t++)i[t]=e[t];return i}}(e)||function(e){if(c()(Object(e))||"[object Arguments]"===Object.prototype.toString.call(e))return s()(e)}(e)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance")}()}i.d(t,"default",function(){return l})},eEXe:function(e){e.exports={date:"2019-08",title:"Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff",authors:["Jagoda Walny","Christian Frisson","Mieka West","Doris Kosminsky","Søren Knudsen","Sheelagh Carpendale","Wesley Willett"],series:"TVCG 2019",doi:"https://doi.org/10.1109/TVCG.2019.2934538",keywords:"information visualization, design handoff, data mapping, design process",award:"Best Paper",pages:10,video:"https://vimeo.com/360483702",talk:"https://vimeo.com/368703151",abstract:"Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.",dir:"content/output/publications",base:"tvcg-2019-walny.json",ext:".json",sourceBase:"tvcg-2019-walny.yaml",sourceExt:".yaml"}},eHLl:function(e){e.exports={date:"2015-04",title:"Stratos: Using Visualization to Support Decisions in Strategic Software Release Planning",authors:["Bon Adriel Aseniero","Tiffany Wun","David Ledo","Guenther Ruhe","Anthony Tang","Sheelagh Carpendale"],series:"CHI 2015",doi:"https://doi.org/10.1145/2702123.2702426",keywords:"software engineering, information visualization, release planning",video:"https://www.youtube.com/watch?v=qm57aHjTAYc",pages:10,abstract:"Software is typically developed incrementally and released in stages. Planning these releases involves deciding which features of the system should be implemented for each release. This is a complex planning process involving numerous trade-offs-constraints and factors that often make decisions difficult. Since the success of a product depends on this plan, it is important to understand the trade-offs between different release plans in order to make an informed choice. We present STRATOS, a tool that simultaneously visualizes several software release plans. The visualization shows several attributes about each plan that are important to planners. Multiple plans are shown in a single layout to help planners find and understand the trade-offs between alternative plans. We evaluated our tool via a qualitative study and found that STRATOS enables a range of decision-making processes, helping participants decide on which plan is most optimal.",dir:"content/output/publications",base:"chi-2015-aseniero.json",ext:".json",sourceBase:"chi-2015-aseniero.yaml",sourceExt:".yaml"}},eVuF:function(e,t,i){e.exports=i("aW7e")},eWNY:function(e){e.exports={date:"2015-08",title:"Proxemic-Aware Controls: Designing Remote Controls for Ubiquitous Computing Ecologies",authors:["David Ledo","Saul Greenberg","Nicolai Marquardt","Sebastian Boring"],series:"MobileHCI 2015",doi:"https://doi.org/10.1145/2785830.2785871",keywords:"ubiquitous computing, proxemic-interaction, mobile interaction, control of appliances",video:"https://www.youtube.com/watch?v=1AlMUmD6E3U",pages:10,abstract:"Remote controls facilitate interactions at-a-distance with appliances. However, the complexity, diversity, and increasing number of digital appliances in ubiquitous computing ecologies make it increasingly difficult to: (1) discover which appliances are controllable; (2) select a particular appliance from the large number available; (3) view information about its status; and (4) control the appliance in a pertinent manner. To mitigate these problems we contribute proxemic-aware controls, which exploit the spatial relationships between a person's handheld device and all surrounding appliances to create a dynamic appliance control interface. Specifically, a person can discover and select an appliance by the way one orients a mobile device around the room, and then progressively view the appliance's status and control its features in increasing detail by simply moving towards it. We illustrate proxemic-aware controls of assorted appliances through various scenarios. We then provide a generalized conceptual framework that informs future designs of proxemic-aware controls.",dir:"content/output/publications",base:"mobilehci-2015-ledo.json",ext:".json",sourceBase:"mobilehci-2015-ledo.yaml",sourceExt:".yaml"}},h56d:function(e){e.exports={date:"2019-10",title:"WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction",authors:["Michael Hung","David Ledo","Lora Oehlberg"],series:"MobileHCI 2019",doi:"https://doi.org/10.1145/3338286.3340122",keywords:"smartwatch, cross-device interaction, pen interaction, interaction techniques",video:"https://www.youtube.com/watch?v=ilyJBzTzQAA",pages:8,abstract:"Pen-based input is often treated as auxiliary to mobile devices. We posit that cross-device interactions can inspire and extend the design space of pen-based interactions into new, expressive directions. We realize this through WatchPen, a smartwatch mounted on a passive, capacitive stylus that: (1) senses the usage context and leverages it for expression (e.g., changing colour), (2) contains tools and parameters within the display, and (3) acts as an on-demand output. As a result, it provides users with a dynamic relationship between inputs and outputs, awareness of current tool selection and parameters, and increased expressive match (e.g., added ability to mimic physical tools, showing clipboard contents). We discuss and reflect upon a series of interaction techniques that demonstrate WatchPen within a drawing application. We highlight the expressive power of leveraging multiple sensing and output capabilities across both the watch-augmented stylus and the tablet surface.",dir:"content/output/publications",base:"mobilehci-2019-hung.json",ext:".json",sourceBase:"mobilehci-2019-hung.yaml",sourceExt:".yaml"}},hpIb:function(e){e.exports={date:"2020-06",title:"AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters",authors:["Xiyue Wang","Kazuki Takashima","Tomoaki Adachi","Patrick Finn","Ehud Sharlin","Yoshifumi Kitamura"],series:"IMWUT 2020",doi:"https://doi.org/10.1145/3381016",keywords:"well being, toy blocks, PTSD, tangibles for health, stress assessment, play, children",pages:29,video:"https://www.youtube.com/watch?v=fxxvZBY80ug",abstract:"Natural disasters cause long-lasting mental health problems such as PTSD in children. Following the 2011 Earthquake and Tsunami in Japan, we witnessed a shift of toy block play behavior in young children who suffered from stress after the disaster. The behavior reflected their emotional responses to the traumatic event. In this paper, we explore the feasibility of using data captured from block-play to assess children's stress after a major natural disaster. We prototyped sets of sensor-embedded toy blocks, AssessBlocks, that automate quantitative play data acquisition. During a three-year period, the blocks were dispatched to fifty-two post-disaster children. Within a free play session, we captured block features, a child's playing behavior, and stress evaluated by several methods. The result from our analysis reveal correlations between block play features and stress measurements and show initial promise of using the effectiveness of using AssessBlocks to assess children's stress after a disaster. We provide detailed insights into the potential as well as the challenges of our approach and unique conditions. From these insights we summarize guidelines for future research in automated play assessment systems that support children's mental health.",dir:"content/output/publications",base:"imwut-2020-wang.json",ext:".json",sourceBase:"imwut-2020-wang.yaml",sourceExt:".yaml"}},imt6:function(e,t,i){"use strict";var a=function(e){if(e&&e.__esModule)return e;var t={};if(null!=e)for(var i in e)Object.hasOwnProperty.call(e,i)&&(t[i]=e[i]);return t.default=e,t};Object.defineProperty(t,"__esModule",{value:!0});var n=a(i("q1tI"));t.AmpModeContext=n.createContext({})},j7IG:function(e,t,i){var a=function(){return this||"object"==typeof self&&self}()||Function("return this")(),n=a.regeneratorRuntime&&Object.getOwnPropertyNames(a).indexOf("regeneratorRuntime")>=0,o=n&&a.regeneratorRuntime;if(a.regeneratorRuntime=void 0,e.exports=i("S8m4"),n)a.regeneratorRuntime=o;else try{delete a.regeneratorRuntime}catch(s){a.regeneratorRuntime=void 0}},jrLG:function(e){e.exports={date:"2017-05",title:"Environmental Protection and Agency: Motivations, Capacity, and Goals in Participatory Sensing",authors:["Paul Aoki","Allison Woodruff","Baladitya Yellapragada","Wesley Willett"],series:"CHI 2017",doi:"https://doi.org/10.1145/3025453.3025667",keywords:"citizen science, environmental sensing",pages:13,abstract:'In this paper we consider various genres of citizen science from the perspective of citizen participants. As a mode of scientific inquiry, citizen science has the potential to "scale up" scientific data collection efforts and increase lay engagement with science. However, current technological directions risk losing sight of the ways in which citizen science is actually practiced. As citizen science is increasingly used to describe a wide range of activities, we begin by presenting a framework of citizen science genres. We then present findings from four interlocking qualitative studies and technological interventions of community air quality monitoring efforts, examining the motivations and capacities of citizen participants and characterizing their alignment with different types of citizen science. Based on these studies, we suggest that data acquisition involves complex multi-dimensional tradeoffs, and the commonly held view that citizen science systems are a win-win for citizens and science may be overstated.',dir:"content/output/publications",base:"chi-2017-aoki.json",ext:".json",sourceBase:"chi-2017-aoki.yaml",sourceExt:".yaml"}},k0FI:function(e){e.exports={date:"2019-03",title:"You say Potato, I say Po-Data: Physical Template Tools for Authoring Visualizations",authors:["Tiffany Wun","Lora Oehlberg","Miriam Sturdee","Sheelagh Carpendale"],series:"TEI 2019",doi:"https://doi.org/10.1145/3294109.3295627",keywords:"potato, tangible tools, authoring visualizations, block-printing, physical template tools, information visualization",pages:10,abstract:"Providing data visualization authoring tools for the general public remains an ongoing challenge. Inspired by block-printing, we explore how visualization stamps as a physical visualization authoring tool could leverage both visual freedom and ease of repetition. We conducted a workshop with two groups---visualization experts and non-experts---where participants authored visualizations on paper using hand-carved stamps made from potatoes and sponges. The low-fidelity medium freed participants to test new stamp patterns and accept mistakes. From the created visualizations, we observed several unique traits and uses of block-printing tools for visualization authoring, including: modularity of patterns, annotation guides, creation of multiple patterns from one stamp, and various techniques to apply data onto paper. We discuss the issues around expressivity and effectiveness of block-printed stamps in visualization authoring, and identify implications for the design and assembly of primitives in potential visualization stamp kits, as well as applications for future use in non-digital environments.",dir:"content/output/publications",base:"tei-2019-wun.json",ext:".json",sourceBase:"tei-2019-wun.yaml",sourceExt:".yaml"}},k7FW:function(e){e.exports={date:"2019-10",title:"ShapeBots: Shape-changing Swarm Robots",authors:["Ryo Suzuki","Clement Zheng","Yasuaki Kakehi","Tom Yeh","Ellen Yi-Luen Do","Mark D. Gross","Daniel Leithinger"],series:"UIST 2019",keywords:"swarm user interfaces, shape-changing user interfaces",doi:"https://doi.org/10.1145/3332165.3347911",video:"https://www.youtube.com/watch?v=cwPaof0kKdM",pages:13,abstract:"We introduce shape-changing swarm robots. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.",dir:"content/output/publications",base:"uist-2019-suzuki.json",ext:".json",sourceBase:"uist-2019-suzuki.yaml",sourceExt:".yaml"}},kU4l:function(e,t,i){"use strict";i.r(t);var a=i("pbKT"),n=i.n(a),o=i("0iUn"),s=i("sLSF"),r=i("Tit0"),c=i("MI3g"),l=i("a7VT"),u=i("q1tI"),p=i.n(u),h=i("m/Pd"),d=i.n(h);function m(e){var t=function(){if("undefined"==typeof Reflect||!n.a)return!1;if(n.a.sham)return!1;if("function"==typeof Proxy)return!0;try{return Date.prototype.toString.call(n()(Date,[],function(){})),!0}catch(e){return!1}}();return function(){var i,a=Object(l.default)(e);if(t){var o=Object(l.default)(this).constructor;i=n()(a,arguments,o)}else i=a.apply(this,arguments);return Object(c.default)(this,i)}}var g=function(e){Object(r.default)(i,e);var t=m(i);function i(e){var a;Object(o.default)(this,i);var n="Interactions Lab - University of Calgary HCI Group";return(a=t.call(this,e)).props.title?a.title="".concat(a.props.title," | ").concat(n):a.title=n,a.props.description?a.description=a.props.description:a.description="Human-Computer Interaction and Information Visualization Group at the University of Calgary",a.props.keywords?a.keywords=a.props.keywords:a.keywords="Human-Computer Interaction, HCI, Information Visualization, University of Calgary, CHI, UIST",a.props.image?a.image="https://ilab.ucalgary.ca"+a.props.image:a.image="https://ilab.ucalgary.ca/static/images/cover.jpg",a.props.url?a.url=a.props.url:a.url="https://ilab.ucalgary.ca/",a}return Object(s.default)(i,[{key:"render",value:function(){return p.a.createElement(d.a,null,p.a.createElement("title",null,this.title),p.a.createElement("meta",{name:"keywords",content:this.keywords}),p.a.createElement("meta",{name:"description",content:this.description}),p.a.createElement("meta",{property:"og:title",content:this.title}),p.a.createElement("meta",{property:"og:description",content:this.description}),p.a.createElement("meta",{property:"og:site_name",content:"University of Calgary Interactions Lab"}),p.a.createElement("meta",{property:"og:url",content:this.url}),p.a.createElement("meta",{property:"og:image",content:this.image}),p.a.createElement("meta",{property:"og:type",content:"website"}),p.a.createElement("meta",{name:"twitter:title",content:this.title}),p.a.createElement("meta",{name:"twitter:description",content:this.description}),p.a.createElement("meta",{name:"twitter:image",content:this.image}),p.a.createElement("meta",{name:"twitter:card",content:"summary"}),p.a.createElement("meta",{name:"twitter:site",content:"@ucalgary"}),p.a.createElement("meta",{name:"twitter:url",content:this.url}))}}]),i}(p.a.Component);t.default=g},ldVq:function(e,t,i){var a=i("QMMT"),n=i("UWiX")("iterator"),o=i("SBuE");e.exports=i("WEpk").isIterable=function(e){var t=Object(e);return void 0!==t[n]||"@@iterator"in t||o.hasOwnProperty(a(t))}},llnM:function(e){e.exports={date:"2019-06",title:"Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications",authors:["David Ledo","Jo Vermeulen","Sheelagh Carpendale","Saul Greenberg","Lora Oehlberg","Sebastian Boring"],series:"DIS 2019",doi:"https://doi.org/10.1145/3322276.3322329",keywords:"smart objects, mobile interfaces, prototyping, design tool, interactive behaviour",pages:14,abstract:"Astral is a prototyping tool for authoring mobile and smart object interactive behaviours. It mirrors selected display contents of desktop applications onto mobile devices (smartphones and smartwatches), and streams/remaps mobile sensor data to desktop input events (mouse or keyboard) to manipulate selected desktop contents. This allows designers to use familiar desktop applications (e.g. PowerPoint, AfterEffects) to prototype rich interactive behaviours. Astral combines and integrates display mirroring, sensor streaming and input remapping, where designers can exploit familiar desktop applications to prototype, explore and fine-tune dynamic interactive behaviours. With Astral, designers can visually author rules to test real-time behaviours while interactions take place, as well as after the interaction has occurred. We demonstrate Astral's applicability, workflow and expressiveness within the interaction design process through both new examples and replication of prior approaches that illustrate how various familiar desktop applications are leveraged and repurposed.",dir:"content/output/publications",base:"dis-2019-ledo.json",ext:".json",sourceBase:"dis-2019-ledo.yaml",sourceExt:".yaml"}},ln6h:function(e,t,i){e.exports=i("j7IG")},"m/Pd":function(e,t,i){"use strict";var a=i("KI45")(i("ttDY")),n=function(e){return e&&e.__esModule?e:{default:e}};Object.defineProperty(t,"__esModule",{value:!0});var o=n(i("q1tI")),s=n(i("4hZ1")),r=i("imt6"),c=i("IClC"),l=i("Wziy");function u(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"next-head",t=arguments.length>1&&void 0!==arguments[1]&&arguments[1],i=[o.default.createElement("meta",{key:"charSet",charSet:"utf-8",className:e})];return t||i.push(o.default.createElement("meta",{key:"viewport",name:"viewport",content:"width=device-width,minimum-scale=1,initial-scale=1",className:e})),i}function p(e,t){return"string"==typeof t||"number"==typeof t?e:t.type===o.default.Fragment?e.concat(o.default.Children.toArray(t.props.children).reduce(function(e,t){return"string"==typeof t||"number"==typeof t?e:e.concat(t)},[])):e.concat(t)}t.defaultHead=u;var h=["name","httpEquiv","charSet","viewport","itemProp"];function d(e,t){return e.reduce(function(e,t){var i=o.default.Children.toArray(t.props.children);return e.concat(i)},[]).reduce(p,[]).reverse().concat(u("",t.isAmp)).filter((i=new a.default,n=new a.default,s=new a.default,r={},function(e){if(e.key&&"number"!=typeof e.key&&0===e.key.indexOf(".$"))return!i.has(e.key)&&(i.add(e.key),!0);switch(e.type){case"title":case"base":if(n.has(e.type))return!1;n.add(e.type);break;case"meta":for(var t=0,o=h.length;t<o;t++){var c=h[t];if(e.props.hasOwnProperty(c))if("charSet"===c||"viewport"===c){if(s.has(c))return!1;s.add(c)}else{var l=e.props[c],u=r[c]||new a.default;if(u.has(l))return!1;u.add(l),r[c]=u}}}return!0})).reverse().map(function(e,t){var i=(e.props&&e.props.className?e.props.className+" ":"")+"next-head",a=e.key||t;return o.default.cloneElement(e,{key:a,className:i})});var i,n,s,r}var m=s.default();function g(e){var t=e.children;return o.default.createElement(r.AmpModeContext.Consumer,null,function(e){return o.default.createElement(c.HeadManagerContext.Consumer,null,function(i){return o.default.createElement(m,{reduceComponentsToState:d,handleStateChange:i,isAmp:l.isAmp(e)},t)})})}g.rewind=m.rewind,t.default=g},mDBt:function(e,t,i){(window.__NEXT_P=window.__NEXT_P||[]).push(["/publication",function(){var e=i("abLL");return{page:e.default||e}}])},n3ko:function(e,t,i){var a=i("93I4");e.exports=function(e,t){if(!a(e)||e._t!==t)throw TypeError("Incompatible receiver, "+t+" required!");return e}},oioR:function(e,t,i){var a=i("2GTP"),n=i("sNwI"),o=i("NwJ3"),s=i("5K7Z"),r=i("tEej"),c=i("fNZA"),l={},u={};(t=e.exports=function(e,t,i,p,h){var d,m,g,f,y=h?function(){return e}:c(e),v=a(i,p,t?2:1),b=0;if("function"!=typeof y)throw TypeError(e+" is not iterable!");if(o(y)){for(d=r(e.length);d>b;b++)if((f=t?v(s(m=e[b])[0],m[1]):v(e[b]))===l||f===u)return f}else for(g=y.call(e);!(m=g.next()).done;)if((f=n(g,v,m.value,t))===l||f===u)return f}).BREAK=l,t.RETURN=u},pZAB:function(e,t,i){"use strict";i.r(t);var a=i("pbKT"),n=i.n(a),o=i("0iUn"),s=i("sLSF"),r=i("Tit0"),c=i("MI3g"),l=i("a7VT"),u=i("q1tI"),p=i.n(u);i("IujW");function h(e){var t=function(){if("undefined"==typeof Reflect||!n.a)return!1;if(n.a.sham)return!1;if("function"==typeof Proxy)return!0;try{return Date.prototype.toString.call(n()(Date,[],function(){})),!0}catch(e){return!1}}();return function(){var i,a=Object(l.default)(e);if(t){var o=Object(l.default)(this).constructor;i=n()(a,arguments,o)}else i=a.apply(this,arguments);return Object(c.default)(this,i)}}var d=function(e){Object(r.default)(i,e);var t=h(i);function i(e){var a;return Object(o.default)(this,i),(a=t.call(this,e)).items=["Publications","People","Courses","Facility","Seminar","Location"],a}return Object(s.default)(i,[{key:"render",value:function(){var e=this;return p.a.createElement("div",null,p.a.createElement("div",{className:"ui right vertical sidebar menu"},p.a.createElement("a",{className:"item",href:"/"},"Home"),this.items.map(function(t){return p.a.createElement("a",{className:e.props.current==t?"item active":"item",href:"/".concat(t.toLowerCase()),key:t},t)})),p.a.createElement("div",{className:"ui stackable secondary pointing container menu",style:{borderBottom:"none",marginRight:"15%",fontSize:"1.1em"}},p.a.createElement("div",{className:"left menu"},p.a.createElement("a",{className:"item",href:"/"},p.a.createElement("b",null,"UCalgary iLab"))),p.a.createElement("div",{className:"right menu"},this.items.map(function(t){return p.a.createElement("a",{className:e.props.current==t?"item active":"item",href:"/".concat(t.toLowerCase()),key:t},t)}),p.a.createElement("div",{className:"toc item"},p.a.createElement("a",{href:"/"},p.a.createElement("b",null,"UCalgary iLab")),p.a.createElement("i",{style:{float:"right"},className:"sidebar icon"})))))}}]),i}(p.a.Component);t.default=d},pbez:function(e){e.exports={date:"2021-05",title:"A Design Space Exploration of Worlds in Miniature",authors:["Kurtis Danyluk","Barrett Ens","Bernhard Jenny","Wesley Willett"],series:"CHI 2021",doi:null,keywords:"Virtual/Augmented Reality, Meta-Analysis/Literature Survey",pages:20,video:null,abstract:"Worlds-in-Miniature (WiMs) are interactive worlds within a world and combine the advantages of an input space, a cartographicmap, and an overview+detail interface. They have been used across the extended virtuality spectrum for a variety of applications.Building on an analysis of examples of WiMs from the research literature we contribute a design space for WiMs based on sevendesign dimensions. Further, we expand upon existing definitions of WiMs to provide a definition that applies across the extendedreality spectrum. We identify the design dimensions of size-scope-scale, abstraction, geometry, reference frame, links, multiples, andvirtuality. Using our framework we describe existing Worlds-in-Miniature from the research literature and reveal unexplored researchareas. Finally, we generate new examples of WiMs using our framework to fill some of these gaps. With our findings, we identifyopportunities that can guide future research into WiMs.",dir:"content/output/publications",base:"chi-2021-danyluk.json",ext:".json",sourceBase:"chi-2021-danyluk.yaml",sourceExt:".yaml"}},q6LJ:function(e,t,i){var a=i("5T2Y"),n=i("QXhf").set,o=a.MutationObserver||a.WebKitMutationObserver,s=a.process,r=a.Promise,c="process"==i("a0xu")(s);e.exports=function(){var e,t,i,l=function(){var a,n;for(c&&(a=s.domain)&&a.exit();e;){n=e.fn,e=e.next;try{n()}catch(o){throw e?i():t=void 0,o}}t=void 0,a&&a.enter()};if(c)i=function(){s.nextTick(l)};else if(!o||a.navigator&&a.navigator.standalone)if(r&&r.resolve){var u=r.resolve(void 0);i=function(){u.then(l)}}else i=function(){n.call(a,l)};else{var p=!0,h=document.createTextNode("");new o(l).observe(h,{characterData:!0}),i=function(){h.data=p=!p}}return function(a){var n={fn:a,next:void 0};t&&(t.next=n),e||(e=n,i()),t=n}}},qFtH:function(e){e.exports={date:"2017-05",title:"'Maker' within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India",authors:["Sowmya Somanath","Lora Oehlberg","Janette Hughes","Ehud Sharlin","Mario Costa Sousa"],series:"CHI 2017",doi:"https://doi.org/10.1145/3025453.3025849",pages:13,keywords:"India, HCI4D, physical computing, DIY, young learners, maker culture",video:"https://www.youtube.com/watch?v=NpIME1h1mH8",abstract:"Do-it-yourself (DIY) inspired activities have gained popularity as a means of creative expression and self-directed learning. However, DIY culture is difficult to implement in places with limited technology infrastructure and traditional learning cultures. Our goal is to understand how learners in such a setting react to DIY activities. We present observations from a physical computing workshop with 12 students (13-15 years old) conducted at a high school in India. We observed unique challenges for these students when tackling DIY activities: a high monetary and psychological cost to exploration, limited independent learning resources, difficulties with finding intellectual courage and assumed technical language proficiency. Our participants, however, overcome some of these challenges by adopting their own local strategies: resilience, nonverbal and verbal learning techniques, and creating documentation and fallback circuit versions. Based on our findings, we discuss a set of lessons learned about makerspaces in a context with socio-technical challenges.",dir:"content/output/publications",base:"chi-2017-somanath.json",ext:".json",sourceBase:"chi-2017-somanath.yaml",sourceExt:".yaml"}},qVJz:function(e){e.exports={date:"2017-10",title:"Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models",authors:["Nico Li","Wesley Willett","Ehud Sharlin","Mario Costa Sousa"],series:"SUI 2017",doi:"https://doi.org/10.1145/3131277.3132178",keywords:"terrain visualization, geospatial visualization, dynamic viewshed, topographic maps, tangible user interfaces",pages:9,video:"https://vimeo.com/275404995",talk:"https://www.youtube.com/watch?v=aVXUojoQF60",abstract:"We compare the effectiveness of 2D maps and 3D terrain models for visibility tasks and demonstrate how interactive dynamic viewsheds can improve performance for both types of terrain representations. In general, the two-dimensional nature of classic topographic maps limits their legibility and can make complex yet typical cartographic tasks like determining the visibility between locations difficult. Both 3D physical models and interactive techniques like dynamic viewsheds have the potential to improve viewers' understanding of topography, but their impact has not been deeply explored. We evaluate the effectiveness of 2D maps, 3D models, and interactive viewsheds for both simple and complex visibility tasks. Our results demonstrate the benefits of the dynamic viewshed technique and highlight opportunities for additional tactile interactions. Based on these findings we present guidelines for improving the design and usability of future topographic maps and models.",dir:"content/output/publications",base:"sui-2017-li.json",ext:".json",sourceBase:"sui-2017-li.yaml",sourceExt:".yaml"}},rPih:function(e){e.exports={date:"2017-05",title:"Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices",authors:["David Ledo","Fraser Anderson","Ryan Schmidt","Lora Oehlberg","Saul Greenberg","Tovi Grossman"],series:"CHI 2017",doi:"https://doi.org/10.1145/3025453.3025652",pages:10,keywords:"fabrication, 3d printing, smart objects, rapid prototyping, toolkits, prototyping tool, interaction design",video:"https://www.youtube.com/watch?v=ORN9jljPncc","video-2":"https://www.youtube.com/watch?v=ifEIR7cHWxc",talk:"https://www.youtube.com/watch?v=QB0kbcu_CsY",abstract:'Interactive, smart objects – customized to individuals and uses – are central to many movements, such as tangibles, the internet of things (IoT), and ubiquitous computing. Yet, rapid prototyping both the form and function of these custom objects can be problematic, particularly for those with limited electronics or programming experience. Designers often need to embed custom circuitry; program its workings; and create a form factor that not only reflects the desired user experience but can also house the required circuitry and electronics. To mitigate this, we created Pineal, a design tool that lets end-users: (1) modify 3D models to include a smart watch or phone as its heart; (2) specify high-level interactive behaviours through visual programming; and (3) have the phone or watch act out such behaviours as the objects\' "smarts". Furthermore, a series of prototypes show how Pineal exploits mobile sensing and output, and automatically generates 3D printed form-factors for rich, interactive, objects.',dir:"content/output/publications",base:"chi-2017-ledo.json",ext:".json",sourceBase:"chi-2017-ledo.yaml",sourceExt:".yaml"}},rQPd:function(e){e.exports={date:"2015-05",title:"Patterns of Physical Design Remixing in Online Maker Communities",authors:["Lora Oehlberg","Wesley Willett","Wendy E. Mackay"],series:"CHI 2015",doi:"https://doi.org/10.1145/2702123.2702175",keywords:"customization, maker communities, user innovation, collaboration, hacking, remixing",pages:12,talk:"https://www.youtube.com/watch?v=vJrVjH04nGc",abstract:"Makers participate in remixing culture by drawing inspiration from, combining, and adapting designs for physical objects. To examine how makers remix each others' designs on a community scale, we analyzed metadata from over 175,000 digital designs from Thingiverse, the largest online design community for digital fabrication. Remixed designs on Thingiverse are predominantly generated designs from Customizer a built-in web app for adjusting parametric designs. However, we find that these designs do not elicit subsequent user activity and the authors who generate them tend not to contribute additional content to Thingiverse. Outside of Customizer, influential sources of remixing include complex assemblies and design primitives, as well as non-physical resources posing as physical designs. Building on our findings, we discuss ways in which online maker communities could become more than just design repositories and better support collaborative remixing.",dir:"content/output/publications",base:"chi-2015-oehlberg.json",ext:".json",sourceBase:"chi-2015-oehlberg.yaml",sourceExt:".yaml"}},raTm:function(e,t,i){"use strict";var a=i("5T2Y"),n=i("Y7ZC"),o=i("6/1s"),s=i("KUxP"),r=i("NegM"),c=i("XJU/"),l=i("oioR"),u=i("EXMj"),p=i("93I4"),h=i("RfKB"),d=i("2faE").f,m=i("V7Et")(0),g=i("jmDH");e.exports=function(e,t,i,f,y,v){var b=a[e],w=b,j=y?"set":"add",k=w&&w.prototype,x={};return g&&"function"==typeof w&&(v||k.forEach&&!s(function(){(new w).entries().next()}))?(w=t(function(t,i){u(t,w,e,"_c"),t._c=new b,null!=i&&l(i,y,t[j],t)}),m("add,clear,delete,forEach,get,has,set,keys,values,entries,toJSON".split(","),function(e){var t="add"==e||"set"==e;e in k&&(!v||"clear"!=e)&&r(w.prototype,e,function(i,a){if(u(this,w,e),!t&&v&&!p(i))return"get"==e&&void 0;var n=this._c[e](0===i?0:i,a);return t?this:n})}),v||d(w.prototype,"size",{get:function(){return this._c.size}})):(w=f.getConstructor(t,e,y,j),c(w.prototype,i),o.NEED=!0),h(w,e),x[e]=w,n(n.G+n.W+n.F,x),v||f.setStrong(w,e,y),w}},slZ3:function(e){e.exports={date:"2019-03",title:"Augmented Reality Map Navigation with Freehand Gestures",authors:["Kadek Ananta Satriadi","Barrett Ens","Maxime Cordeil","Bernhard Jenny","Tobias Czauderna","Wesley Willett"],series:"IEEE VR 2019",doi:"https://doi.org/10.1109/VR.2019.8798340",keywords:"augmented reality, gesture recognition, human computer interaction, interactive devices",pages:11,video:"https://www.youtube.com/watch?v=TE6AJEu8zdY",talk:"https://www.youtube.com/watch?v=jNeEbB3sTn0",abstract:"Freehand gesture interaction has long been proposed as a `natural' input method for Augmented Reality (AR) applications, yet has been little explored for intensive applications like multiscale navigation. In multiscale navigation, such as digital map navigation, pan and zoom are the predominant interactions. A position-based input mapping (e.g. grabbing metaphor) is intuitive for such interactions, but is prone to arm fatigue. This work focuses on improving digital map navigation in AR with mid-air hand gestures, using a horizontal intangible map display. First, we conducted a user study to explore the effects of handedness (unimanual and bimanual) and input mapping (position-based and rate-based). From these findings we designed DiveZoom and TerraceZoom, two novel hybrid techniques that smoothly transition between position- and rate-based mappings. A second user study evaluated these designs. Our results indicate that the introduced input-mapping transitions can reduce perceived arm fatigue with limited impact on performance.",dir:"content/output/publications",base:"vr-2019-satriadi.json",ext:".json",sourceBase:"vr-2019-satriadi.yaml",sourceExt:".yaml"}},ttDY:function(e,t,i){e.exports=i("+iuc")},tyxG:function(e){e.exports={date:"2018-04",title:"Evaluation Strategies for HCI Toolkit Research",authors:["David Ledo","Steven Houben","Jo Vermeulen","Nicolai Marquardt","Lora Oehlberg","Saul Greenberg"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3173610",keywords:"user interfaces, design, evaluation, prototyping, toolkits",pages:17,video:"https://www.youtube.com/watch?v=3lAwhCk60C4",talk:"https://www.youtube.com/watch?v=NOhsvN_Kv-I",abstract:"Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what 'evaluating' a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.",dir:"content/output/publications",base:"chi-2018-ledo.json",ext:".json",sourceBase:"chi-2018-ledo.yaml",sourceExt:".yaml"}},v0jR:function(e){e.exports={date:"2019-04",title:"Look-From Camera Control for 3D Terrain Maps",authors:["Kurtis Danyluk","Bernhard Jenny","Wesley Willett"],series:"CHI 2019",doi:"https://doi.org/10.1145/3290605.3300594",keywords:"terrain, touch, map interaction, look-from camera control",award:"Honorable Mention",pages:12,abstract:"We introduce three lightweight interactive camera control techniques for 3D terrain maps on touch devices based on a look-from metaphor (Discrete Look-From-At, Continuous Look-From-Forwards, and Continuous Look-From-Towards). These techniques complement traditional touch screen pan, zoom, rotate, and pitch controls allowing viewers to quickly transition between top-down, oblique, and ground-level views. We present the results of a study in which we asked participants to perform elevation comparison and line-of-sight determination tasks using each technique. Our results highlight how look-from techniques can be integrated on top of current direct manipulation navigation approaches by combining several direct manipulation operations into a single look-from operation. Additionally, they show how look-from techniques help viewers complete a variety of common and challenging map-based tasks.",dir:"content/output/publications",base:"chi-2019-danyluk.json",ext:".json",sourceBase:"chi-2019-danyluk.yaml",sourceExt:".yaml"}},v6xn:function(e,t,i){var a=i("C2SN");e.exports=function(e,t){return new(a(e))(t)}},vBP9:function(e,t,i){var a=i("5T2Y").navigator;e.exports=a&&a.userAgent||""},xLid:function(e){e.exports={date:"2019-06",title:"Exploration Strategies for Discovery of Interactivity in Visualizations",authors:["Tanja Blascheck","Lindsay MacDonald Vermeulen","Jo Vermeulen","Charles Perin","Wesley Willett","Thomas Ertl","Sheelagh Carpendale"],series:"TVCG 2019",doi:"https://doi.org/10.1109/TVCG.2018.2802520",keywords:"discovery, visualization, open data, evaluation, eye tracking, interaction logs, think-aloud",video:"https://vimeo.com/289789025",pages:13,abstract:"We investigate how people discover the functionality of an interactive visualization that was designed for the general public. While interactive visualizations are increasingly available for public use, we still know little about how the general public discovers what they can do with these visualizations and what interactions are available. Developing a better understanding of this discovery process can help inform the design of visualizations for the general public, which in turn can help make data more accessible. To unpack this problem, we conducted a lab study in which participants were free to use their own methods to discover the functionality of a connected set of interactive visualizations of public energy data. We collected eye movement data and interaction logs as well as video and audio recordings. By analyzing this combined data, we extract exploration strategies that the participants employed to discover the functionality in these interactive visualizations. These exploration strategies illuminate possible design directions for improving the discoverability of a visualization's functionality.",dir:"content/output/publications",base:"tvcg-2019-blascheck.json",ext:".json",sourceBase:"tvcg-2019-blascheck.yaml",sourceExt:".yaml"}},xjNh:function(e){e.exports={path:"./static",name:"static",children:[{path:"static/css",name:"css",children:[],size:0,type:"directory"},{path:"static/images",name:"images",children:[{path:"static/images/cover-2.jpg",name:"cover-2.jpg",size:146836,extension:".jpg",type:"file"},{path:"static/images/cover.jpg",name:"cover.jpg",size:191780,extension:".jpg",type:"file"},{path:"static/images/facility",name:"facility",children:[{path:"static/images/facility/andonstar.jpg",name:"andonstar.jpg",size:144590,extension:".jpg",type:"file"},{path:"static/images/facility/azure-kinect.jpg",name:"azure-kinect.jpg",size:114922,extension:".jpg",type:"file"},{path:"static/images/facility/band-saw.jpg",name:"band-saw.jpg",size:77736,extension:".jpg",type:"file"},{path:"static/images/facility/bantam.jpg",name:"bantam.jpg",size:31689,extension:".jpg",type:"file"},{path:"static/images/facility/baxter.jpg",name:"baxter.jpg",size:55056,extension:".jpg",type:"file"},{path:"static/images/facility/black-decker.jpg",name:"black-decker.jpg",size:130755,extension:".jpg",type:"file"},{path:"static/images/facility/brother-2.jpg",name:"brother-2.jpg",size:18415,extension:".jpg",type:"file"},{path:"static/images/facility/brother.jpg",name:"brother.jpg",size:9146,extension:".jpg",type:"file"},{path:"static/images/facility/camera-slider.jpg",name:"camera-slider.jpg",size:104608,extension:".jpg",type:"file"},{path:"static/images/facility/cetus.jpg",name:"cetus.jpg",size:148855,extension:".jpg",type:"file"},{path:"static/images/facility/conman.jpg",name:"conman.jpg",size:15436,extension:".jpg",type:"file"},{path:"static/images/facility/dji.jpg",name:"dji.jpg",size:66755,extension:".jpg",type:"file"},{path:"static/images/facility/drill-press.jpg",name:"drill-press.jpg",size:85319,extension:".jpg",type:"file"},{path:"static/images/facility/emart.jpg",name:"emart.jpg",size:104439,extension:".jpg",type:"file"},{path:"static/images/facility/epilog-2.jpg",name:"epilog-2.jpg",size:66806,extension:".jpg",type:"file"},{path:"static/images/facility/epilog.jpg",name:"epilog.jpg",size:23450,extension:".jpg",type:"file"},{path:"static/images/facility/eventek.jpg",name:"eventek.jpg",size:99421,extension:".jpg",type:"file"},{path:"static/images/facility/form-3.jpg",name:"form-3.jpg",size:30803,extension:".jpg",type:"file"},{path:"static/images/facility/form-cure.jpg",name:"form-cure.jpg",size:30223,extension:".jpg",type:"file"},{path:"static/images/facility/form-wash.jpg",name:"form-wash.jpg",size:224899,extension:".jpg",type:"file"},{path:"static/images/facility/hitachi.jpg",name:"hitachi.jpg",size:34856,extension:".jpg",type:"file"},{path:"static/images/facility/hololens-1.jpg",name:"hololens-1.jpg",size:105799,extension:".jpg",type:"file"},{path:"static/images/facility/hololens-2.jpg",name:"hololens-2.jpg",size:47777,extension:".jpg",type:"file"},{path:"static/images/facility/hpusn.jpg",name:"hpusn.jpg",size:65581,extension:".jpg",type:"file"},{path:"static/images/facility/limostudio.jpg",name:"limostudio.jpg",size:37962,extension:".jpg",type:"file"},{path:"static/images/facility/mayku.jpg",name:"mayku.jpg",size:122996,extension:".jpg",type:"file"},{path:"static/images/facility/newer.jpg",name:"newer.jpg",size:34501,extension:".jpg",type:"file"},{path:"static/images/facility/oculus-quest.jpg",name:"oculus-quest.jpg",size:19656,extension:".jpg",type:"file"},{path:"static/images/facility/pfaff.jpg",name:"pfaff.jpg",size:57957,extension:".jpg",type:"file"},{path:"static/images/facility/silver-bullet.jpg",name:"silver-bullet.jpg",size:31696,extension:".jpg",type:"file"},{path:"static/images/facility/sony-a7.jpg",name:"sony-a7.jpg",size:512403,extension:".jpg",type:"file"},{path:"static/images/facility/t962.jpg",name:"t962.jpg",size:111818,extension:".jpg",type:"file"},{path:"static/images/facility/tamron.jpg",name:"tamron.jpg",size:185101,extension:".jpg",type:"file"},{path:"static/images/facility/toio.jpg",name:"toio.jpg",size:70321,extension:".jpg",type:"file"},{path:"static/images/facility/ultimaker.jpg",name:"ultimaker.jpg",size:28869,extension:".jpg",type:"file"},{path:"static/images/facility/vicon-2.jpg",name:"vicon-2.jpg",size:41724,extension:".jpg",type:"file"},{path:"static/images/facility/vicon.jpg",name:"vicon.jpg",size:7377,extension:".jpg",type:"file"},{path:"static/images/facility/voltera.jpg",name:"voltera.jpg",size:78628,extension:".jpg",type:"file"},{path:"static/images/facility/wellner.jpg",name:"wellner.jpg",size:38451,extension:".jpg",type:"file"},{path:"static/images/facility/x-carve-2.jpg",name:"x-carve-2.jpg",size:117958,extension:".jpg",type:"file"},{path:"static/images/facility/x-carve.jpg",name:"x-carve.jpg",size:27540,extension:".jpg",type:"file"}],size:3362294,type:"directory"},{path:"static/images/labs",name:"labs",children:[{path:"static/images/labs/c3-lab.png",name:"c3-lab.png",size:69551,extension:".png",type:"file"},{path:"static/images/labs/curiosity.png",name:"curiosity.png",size:9873,extension:".png",type:"file"},{path:"static/images/labs/dataexperience.png",name:"dataexperience.png",size:17064,extension:".png",type:"file"},{path:"static/images/labs/grouplab.png",name:"grouplab.png",size:27402,extension:".png",type:"file"},{path:"static/images/labs/innovis.png",name:"innovis.png",size:5951,extension:".png",type:"file"},{path:"static/images/labs/ricelab.png",name:"ricelab.png",size:14759,extension:".png",type:"file"},{path:"static/images/labs/suzuki.png",name:"suzuki.png",size:58625,extension:".png",type:"file"},{path:"static/images/labs/utouch.png",name:"utouch.png",size:7454,extension:".png",type:"file"}],size:210679,type:"directory"},{path:"static/images/logo-1.png",name:"logo-1.png",size:12856,extension:".png",type:"file"},{path:"static/images/logo-2.png",name:"logo-2.png",size:25148,extension:".png",type:"file"},{path:"static/images/logo-3.png",name:"logo-3.png",size:56423,extension:".png",type:"file"},{path:"static/images/logo-4.png",name:"logo-4.png",size:35184,extension:".png",type:"file"},{path:"static/images/logo-5.png",name:"logo-5.png",size:25573,extension:".png",type:"file"},{path:"static/images/logo-6.png",name:"logo-6.png",size:40570,extension:".png",type:"file"},{path:"static/images/logo-white.png",name:"logo-white.png",size:33500,extension:".png",type:"file"},{path:"static/images/news",name:"news",children:[{path:"static/images/news/chi-2020.jpg",name:"chi-2020.jpg",size:7724,extension:".jpg",type:"file"},{path:"static/images/news/imwut.jpg",name:"imwut.jpg",size:5278,extension:".jpg",type:"file"},{path:"static/images/news/iros-2020.jpg",name:"iros-2020.jpg",size:52149,extension:".jpg",type:"file"},{path:"static/images/news/uist-2020.jpg",name:"uist-2020.jpg",size:14485,extension:".jpg",type:"file"}],size:79636,type:"directory"},{path:"static/images/people",name:"people",children:[{path:"static/images/people/adnan-karim.jpg",name:"adnan-karim.jpg",size:3301317,extension:".jpg",type:"file"},{path:"static/images/people/anthony-tang-1.jpg",name:"anthony-tang-1.jpg",size:64243,extension:".jpg",type:"file"},{path:"static/images/people/anthony-tang.jpg",name:"anthony-tang.jpg",size:31082,extension:".jpg",type:"file"},{path:"static/images/people/april-zhang.jpg",name:"april-zhang.jpg",size:19233,extension:".jpg",type:"file"},{path:"static/images/people/ashratuz-zavin-asha.jpg",name:"ashratuz-zavin-asha.jpg",size:10611,extension:".jpg",type:"file"},{path:"static/images/people/bon-adriel-aseniero.jpg",name:"bon-adriel-aseniero.jpg",size:161222,extension:".jpg",type:"file"},{path:"static/images/people/brennan-jones.jpg",name:"brennan-jones.jpg",size:791500,extension:".jpg",type:"file"},{path:"static/images/people/carmen-hull.jpg",name:"carmen-hull.jpg",size:4063,extension:".jpg",type:"file"},{path:"static/images/people/christopher-rodriguez.jpg",name:"christopher-rodriguez.jpg",size:13496,extension:".jpg",type:"file"},{path:"static/images/people/christopher-smith.jpg",name:"christopher-smith.jpg",size:23212,extension:".jpg",type:"file"},{path:"static/images/people/colin-auyeung.jpg",name:"colin-auyeung.jpg",size:4198802,extension:".jpg",type:"file"},{path:"static/images/people/darcy-norman.jpg",name:"darcy-norman.jpg",size:9520,extension:".jpg",type:"file"},{path:"static/images/people/david-ledo-2.jpg",name:"david-ledo-2.jpg",size:434237,extension:".jpg",type:"file"},{path:"static/images/people/david-ledo.jpg",name:"david-ledo.jpg",size:12664,extension:".jpg",type:"file"},{path:"static/images/people/ehud-sharlin.jpg",name:"ehud-sharlin.jpg",size:13398,extension:".jpg",type:"file"},{path:"static/images/people/georgina-freeman.jpg",name:"georgina-freeman.jpg",size:270017,extension:".jpg",type:"file"},{path:"static/images/people/grace-ferguson.jpg",name:"grace-ferguson.jpg",size:117840,extension:".jpg",type:"file"},{path:"static/images/people/harrison-chen.jpg",name:"harrison-chen.jpg",size:343007,extension:".jpg",type:"file"},{path:"static/images/people/helen-ai-he.jpg",name:"helen-ai-he.jpg",size:11430,extension:".jpg",type:"file"},{path:"static/images/people/jessi-stark.jpg",name:"jessi-stark.jpg",size:65755,extension:".jpg",type:"file"},{path:"static/images/people/karthik-mahadevan.jpg",name:"karthik-mahadevan.jpg",size:8045,extension:".jpg",type:"file"},{path:"static/images/people/kathryn-blair.jpg",name:"kathryn-blair.jpg",size:95099,extension:".jpg",type:"file"},{path:"static/images/people/kaynen-mitchell.jpg",name:"kaynen-mitchell.jpg",size:30841,extension:".jpg",type:"file"},{path:"static/images/people/kendra-wannamaker.jpg",name:"kendra-wannamaker.jpg",size:93930,extension:".jpg",type:"file"},{path:"static/images/people/kurtis-danyluk.jpg",name:"kurtis-danyluk.jpg",size:20604,extension:".jpg",type:"file"},{path:"static/images/people/lora-oehlberg.jpg",name:"lora-oehlberg.jpg",size:9123,extension:".jpg",type:"file"},{path:"static/images/people/manjot-khangura.jpg",name:"manjot-khangura.jpg",size:10607,extension:".jpg",type:"file"},{path:"static/images/people/manuel-rodriguez.jpg",name:"manuel-rodriguez.jpg",size:6652,extension:".jpg",type:"file"},{path:"static/images/people/marcus-friedel.jpg",name:"marcus-friedel.jpg",size:28755,extension:".jpg",type:"file"},{path:"static/images/people/martin-feick.jpg",name:"martin-feick.jpg",size:55078,extension:".jpg",type:"file"},{path:"static/images/people/michael-hung.jpg",name:"michael-hung.jpg",size:202447,extension:".jpg",type:"file"},{path:"static/images/people/nathalie-bressa.jpg",name:"nathalie-bressa.jpg",size:436534,extension:".jpg",type:"file"},{path:"static/images/people/neil-chulpongsatorn.jpg",name:"neil-chulpongsatorn.jpg",size:92054,extension:".jpg",type:"file"},{path:"static/images/people/nicolai-marquardt.jpg",name:"nicolai-marquardt.jpg",size:6739,extension:".jpg",type:"file"},{path:"static/images/people/no-profile-1.jpg",name:"no-profile-1.jpg",size:3265,extension:".jpg",type:"file"},{path:"static/images/people/no-profile-2.jpg",name:"no-profile-2.jpg",size:5037,extension:".jpg",type:"file"},{path:"static/images/people/no-profile.jpg",name:"no-profile.jpg",size:2740,extension:".jpg",type:"file"},{path:"static/images/people/nour-hammad.jpg",name:"nour-hammad.jpg",size:70047,extension:".jpg",type:"file"},{path:"static/images/people/roberta-cabral-mota.jpg",name:"roberta-cabral-mota.jpg",size:81517,extension:".jpg",type:"file"},{path:"static/images/people/ryo-suzuki.jpg",name:"ryo-suzuki.jpg",size:59005,extension:".jpg",type:"file"},{path:"static/images/people/sabrina-lakhdhir.jpg",name:"sabrina-lakhdhir.jpg",size:139984,extension:".jpg",type:"file"},{path:"static/images/people/samin-farajian-2.jpg",name:"samin-farajian-2.jpg",size:49335,extension:".jpg",type:"file"},{path:"static/images/people/samin-farajian.jpg",name:"samin-farajian.jpg",size:28010,extension:".jpg",type:"file"},{path:"static/images/people/sasha-ivanov.jpg",name:"sasha-ivanov.jpg",size:9585,extension:".jpg",type:"file"},{path:"static/images/people/saul-greenberg.jpg",name:"saul-greenberg.jpg",size:22868,extension:".jpg",type:"file"},{path:"static/images/people/sheelagh-carpendale.jpg",name:"sheelagh-carpendale.jpg",size:173802,extension:".jpg",type:"file"},{path:"static/images/people/shivesh-jadon.jpg",name:"shivesh-jadon.jpg",size:64970,extension:".jpg",type:"file"},{path:"static/images/people/soren-knudsen.jpg",name:"soren-knudsen.jpg",size:37313,extension:".jpg",type:"file"},{path:"static/images/people/sowmya-somanath.jpg",name:"sowmya-somanath.jpg",size:43231,extension:".jpg",type:"file"},{path:"static/images/people/sydney-pratte.jpg",name:"sydney-pratte.jpg",size:24127,extension:".jpg",type:"file"},{path:"static/images/people/teddy-seyed.jpg",name:"teddy-seyed.jpg",size:158626,extension:".jpg",type:"file"},{path:"static/images/people/terrance-mok.jpg",name:"terrance-mok.jpg",size:25613,extension:".jpg",type:"file"},{path:"static/images/people/tian-xia.jpg",name:"tian-xia.jpg",size:107076,extension:".jpg",type:"file"},{path:"static/images/people/tim-au-yeung.jpg",name:"tim-au-yeung.jpg",size:63006,extension:".jpg",type:"file"},{path:"static/images/people/wesley-willett.jpg",name:"wesley-willett.jpg",size:6466,extension:".jpg",type:"file"},{path:"static/images/people/zachary-mckendrick.jpg",name:"zachary-mckendrick.jpg",size:19509,extension:".jpg",type:"file"}],size:12188289,type:"directory"},{path:"static/images/publications",name:"publications",children:[{path:"static/images/publications/cover",name:"cover",children:[{path:"static/images/publications/cover/TVCG-2016-lopez.jpg",name:"TVCG-2016-lopez.jpg",size:22275,extension:".jpg",type:"file"},{path:"static/images/publications/cover/assets-2017-suzuki.jpg",name:"assets-2017-suzuki.jpg",size:629289,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2015-aseniero.jpg",name:"chi-2015-aseniero.jpg",size:70348,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2015-jones.jpg",name:"chi-2015-jones.jpg",size:11817,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2015-oehlberg.jpg",name:"chi-2015-oehlberg.jpg",size:30020,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2015-willett.jpg",name:"chi-2015-willett.jpg",size:21011,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2017-aoki.jpg",name:"chi-2017-aoki.jpg",size:24579,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2017-hull.jpg",name:"chi-2017-hull.jpg",size:163722,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2017-ledo.jpg",name:"chi-2017-ledo.jpg",size:109678,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2017-somanath.jpg",name:"chi-2017-somanath.jpg",size:6434,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2018-dillman.jpg",name:"chi-2018-dillman.jpg",size:46867,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2018-feick-2.jpg",name:"chi-2018-feick-2.jpg",size:110058,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2018-feick.jpg",name:"chi-2018-feick.jpg",size:101834,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2018-heshmat.jpg",name:"chi-2018-heshmat.jpg",size:82852,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2018-ledo.jpg",name:"chi-2018-ledo.jpg",size:22239,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2018-mahadevan.jpg",name:"chi-2018-mahadevan.jpg",size:149679,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2018-neustaedter.jpg",name:"chi-2018-neustaedter.jpg",size:80074,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2018-oh.jpg",name:"chi-2018-oh.jpg",size:39816,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2018-suzuki.jpg",name:"chi-2018-suzuki.jpg",size:11827,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2018-wuertz.jpg",name:"chi-2018-wuertz.jpg",size:142844,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2019-danyluk.jpg",name:"chi-2019-danyluk.jpg",size:65671,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2020-anjani.jpg",name:"chi-2020-anjani.jpg",size:132575,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2020-goffin.jpg",name:"chi-2020-goffin.jpg",size:54945,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2020-hou.jpg",name:"chi-2020-hou.jpg",size:61276,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2020-suzuki.jpg",name:"chi-2020-suzuki.jpg",size:84892,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2021-danyluk.jpg",name:"chi-2021-danyluk.jpg",size:15877,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2021-ens.jpg",name:"chi-2021-ens.jpg",size:61704,extension:".jpg",type:"file"},{path:"static/images/publications/cover/chi-2021-hammad.jpg",name:"chi-2021-hammad.jpg",size:332049,extension:".jpg",type:"file"},{path:"static/images/publications/cover/cnc-2019-hammad.jpg",name:"cnc-2019-hammad.jpg",size:435009,extension:".jpg",type:"file"},{path:"static/images/publications/cover/cupum-2021-rout.jpg",name:"cupum-2021-rout.jpg",size:91936,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2016-jones.jpg",name:"dis-2016-jones.jpg",size:16683,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2017-mok.jpg",name:"dis-2017-mok.jpg",size:160130,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2018-mikalauskas.jpg",name:"dis-2018-mikalauskas.jpg",size:163764,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2018-pham.jpg",name:"dis-2018-pham.jpg",size:27945,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2018-ta-2.jpg",name:"dis-2018-ta-2.jpg",size:77431,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2018-ta.jpg",name:"dis-2018-ta.jpg",size:107614,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2019-bressa.jpg",name:"dis-2019-bressa.jpg",size:26826,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2019-ledo.jpg",name:"dis-2019-ledo.jpg",size:224383,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2019-mahadevan.jpg",name:"dis-2019-mahadevan.jpg",size:1093698,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2019-nakayama-2.jpg",name:"dis-2019-nakayama-2.jpg",size:117709,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2019-nakayama.jpg",name:"dis-2019-nakayama.jpg",size:33276,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2019-seyed.jpg",name:"dis-2019-seyed.jpg",size:110477,extension:".jpg",type:"file"},{path:"static/images/publications/cover/dis-2021-wannamaker.jpg",name:"dis-2021-wannamaker.jpg",size:30999,extension:".jpg",type:"file"},{path:"static/images/publications/cover/gi-2021-mactavish.jpg",name:"gi-2021-mactavish.jpg",size:58685,extension:".jpg",type:"file"},{path:"static/images/publications/cover/hri-2018-feick.jpg",name:"hri-2018-feick.jpg",size:257601,extension:".jpg",type:"file"},{path:"static/images/publications/cover/imwut-2020-wang-2.jpg",name:"imwut-2020-wang-2.jpg",size:100584,extension:".jpg",type:"file"},{path:"static/images/publications/cover/imwut-2020-wang.jpg",name:"imwut-2020-wang.jpg",size:30393,extension:".jpg",type:"file"},{path:"static/images/publications/cover/iros-2020-hedayati.jpg",name:"iros-2020-hedayati.jpg",size:201132,extension:".jpg",type:"file"},{path:"static/images/publications/cover/mobilehci-2015-ledo.jpg",name:"mobilehci-2015-ledo.jpg",size:27865,extension:".jpg",type:"file"},{path:"static/images/publications/cover/mobilehci-2019-hung.jpg",name:"mobilehci-2019-hung.jpg",size:19530,extension:".jpg",type:"file"},{path:"static/images/publications/cover/nime-2020-ko.jpg",name:"nime-2020-ko.jpg",size:64497,extension:".jpg",type:"file"},{path:"static/images/publications/cover/sui-2017-li-2.jpg",name:"sui-2017-li-2.jpg",size:62810,extension:".jpg",type:"file"},{path:"static/images/publications/cover/sui-2017-li.jpg",name:"sui-2017-li.jpg",size:47231,extension:".jpg",type:"file"},{path:"static/images/publications/cover/tei-2016-somanath.jpg",name:"tei-2016-somanath.jpg",size:135052,extension:".jpg",type:"file"},{path:"static/images/publications/cover/tei-2019-mikalauskas.jpg",name:"tei-2019-mikalauskas.jpg",size:411686,extension:".jpg",type:"file"},{path:"static/images/publications/cover/tei-2019-tolley-2.jpg",name:"tei-2019-tolley-2.jpg",size:267136,extension:".jpg",type:"file"},{path:"static/images/publications/cover/tei-2019-tolley.jpg",name:"tei-2019-tolley.jpg",size:212698,extension:".jpg",type:"file"},{path:"static/images/publications/cover/tei-2019-wun.jpg",name:"tei-2019-wun.jpg",size:327008,extension:".jpg",type:"file"},{path:"static/images/publications/cover/tei-2020-suzuki-2.jpg",name:"tei-2020-suzuki-2.jpg",size:118606,extension:".jpg",type:"file"},{path:"static/images/publications/cover/tei-2020-suzuki.jpg",name:"tei-2020-suzuki.jpg",size:127496,extension:".jpg",type:"file"},{path:"static/images/publications/cover/tvcg-2017-goffin.jpg",name:"tvcg-2017-goffin.jpg",size:78249,extension:".jpg",type:"file"},{path:"static/images/publications/cover/tvcg-2017-willett.jpg",name:"tvcg-2017-willett.jpg",size:64192,extension:".jpg",type:"file"},{path:"static/images/publications/cover/tvcg-2019-blascheck.jpg",name:"tvcg-2019-blascheck.jpg",size:143284,extension:".jpg",type:"file"},{path:"static/images/publications/cover/tvcg-2019-walny.jpg",name:"tvcg-2019-walny.jpg",size:98163,extension:".jpg",type:"file"},{path:"static/images/publications/cover/uist-2018-suzuki-2.jpg",name:"uist-2018-suzuki-2.jpg",size:213980,extension:".jpg",type:"file"},{path:"static/images/publications/cover/uist-2018-suzuki.jpg",name:"uist-2018-suzuki.jpg",size:755902,extension:".jpg",type:"file"},{path:"static/images/publications/cover/uist-2019-suzuki-1.jpg",name:"uist-2019-suzuki-1.jpg",size:380412,extension:".jpg",type:"file"},{path:"static/images/publications/cover/uist-2019-suzuki-2.jpg",name:"uist-2019-suzuki-2.jpg",size:104561,extension:".jpg",type:"file"},{path:"static/images/publications/cover/uist-2019-suzuki.jpg",name:"uist-2019-suzuki.jpg",size:125986,extension:".jpg",type:"file"},{path:"static/images/publications/cover/uist-2020-suzuki.jpg",name:"uist-2020-suzuki.jpg",size:326140,extension:".jpg",type:"file"},{path:"static/images/publications/cover/uist-2020-yixian-2.jpg",name:"uist-2020-yixian-2.jpg",size:124407,extension:".jpg",type:"file"},{path:"static/images/publications/cover/uist-2020-yixian.jpg",name:"uist-2020-yixian.jpg",size:54328,extension:".jpg",type:"file"},{path:"static/images/publications/cover/uist-2021-suzuki-1.jpg",name:"uist-2021-suzuki-1.jpg",size:743218,extension:".jpg",type:"file"},{path:"static/images/publications/cover/uist-2021-suzuki-2.jpg",name:"uist-2021-suzuki-2.jpg",size:105440,extension:".jpg",type:"file"},{path:"static/images/publications/cover/uist-2021-suzuki.jpg",name:"uist-2021-suzuki.jpg",size:308865,extension:".jpg",type:"file"},{path:"static/images/publications/cover/vr-2019-satriadi.jpg",name:"vr-2019-satriadi.jpg",size:32702,extension:".jpg",type:"file"}],size:11533971,type:"directory"},{path:"static/images/publications/figures",name:"figures",children:[{path:"static/images/publications/figures/iros-2020-hedayati",name:"iros-2020-hedayati",children:[{path:"static/images/publications/figures/iros-2020-hedayati/assembly.jpg",name:"assembly.jpg",size:547418,extension:".jpg",type:"file"},{path:"static/images/publications/figures/iros-2020-hedayati/cover-1-2.jpg",name:"cover-1-2.jpg",size:652051,extension:".jpg",type:"file"},{path:"static/images/publications/figures/iros-2020-hedayati/cover-2-2.jpg",name:"cover-2-2.jpg",size:645294,extension:".jpg",type:"file"},{path:"static/images/publications/figures/iros-2020-hedayati/cover-5.jpg",name:"cover-5.jpg",size:414199,extension:".jpg",type:"file"},{path:"static/images/publications/figures/iros-2020-hedayati/design-space.jpg",name:"design-space.jpg",size:2747263,extension:".jpg",type:"file"},{path:"static/images/publications/figures/iros-2020-hedayati/drone.jpg",name:"drone.jpg",size:631516,extension:".jpg",type:"file"},{path:"static/images/publications/figures/iros-2020-hedayati/expand-1.jpg",name:"expand-1.jpg",size:850378,extension:".jpg",type:"file"},{path:"static/images/publications/figures/iros-2020-hedayati/expand-2.jpg",name:"expand-2.jpg",size:1046583,extension:".jpg",type:"file"},{path:"static/images/publications/figures/iros-2020-hedayati/expand-perspective.jpg",name:"expand-perspective.jpg",size:267486,extension:".jpg",type:"file"},{path:"static/images/publications/figures/iros-2020-hedayati/expand-top-v3.jpg",name:"expand-top-v3.jpg",size:230433,extension:".jpg",type:"file"},{path:"static/images/publications/figures/iros-2020-hedayati/mechanical.jpg",name:"mechanical.jpg",size:190470,extension:".jpg",type:"file"},{path:"static/images/publications/figures/iros-2020-hedayati/use-cases.jpg",name:"use-cases.jpg",size:330902,extension:".jpg",type:"file"}],size:8553993,type:"directory"},{path:"static/images/publications/figures/uist-2020-suzuki",name:"uist-2020-suzuki",children:[{path:"static/images/publications/figures/uist-2020-suzuki/RealitySketch-1.jpg",name:"RealitySketch-1.jpg",size:171605,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/RealitySketch-2-2.jpg",name:"RealitySketch-2-2.jpg",size:224100,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/applications-cycloid.jpg",name:"applications-cycloid.jpg",size:89887,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/applications-dino-2.jpg",name:"applications-dino-2.jpg",size:89989,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/applications-dino.jpg",name:"applications-dino.jpg",size:103552,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/applications-elephant.jpg",name:"applications-elephant.jpg",size:80521,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/applications-exercise.jpg",name:"applications-exercise.jpg",size:134628,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/applications-gear.jpg",name:"applications-gear.jpg",size:148625,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/applications-gravity.jpg",name:"applications-gravity.jpg",size:96270,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/applications-pulley.jpg",name:"applications-pulley.jpg",size:115208,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/applications-rehab.jpg",name:"applications-rehab.jpg",size:165410,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/applications-shark.jpg",name:"applications-shark.jpg",size:102931,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/applications-stretch.jpg",name:"applications-stretch.jpg",size:158933,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/applications-tree.jpg",name:"applications-tree.jpg",size:102073,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/cover-2.jpg",name:"cover-2.jpg",size:217496,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/design-space copy 2.jpg",name:"design-space copy 2.jpg",size:438498,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/future-sketch.jpg",name:"future-sketch.jpg",size:987133,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/reality-sketch-applications.jpg",name:"reality-sketch-applications.jpg",size:525146,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/system-angle.jpg",name:"system-angle.jpg",size:120029,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/system-bind.jpg",name:"system-bind.jpg",size:126703,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/system-function.jpg",name:"system-function.jpg",size:117431,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/system-line.jpg",name:"system-line.jpg",size:124436,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/system-naming.jpg",name:"system-naming.jpg",size:128322,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/system-plot.jpg",name:"system-plot.jpg",size:122300,extension:".jpg",type:"file"},{path:"static/images/publications/figures/uist-2020-suzuki/system-tracking.jpg",name:"system-tracking.jpg",size:115087,extension:".jpg",type:"file"}],size:4806313,type:"directory"}],size:13360306,type:"directory"}],size:24894277,type:"directory"},{path:"static/images/seminar",name:"seminar",children:[{path:"static/images/seminar/alberto-de-salvatierra.jpg",name:"alberto-de-salvatierra.jpg",size:156899,extension:".jpg",type:"file"},{path:"static/images/seminar/alicia-nahmad-vazquez.jpg",name:"alicia-nahmad-vazquez.jpg",size:93235,extension:".jpg",type:"file"},{path:"static/images/seminar/benjamin-bach.jpg",name:"benjamin-bach.jpg",size:30684,extension:".jpg",type:"file"},{path:"static/images/seminar/charles-perin.jpg",name:"charles-perin.jpg",size:140391,extension:".jpg",type:"file"},{path:"static/images/seminar/james-young.jpg",name:"james-young.jpg",size:10208,extension:".jpg",type:"file"},{path:"static/images/seminar/jessica-cauchard.jpg",name:"jessica-cauchard.jpg",size:32796,extension:".jpg",type:"file"},{path:"static/images/seminar/ken-nakagaki.jpg",name:"ken-nakagaki.jpg",size:46038,extension:".jpg",type:"file"},{path:"static/images/seminar/nicolai-marquardt.jpg",name:"nicolai-marquardt.jpg",size:51550,extension:".jpg",type:"file"},{path:"static/images/seminar/rubaiat-habib.jpg",name:"rubaiat-habib.jpg",size:175051,extension:".jpg",type:"file"},{path:"static/images/seminar/stefanie-mueller.jpg",name:"stefanie-mueller.jpg",size:229864,extension:".jpg",type:"file"},{path:"static/images/seminar/xing-dong-yang.jpg",name:"xing-dong-yang.jpg",size:132975,extension:".jpg",type:"file"}],size:1099691,type:"directory"},{path:"static/images/space",name:"space",children:[{path:"static/images/space/UofC Science iLab1.jpg",name:"UofC Science iLab1.jpg",size:409786,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab10.jpg",name:"UofC Science iLab10.jpg",size:385466,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab11.jpg",name:"UofC Science iLab11.jpg",size:350574,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab12.jpg",name:"UofC Science iLab12.jpg",size:405447,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab13.jpg",name:"UofC Science iLab13.jpg",size:455381,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab14.jpg",name:"UofC Science iLab14.jpg",size:207016,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab15.jpg",name:"UofC Science iLab15.jpg",size:477584,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab16.jpg",name:"UofC Science iLab16.jpg",size:210646,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab17.jpg",name:"UofC Science iLab17.jpg",size:229114,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab18.jpg",name:"UofC Science iLab18.jpg",size:231163,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab19.jpg",name:"UofC Science iLab19.jpg",size:440738,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab2.jpg",name:"UofC Science iLab2.jpg",size:390743,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab20.jpg",name:"UofC Science iLab20.jpg",size:250405,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab21.jpg",name:"UofC Science iLab21.jpg",size:232037,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab22.jpg",name:"UofC Science iLab22.jpg",size:201359,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab23.jpg",name:"UofC Science iLab23.jpg",size:235615,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab24.jpg",name:"UofC Science iLab24.jpg",size:317130,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab25.jpg",name:"UofC Science iLab25.jpg",size:403853,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab26.jpg",name:"UofC Science iLab26.jpg",size:363797,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab27.jpg",name:"UofC Science iLab27.jpg",size:285808,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab28.jpg",name:"UofC Science iLab28.jpg",size:257052,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab29.jpg",name:"UofC Science iLab29.jpg",size:253065,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab3.jpg",name:"UofC Science iLab3.jpg",size:193580,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab30.jpg",name:"UofC Science iLab30.jpg",size:437889,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab31.jpg",name:"UofC Science iLab31.jpg",size:287346,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab32.jpg",name:"UofC Science iLab32.jpg",size:220811,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab33.jpg",name:"UofC Science iLab33.jpg",size:316287,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab34.jpg",name:"UofC Science iLab34.jpg",size:340612,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab35.jpg",name:"UofC Science iLab35.jpg",size:289404,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab36.jpg",name:"UofC Science iLab36.jpg",size:458069,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab37.jpg",name:"UofC Science iLab37.jpg",size:419310,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab38.jpg",name:"UofC Science iLab38.jpg",size:279740,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab39.jpg",name:"UofC Science iLab39.jpg",size:259512,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab4.jpg",name:"UofC Science iLab4.jpg",size:192329,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab40.jpg",name:"UofC Science iLab40.jpg",size:281255,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab41.jpg",name:"UofC Science iLab41.jpg",size:278338,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab42.jpg",name:"UofC Science iLab42.jpg",size:303578,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab43.jpg",name:"UofC Science iLab43.jpg",size:231721,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab44.jpg",name:"UofC Science iLab44.jpg",size:223021,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab45.jpg",name:"UofC Science iLab45.jpg",size:412828,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab46.jpg",name:"UofC Science iLab46.jpg",size:435832,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab47.jpg",name:"UofC Science iLab47.jpg",size:239924,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab48.jpg",name:"UofC Science iLab48.jpg",size:211053,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab5.jpg",name:"UofC Science iLab5.jpg",size:389587,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab6.jpg",name:"UofC Science iLab6.jpg",size:422808,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab7.jpg",name:"UofC Science iLab7.jpg",size:399845,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab8.jpg",name:"UofC Science iLab8.jpg",size:315405,extension:".jpg",type:"file"},{path:"static/images/space/UofC Science iLab9.jpg",name:"UofC Science iLab9.jpg",size:374852,extension:".jpg",type:"file"}],size:15208715,type:"directory"}],size:57611451,type:"directory"},{path:"static/publications",name:"publications",children:[],size:0,type:"directory"},{path:"static/videos",name:"videos",children:[],size:0,type:"directory"},{path:"static/videos-original",name:"videos-original",children:[],size:0,type:"directory"}],size:57611451,type:"directory"}},xvv9:function(e,t,i){i("cHUd")("Set")},yLu3:function(e,t,i){e.exports=i("VKFn")},yO3u:function(e){e.exports={date:"2018-04",title:"The Benefits and Challenges of Video Calling for Emergency Situations",authors:["Carman Neustaedter","Brennan Jones","Kenton O'Hara","Abigail Sellen"],series:"CHI 2018",doi:"https://doi.org/10.1145/3173574.3174231",keywords:"collaboration, situation awareness, emergency calling, call takers, mobile video calling, dispatchers",pages:13,award:"Honorable Mention",abstract:"In the coming years, emergency calling services in North America will begin to incorporate new modalities for reporting emergencies, including video-based calling. The challenge is that we know little of how video calling systems should be designed and what benefits or challenges video calling might bring. We conducted observations and contextual interviews within three emergency response call centres to investigate these points. We focused on the work practices of call takers and dispatchers. Results show that video calls could provide valuable contextual information about a situation and help to overcome call taker challenges with information ambiguity, location, deceit, and communication issues. Yet video calls have the potential to introduce issues around control, information overload, and privacy if systems are not designed well. These results point to the need to think about emergency video calling along a continuum of visual modalities ranging from audio calls accompanied with images or video clips to one-way video streams to two-way video streams where camera control and camera work need to be carefully designed.",dir:"content/output/publications",base:"chi-2018-neustaedter.json",ext:".json",sourceBase:"chi-2018-neustaedter.yaml",sourceExt:".yaml"}},"yYy+":function(e,t,i){"use strict";i.r(t);var a=i("pbKT"),n=i.n(a),o=i("0iUn"),s=i("sLSF"),r=i("Tit0"),c=i("MI3g"),l=i("a7VT"),u=i("q1tI"),p=i.n(u);i("IujW");function h(e){var t=function(){if("undefined"==typeof Reflect||!n.a)return!1;if(n.a.sham)return!1;if("function"==typeof Proxy)return!0;try{return Date.prototype.toString.call(n()(Date,[],function(){})),!0}catch(e){return!1}}();return function(){var i,a=Object(l.default)(e);if(t){var o=Object(l.default)(this).constructor;i=n()(a,arguments,o)}else i=a.apply(this,arguments);return Object(c.default)(this,i)}}var d=function(e){Object(r.default)(i,e);var t=h(i);function i(){return Object(o.default)(this,i),t.apply(this,arguments)}return Object(s.default)(i,[{key:"render",value:function(){return p.a.createElement("footer",null,p.a.createElement("div",{className:"ui center aligned container"},p.a.createElement("div",{className:"ui section divider"}),p.a.createElement("img",{style:{maxWidth:"180px",margin:"30px auto"},src:"/static/images/logo-6.png"}),p.a.createElement("div",{className:"content"},p.a.createElement("img",{style:{maxWidth:"200px",margin:"0px auto"},src:"/static/images/logo-4.png"}),p.a.createElement("div",{className:"sub header"},"Department of Computer Science"))))}}]),i}(p.a.Component);t.default=d},zRIs:function(e){e.exports={date:"2015-04",title:"Lightweight Relief Shearing for Enhanced Terrain Perception on Interactive Maps",authors:["Wesley Willett","Bernhard Jenny","Tobias Isenberg","Pierre Dragicevic"],series:"CHI 2015",doi:"https://doi.org/10.1145/2702123.2702172",keywords:"plan oblique relief, interaction, depth perception, terrain maps, relief shearing",pages:10,video:"https://www.youtube.com/watch?v=YW31lmzQzpc",abstract:"We explore interactive relief shearing, a set of non-intrusive, direct manipulation interactions that expose depth and shape information in terrain maps using ephemeral animations. Reading and interpreting topography and relief on terrain maps is an important aspect of map use, but extracting depth information from 2D maps is notoriously difficult. Modern mapping software attempts to alleviate this limitation by presenting digital terrain using 3D views. However, 3D views introduce occlusion, complicate distance estimations, and typically require more complex interactions. In contrast, our approach reveals depth information via shearing animations on 2D maps, and can be paired with existing interactions such as pan and zoom. We examine explicit, integrated, and hybrid interactions for triggering relief shearing and present a version that uses device tilt to control depth effects. Our evaluation shows that these interactive techniques improve depth perception when compared to standard 2D and perspective views.",dir:"content/output/publications",base:"chi-2015-willett.json",ext:".json",sourceBase:"chi-2015-willett.yaml",sourceExt:".yaml"}},zXhZ:function(e,t,i){var a=i("5K7Z"),n=i("93I4"),o=i("ZW5q");e.exports=function(e,t){if(a(e),n(t)&&t.constructor===e)return t;var i=o.f(e);return(0,i.resolve)(t),i.promise}}},[["mDBt","5d41","9da1"]]]);