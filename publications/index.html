<!DOCTYPE html><html><head><link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.css" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/css/lightbox.css" rel="stylesheet"/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link href="/static/css/style.css" rel="stylesheet"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox-plus-jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              $('.ui.sidebar')
                .sidebar('attach events', '.sidebar.icon')

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><meta charSet="utf-8" class="next-head"/><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><link rel="preload" href="/_next/static/6bjRBjipLrjz-covNxi6J/pages/page.js" as="script"/><link rel="preload" href="/_next/static/6bjRBjipLrjz-covNxi6J/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.98f1a4e4c94db9943918.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-22d58f57abab872d6e70.js" as="script"/></head><body><div id="__next"><div><title>Publications - Interactions Lab | University of Calgary HCI Group</title><div><div class="ui right vertical sidebar menu"><a class="item" href="/">Home</a><a class="item active" href="/publications">Publications</a><a class="item" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/news">News</a><a class="item" href="/location">Location</a></div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/"><b>UCalgary iLab</b></a></div><div class="right menu"><a class="item active" href="/publications">Publications</a><a class="item" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/news">News</a><a class="item" href="/location">Location</a><div class="toc item"><a href="/"><b>UCalgary iLab</b></a><i style="float:right" class="sidebar icon"></i></div></div></div></div><div class="ui stackable grid"><div class="eleven wide column centered"><div id="publications" class="category"><h1 class="ui horizontal divider header"><i class="file alternate outline icon"></i>Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="uist-2020-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2020</span><span class="ui big label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Rubaiat Habib Kazi</span> , <span>Li-Yi Wei</span> , <span>Stephen DiVerdi</span> , <span>Wilmot Li</span> , <span>Daniel Leithinger</span></p><p><div class="ui labels"><span class="ui label">augmented reality</span><span class="ui label">embedded data visualization</span><span class="ui label">real-time authoring</span><span class="ui label">sketching interfaces</span><span class="ui label">tangible interaction</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2020-yixian"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-yixian.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2020</span></p><p class="color" style="font-size:1.3em"><b>ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World</b></p><p><span>Yan Yixian</span> , <span>Kazuki Takashima</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>Takayuki Tanno</span> , <span>Kazuyuki Fujita</span> , <span>Yoshifumi Kitamura</span></p><p><div class="ui labels"><span class="ui label">encountered-type haptic devices</span><span class="ui label">immersive experience</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="iros-2020-hedayati"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/iros-2020-hedayati.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IROS 2020</span></p><p class="color" style="font-size:1.3em"><b>PufferBot: Actuated Expandable Structures for Aerial Robots</b></p><p><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><p></p></div></div><div class="publication ui vertical segment stackable grid" data-id="imwut-2020-wang"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/imwut-2020-wang.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IMWUT 2020</span></p><p class="color" style="font-size:1.3em"><b>AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</b></p><p><span>Xiyue Wang</span> , <span>Kazuki Takashima</span> , <span>Tomoaki Adachi</span> , <span>Patrick Finn</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <span>Yoshifumi Kitamura</span></p><p><div class="ui labels"><span class="ui label">well being</span><span class="ui label">toy blocks</span><span class="ui label">PTSD</span><span class="ui label">tangibles for health</span><span class="ui label">stress assessment</span><span class="ui label">play</span><span class="ui label">children</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Hooman Hedayati</span> , <span>Clement Zheng</span> , <span>James Bohn</span> , <span>Daniel Szafir</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D Gross</span> , <span>Daniel Leithinger</span></p><p><div class="ui labels"><span class="ui label">virtual reality</span><span class="ui label">room-scale haptics</span><span class="ui label">haptic interfaces</span><span class="ui label">swarm robots</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-goffin"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-goffin.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</b></p><p><span>Pascal Goffin</span> , <span>Tanja Blascheck</span> , <span>Petra Isenberg</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui labels"><span class="ui label">glyphs</span><span class="ui label">word-scale visualization</span><span class="ui label">information visualization</span><span class="ui label">interaction techniques</span><span class="ui label">text visualization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-anjani"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b></p><p><span>Laurensia Anjani</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Terrance Mok</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Wooi Boon Goh</span></p><p><div class="ui labels"><span class="ui label">video streams</span><span class="ui label">mukbang</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-hou"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b></p><p><span>Ming Hou</span> , <a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Karthik Mahadevan</span></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2020-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2020</span></p><p class="color" style="font-size:1.3em"><b>LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Ryosuke Nakayama</span> , <span>Dan Liu</span> , <span>Yasuaki Kakehi</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p><p><div class="ui labels"><span class="ui label">shape-changing interfaces</span><span class="ui label">inflatables</span><span class="ui label">large-scale interactions</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="mobilehci-2019-hung"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/mobilehci-2019-hung.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MobileHCI 2019</span></p><p class="color" style="font-size:1.3em"><b>WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</b></p><p><a href="/people/michael-hung"><img src="/static/images/people/michael-hung.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Michael Hung</span></a> , <a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui labels"><span class="ui label">smartwatch</span><span class="ui label">cross-device interaction</span><span class="ui label">pen interaction</span><span class="ui label">interaction techniques</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2019-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2019-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2019</span></p><p class="color" style="font-size:1.3em"><b>ShapeBots: Shape-changing Swarm Robots</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Clement Zheng</span> , <span>Yasuaki Kakehi</span> , <span>Tom Yeh</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p><p><div class="ui labels"><span class="ui label">swarm user interfaces</span><span class="ui label">shape-changing user interfaces</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2019-walny"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2019-walny.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2019</span></p><p class="color" style="font-size:1.3em"><b>Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff</b></p><p><span>Jagoda Walny</span> , <span>Christian Frisson</span> , <span>Mieka West</span> , <span>Doris Kosminsky</span> , <a href="/people/soren-knudsen"><img src="/static/images/people/soren-knudsen.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Søren Knudsen</span></a> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui labels"><span class="ui label">information visualization</span><span class="ui label">design handoff</span><span class="ui label">data mapping</span><span class="ui label">design process</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="cnc-2019-hammad"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/cnc-2019-hammad.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">C&amp;C 2019</span></p><p class="color" style="font-size:1.3em"><b>Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning</b></p><p><a href="/people/nour-hammad"><img src="/static/images/people/nour-hammad.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nour Hammad</span></a> , <span>Elaheh Sanoubari</span> , <span>Patrick Finn</span> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <span>James E. Young</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-bressa"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-bressa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span></p><p class="color" style="font-size:1.3em"><b>Sketching and Ideation Activities for Situated Visualization Design</b></p><p><a href="/people/nathalie-bressa"><img src="/static/images/people/nathalie-bressa.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nathalie Bressa</span></a> , <a href="/people/kendra-wannamaker"><img src="/static/images/people/kendra-wannamaker.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kendra Wannamaker</span></a> , <span>Henrik Korsgaard</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <span>Jo Vermeulen</span></p><p></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-mahadevan"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-mahadevan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span></p><p class="color" style="font-size:1.3em"><b>AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator</b></p><p><a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Karthik Mahadevan</span></a> , <span>Elaheh Sanoubari</span> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <span>James E. Young</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p><div class="ui labels"><span class="ui label">mixed traffic</span><span class="ui label">pedestrian simulator</span><span class="ui label">autonomous vehicle-pedestrian interaction</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-ledo"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span></p><p class="color" style="font-size:1.3em"><b>Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</b></p><p><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <span>Jo Vermeulen</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Saul Greenberg</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Sebastian Boring</span></p><p><div class="ui labels"><span class="ui label">smart objects</span><span class="ui label">mobile interfaces</span><span class="ui label">prototyping</span><span class="ui label">design tool</span><span class="ui label">interactive behaviour</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-seyed"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-seyed.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span><span class="ui big label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways</b></p><p><a href="/people/teddy-seyed"><img src="/static/images/people/teddy-seyed.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Teddy Seyed</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a></p><p><div class="ui labels"><span class="ui label">fashion</span><span class="ui label">haute couture</span><span class="ui label">e-textiles</span><span class="ui label">maker culture</span><span class="ui label">fashion-tech</span><span class="ui label">wearables</span><span class="ui label">avant-garde</span><span class="ui label">haute-tech couture</span><span class="ui label">modular</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-nakayama"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-nakayama.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span></p><p class="color" style="font-size:1.3em"><b>MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction</b></p><p><span>Ryosuke Nakayama</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Satoshi Nakamaru</span> , <span>Ryuma Niiyama</span> , <span>Yoshihiro Kawahara</span> , <span>Yasuaki Kakehi</span></p><p><div class="ui labels"><span class="ui label">shape-changing interfaces</span><span class="ui label">programming by demonstration</span><span class="ui label">soft robots</span><span class="ui label">pneumatic actuation</span><span class="ui label">tangible interactions</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2019-blascheck"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2019-blascheck.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2019</span></p><p class="color" style="font-size:1.3em"><b>Exploration Strategies for Discovery of Interactivity in Visualizations</b></p><p><span>Tanja Blascheck</span> , <span>Lindsay MacDonald Vermeulen</span> , <span>Jo Vermeulen</span> , <span>Charles Perin</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <span>Thomas Ertl</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a></p><p><div class="ui labels"><span class="ui label">discovery</span><span class="ui label">visualization</span><span class="ui label">open data</span><span class="ui label">evaluation</span><span class="ui label">eye tracking</span><span class="ui label">interaction logs</span><span class="ui label">think-aloud</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2019-danyluk"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2019-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2019</span></p><p class="color" style="font-size:1.3em"><b>Look-From Camera Control for 3D Terrain Maps</b></p><p><span>Kurtis Thorvald Danyluk</span> , <span>Bernhard Jenny</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui labels"><span class="ui label">terrain</span><span class="ui label">touch</span><span class="ui label">map interaction</span><span class="ui label">look-from camera control</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2019-mikalauskas"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-mikalauskas.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2019</span></p><p class="color" style="font-size:1.3em"><b>Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</b></p><p><span>Claire Mikalauskas</span> , <span>April Viczko</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui labels"><span class="ui label">props</span><span class="ui label">performer-controlled technology</span><span class="ui label">improvisational theatre</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2019-wun"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-wun.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2019</span></p><p class="color" style="font-size:1.3em"><b>You say Potato, I say Po-Data: Physical Template Tools for Authoring Visualizations</b></p><p><span>Tiffany Wun</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Miriam Sturdee</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a></p><p><div class="ui labels"><span class="ui label">potato</span><span class="ui label">tangible tools</span><span class="ui label">authoring visualizations</span><span class="ui label">block-printing</span><span class="ui label">physical template tools</span><span class="ui label">information visualization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2019-tolley"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-tolley.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2019</span></p><p class="color" style="font-size:1.3em"><b>WindyWall: Exploring Creative Wind Simulations</b></p><p><span>David Tolley</span> , <span>Thi Ngoc Tram Nguyen</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>Nimesha Ranasinghe</span> , <span>Kensaku Kawauchi</span> , <span>Ching-Chiuan Yen</span></p><p><div class="ui labels"><span class="ui label">tactile/haptic interaction</span><span class="ui label">multimodal interaction</span><span class="ui label">novel actuators/displays</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="vr-2019-satriadi"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/vr-2019-satriadi.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IEEE VR 2019</span></p><p class="color" style="font-size:1.3em"><b>Augmented Reality Map Navigation with Freehand Gestures</b></p><p><span>Kadek Ananta Satriadi</span> , <span>Barrett Ens</span> , <span>Maxime Cordeil</span> , <span>Bernhard Jenny</span> , <span>Tobias Czauderna</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui labels"><span class="ui label">augmented reality</span><span class="ui label">gesture recognition</span><span class="ui label">human computer interaction</span><span class="ui label">interactive devices</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2018-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2018-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2018</span></p><p class="color" style="font-size:1.3em"><b>Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Junichi Yamaoka</span> , <span>Daniel Leithinger</span> , <span>Tom Yeh</span> , <span>Mark D. Gross</span> , <span>Yoshihiro Kawahara</span> , <span>Yasuaki Kakehi</span></p><p><div class="ui labels"><span class="ui label">digital materials</span><span class="ui label">dynamic 3D printing</span><span class="ui label">shape displays</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2018-mikalauskas"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2018-mikalauskas.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2018</span></p><p class="color" style="font-size:1.3em"><b>Improvising with an Audience-Controlled Robot Performer</b></p><p><span>Claire Mikalauskas</span> , <span>Tiffany Wun</span> , <span>Kevin Ta</span> , <span>Joshua Horacsek</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui labels"><span class="ui label">human-robot interaction</span><span class="ui label">improvised theatre</span><span class="ui label">creativity-support tools</span><span class="ui label">crowdsourcing</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2018-pham"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2018-pham.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2018</span></p><p class="color" style="font-size:1.3em"><b>Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design</b></p><p><span>Tran Pham</span> , <span>Jo Vermeulen</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>Lindsay MacDonald Vermeulen</span></p><p><div class="ui labels"><span class="ui label">augmented reality</span><span class="ui label">gestures</span><span class="ui label">gesture elicitation</span><span class="ui label">hololens</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2018-ta"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2018-ta.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2018</span></p><p class="color" style="font-size:1.3em"><b>Bod-IDE: An Augmented Reality Sandbox for eFashion Garments</b></p><p><span>Kevin Ta</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui labels"><span class="ui label">augmented reality</span><span class="ui label">electronic fashion</span><span class="ui label">creativity support tool</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2016-somanath"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2016-somanath.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2016</span></p><p class="color" style="font-size:1.3em"><b>Engaging &#x27;At-Risk&#x27; Students through Maker Culture Activities</b></p><p><a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <span>Laura Morrison</span> , <span>Janette Hughes</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <span>Mario Costa Sousa</span></p><p><div class="ui labels"><span class="ui label">DIY</span><span class="ui label">&#x27;at-risk&#x27; students</span><span class="ui label">maker culture</span><span class="ui label">education</span><span class="ui label">young learners</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-oh"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-oh.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices</b></p><p><span>Hyunjoo Oh</span> , <span>Tung D. Ta</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Mark D. Gross</span> , <span>Yoshihiro Kawahara</span> , <span>Lining Yao</span></p><p><div class="ui labels"><span class="ui label">paper electronics</span><span class="ui label">3d sculpting</span><span class="ui label">paper craft</span><span class="ui label">fabrication techniques</span><span class="ui label">prototyping</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Jun Kato</span> , <span>Mark D. Gross</span> , <span>Tom Yeh</span></p><p><div class="ui labels"><span class="ui label">direct manipulation</span><span class="ui label">tangible programming</span><span class="ui label">swarm user interfaces</span><span class="ui label">programming by demonstration</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-dillman"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-dillman.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</b></p><p><span>Kody R. Dillman</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Terrance Mok</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Alex Mitchell</span></p><p><div class="ui labels"><span class="ui label">game design</span><span class="ui label">guidance</span><span class="ui label">interaction cues</span><span class="ui label">augmented reality</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-feick"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span><span class="ui big label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</b></p><p><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Martin Feick</span></a> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p><div class="ui labels"><span class="ui label">cscw</span><span class="ui label">remote collaboration</span><span class="ui label">object-focused collaboration</span><span class="ui label">physical telepresence</span><span class="ui label">collaborative physical tasks</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-ledo"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>Evaluation Strategies for HCI Toolkit Research</b></p><p><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <span>Steven Houben</span> , <span>Jo Vermeulen</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nicolai Marquardt</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Saul Greenberg</span></a></p><p><div class="ui labels"><span class="ui label">user interfaces</span><span class="ui label">design</span><span class="ui label">evaluation</span><span class="ui label">prototyping</span><span class="ui label">toolkits</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-mahadevan"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-mahadevan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction</b></p><p><a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Karthik Mahadevan</span></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p><div class="ui labels"><span class="ui label">autonomous vehicle-pedestrian interaction</span><span class="ui label">perceived awareness and intent in autonomous vehicles</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-heshmat"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-heshmat.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing</b></p><p><span>Yasamin Heshmat</span> , <a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Brennan Jones</span></a> , <span>Xiaoxuan Xiong</span> , <span>Carman Neustaedter</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>Bernhard E. Riecke</span> , <span>Lillian Yang</span></p><p><div class="ui labels"><span class="ui label">video communication</span><span class="ui label">telepresence robots</span><span class="ui label">leisure activities</span><span class="ui label">social presence</span><span class="ui label">geocaching</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-neustaedter"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-neustaedter.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span><span class="ui big label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>The Benefits and Challenges of Video Calling for Emergency Situations</b></p><p><span>Carman Neustaedter</span> , <a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Brennan Jones</span></a> , <span>Kenton O&#x27;Hara</span> , <span>Abigail Sellen</span></p><p><div class="ui labels"><span class="ui label">collaboration</span><span class="ui label">situation awareness</span><span class="ui label">emergency calling</span><span class="ui label">call takers</span><span class="ui label">mobile video calling</span><span class="ui label">dispatchers</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-wuertz"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-wuertz.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>A Design Framework for Awareness Cues in Distributed Multiplayer Games</b></p><p><span>Jason Wuertz</span> , <span>Sultan A. Alharthi</span> , <span>William A. Hamilton</span> , <span>Scott Bateman</span> , <span>Carl Gutwin</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>Zachary O. Toups</span> , <span>Jessica Hammer</span></p><p><div class="ui labels"><span class="ui label">workspace awareness</span><span class="ui label">situation awareness</span><span class="ui label">game design</span><span class="ui label">distributed multiplayer games</span><span class="ui label">awareness cues</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="hri-2018-feick"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">HRI 2018</span></p><p class="color" style="font-size:1.3em"><b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b></p><p><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Martin Feick</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>André Miede</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p><div class="ui labels"><span class="ui label">movement trajectory &amp; velocity</span><span class="ui label">remote collaboration</span><span class="ui label">robot surrogate</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="sui-2017-li"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/sui-2017-li.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">SUI 2017</span></p><p class="color" style="font-size:1.3em"><b>Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models</b></p><p><span>Nico Li</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <span>Mario Costa Sousa</span></p><p><div class="ui labels"><span class="ui label">terrain visualization</span><span class="ui label">geospatial visualization</span><span class="ui label">dynamic viewshed</span><span class="ui label">topographic maps</span><span class="ui label">tangible user interfaces</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="assets-2017-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/assets-2017-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">ASSETS 2020</span></p><p class="color" style="font-size:1.3em"><b>FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Abigale Stangl</span> , <span>Mark D. Gross</span> , <span>Tom Yeh</span></p><p><div class="ui labels"><span class="ui label">visual impairment</span><span class="ui label">dynamic tactile markers</span><span class="ui label">tangible interfaces</span><span class="ui label">interactive tactile graphics</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2017-mok"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2017-mok.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2017</span></p><p class="color" style="font-size:1.3em"><b>Critiquing Physical Prototypes for a Remote Audience</b></p><p><a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Terrance Mok</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui labels"><span class="ui label">design review</span><span class="ui label">prototype critique</span><span class="ui label">remote collaboration</span><span class="ui label">material experience</span><span class="ui label">open hardware</span><span class="ui label">video conferencing</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-aoki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2017-aoki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>Environmental Protection and Agency: Motivations, Capacity, and Goals in Participatory Sensing</b></p><p><span>Paul Aoki</span> , <span>Allison Woodruff</span> , <span>Baladitya Yellapragada</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui labels"><span class="ui label">citizen science</span><span class="ui label">environmental sensing</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-hull"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2017-hull.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>Building with Data: Architectural Models as Inspiration for Data Physicalization</b></p><p><a href="/people/carmen-hull"><img src="/static/images/people/carmen-hull.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Carmen Hull</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui labels"><span class="ui label">design process</span><span class="ui label">architectural models</span><span class="ui label">data physicalization</span><span class="ui label">embodied interaction</span><span class="ui label">data visualization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-ledo"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2017-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices</b></p><p><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <span>Fraser Anderson</span> , <span>Ryan Schmidt</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Saul Greenberg</span></a> , <span>Tovi Grossman</span></p><p><div class="ui labels"><span class="ui label">fabrication</span><span class="ui label">3d printing</span><span class="ui label">smart objects</span><span class="ui label">rapid prototyping</span><span class="ui label">toolkits</span><span class="ui label">prototyping tool</span><span class="ui label">interaction design</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-somanath"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2017-somanath.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>&#x27;Maker&#x27; within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India</b></p><p><a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Janette Hughes</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <span>Mario Costa Sousa</span></p><p><div class="ui labels"><span class="ui label">India</span><span class="ui label">HCI4D</span><span class="ui label">physical computing</span><span class="ui label">DIY</span><span class="ui label">young learners</span><span class="ui label">maker culture</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2017-goffin"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2017-goffin.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2017</span></p><p class="color" style="font-size:1.3em"><b>An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents</b></p><p><span>Pascal Goffin</span> , <span>Jeremy Boy</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <span>Petra Isenberg</span></p><p><div class="ui labels"><span class="ui label">word-scale visualization</span><span class="ui label">word-scale graphic</span><span class="ui label">text visualization</span><span class="ui label">sparklines</span><span class="ui label">authoring tool</span><span class="ui label">information visualization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2017-willett"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2017-willett.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2017</span></p><p class="color" style="font-size:1.3em"><b>Embedded Data Representations</b></p><p><a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <span>Yvonne Jansen</span> , <span>Pierre Dragicevic</span></p><p><div class="ui labels"><span class="ui label">information visualization</span><span class="ui label">data physicalization</span><span class="ui label">ambient displays</span><span class="ui label">ubiquitous computing</span><span class="ui label">augmented reality</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2016-jones"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2016-jones.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2016</span></p><p class="color" style="font-size:1.3em"><b>Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</b></p><p><a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Brennan Jones</span></a> , <span>Kody Dillman</span> , <span>Richard Tang</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Carman Neustaedter</span> , <span>Scott Bateman</span></p><p><div class="ui labels"><span class="ui label">cscw</span><span class="ui label">telepresence</span><span class="ui label">video communication</span><span class="ui label">shared experiences</span><span class="ui label">teleoperation</span><span class="ui label">drones</span><span class="ui label">collaboration</span><span class="ui label">hri</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2016-lopez"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2016-lopez.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2016</span></p><p class="color" style="font-size:1.3em"><b>Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration</b></p><p><span>David Lopez</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Candemir Doger</span> , <span>Tobias Isenberg</span></p><p><div class="ui labels"><span class="ui label">visualization of 3D data</span><span class="ui label">human-computer interaction</span><span class="ui label">expert interaction</span><span class="ui label">direct-touch input</span><span class="ui label">mobile displays</span><span class="ui label">stereoscopic environments</span><span class="ui label">VR</span><span class="ui label">AR</span><span class="ui label">conceptual model of interaction</span><span class="ui label">interaction reference frame mapping</span><span class="ui label">observational study</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="mobilehci-2015-ledo"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/mobilehci-2015-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MobileHCI 2015</span></p><p class="color" style="font-size:1.3em"><b>Proxemic-Aware Controls: Designing Remote Controls for Ubiquitous Computing Ecologies</b></p><p><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Saul Greenberg</span></a> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nicolai Marquardt</span></a> , <span>Sebastian Boring</span></p><p><div class="ui labels"><span class="ui label">ubiquitous computing</span><span class="ui label">proxemic-interaction</span><span class="ui label">mobile interaction</span><span class="ui label">control of appliances</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-oehlberg"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-oehlberg.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Patterns of Physical Design Remixing in Online Maker Communities</b></p><p><a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <span>Wendy E. Mackay</span></p><p><div class="ui labels"><span class="ui label">customization</span><span class="ui label">maker communities</span><span class="ui label">user innovation</span><span class="ui label">collaboration</span><span class="ui label">hacking</span><span class="ui label">remixing</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-aseniero"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-aseniero.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Stratos: Using Visualization to Support Decisions in Strategic Software Release Planning</b></p><p><a href="/people/bon-adriel-aseniero"><img src="/static/images/people/bon-adriel-aseniero.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Bon Adriel Aseniero</span></a> , <span>Tiffany Wun</span> , <a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <span>Guenther Ruhe</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a></p><p><div class="ui labels"><span class="ui label">software engineering</span><span class="ui label">information visualization</span><span class="ui label">release planning</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-jones"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-jones.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Mechanics of Camera Work in Mobile Video Collaboration</b></p><p><a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Brennan Jones</span></a> , <span>Anna Witcraft</span> , <span>Scott Bateman</span> , <span>Carman Neustaedter</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a></p><p><div class="ui labels"><span class="ui label">video communication</span><span class="ui label">collaboration</span><span class="ui label">mobile computing</span><span class="ui label">handheld devices</span><span class="ui label">cscw</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-willett"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-willett.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Lightweight Relief Shearing for Enhanced Terrain Perception on Interactive Maps</b></p><p><a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <span>Bernhard Jenny</span> , <span>Tobias Isenberg</span> , <span>Pierre Dragicevic</span></p><p><div class="ui labels"><span class="ui label">plan oblique relief</span><span class="ui label">interaction</span><span class="ui label">depth perception</span><span class="ui label">terrain maps</span><span class="ui label">relief shearing</span></div></p></div></div></div><div id="publications-modal"><div id="uist-2020-suzuki" class="ui large modal"><div class="header"><a href="/publications/uist-2020-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2020-suzuki</a>  -  <a href="/publications/uist-2020-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2020-suzuki" target="_blank">RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Rubaiat Habib Kazi</span> , <span>Li-Yi Wei</span> , <span>Stephen DiVerdi</span> , <span>Wilmot Li</span> , <span>Daniel Leithinger</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/L0p-BNU9rXU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/L0p-BNU9rXU?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/L0p-BNU9rXU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.</p><div class="ui labels">Keywords:  <span class="ui large label">augmented reality</span><span class="ui large label">embedded data visualization</span><span class="ui large label">real-time authoring</span><span class="ui large label">sketching interfaces</span><span class="ui large label">tangible interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Li-Yi Wei<!-- -->, <!-- -->Stephen DiVerdi<!-- -->, <!-- -->Wilmot Li<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3379337.3415892" target="_blank">https://doi.org/10.1145/3379337.3415892</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2020-yixian" class="ui large modal"><div class="header"><a href="/publications/uist-2020-yixian" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2020-yixian</a>  -  <a href="/publications/uist-2020-yixian.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-yixian.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2020-yixian" target="_blank">ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World</a></h1><p class="meta"><span>Yan Yixian</span> , <span>Kazuki Takashima</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>Takayuki Tanno</span> , <span>Kazuyuki Fujita</span> , <span>Yoshifumi Kitamura</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/j2iSNDkBxAY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/j2iSNDkBxAY?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/j2iSNDkBxAY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls---autonomous robotic encounter-type haptic wall-shaped props---that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user&#x27;s movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of autonomous mobile reusable props, which we view as an important design direction for haptics in VR.</p><div class="ui labels">Keywords:  <span class="ui large label">encountered-type haptic devices</span><span class="ui large label">immersive experience</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Yan Yixian<!-- -->, <!-- -->Kazuki Takashima<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Takayuki Tanno<!-- -->, <!-- -->Kazuyuki Fujita<!-- -->, <!-- -->Yoshifumi Kitamura<!-- -->. <b>ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3379337.3415859" target="_blank">https://doi.org/10.1145/3379337.3415859</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="iros-2020-hedayati" class="ui large modal"><div class="header"><a href="/publications/iros-2020-hedayati" target="_blank"><i class="fas fa-link fa-fw"></i>iros-2020-hedayati</a>  -  <a href="/publications/iros-2020-hedayati.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IROS 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/iros-2020-hedayati.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/iros-2020-hedayati" target="_blank">PufferBot: Actuated Expandable Structures for Aerial Robots</a></h1><p class="meta"><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drone’s propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique charac- terizations which may be useful in different situations. Finally, we present three motivating scenarios in which PufferBot might be useful beyond existing static propeller guard structures.</p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hooman Hedayati<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Daniel Szafir<!-- -->. <b>PufferBot: Actuated Expandable Structures for Aerial Robots</b>. <i>In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS &#x27;20)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->6<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="imwut-2020-wang" class="ui large modal"><div class="header"><a href="/publications/imwut-2020-wang" target="_blank"><i class="fas fa-link fa-fw"></i>imwut-2020-wang</a>  -  <a href="/publications/imwut-2020-wang.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IMWUT 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/imwut-2020-wang.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/imwut-2020-wang" target="_blank">AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</a></h1><p class="meta"><span>Xiyue Wang</span> , <span>Kazuki Takashima</span> , <span>Tomoaki Adachi</span> , <span>Patrick Finn</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <span>Yoshifumi Kitamura</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/fxxvZBY80ug" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/fxxvZBY80ug?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/fxxvZBY80ug/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Natural disasters cause long-lasting mental health problems such as PTSD in children. Following the 2011 Earthquake and Tsunami in Japan, we witnessed a shift of toy block play behavior in young children who suffered from stress after the disaster. The behavior reflected their emotional responses to the traumatic event. In this paper, we explore the feasibility of using data captured from block-play to assess children&#x27;s stress after a major natural disaster. We prototyped sets of sensor-embedded toy blocks, AssessBlocks, that automate quantitative play data acquisition. During a three-year period, the blocks were dispatched to fifty-two post-disaster children. Within a free play session, we captured block features, a child&#x27;s playing behavior, and stress evaluated by several methods. The result from our analysis reveal correlations between block play features and stress measurements and show initial promise of using the effectiveness of using AssessBlocks to assess children&#x27;s stress after a disaster. We provide detailed insights into the potential as well as the challenges of our approach and unique conditions. From these insights we summarize guidelines for future research in automated play assessment systems that support children&#x27;s mental health.</p><div class="ui labels">Keywords:  <span class="ui large label">well being</span><span class="ui large label">toy blocks</span><span class="ui large label">PTSD</span><span class="ui large label">tangibles for health</span><span class="ui large label">stress assessment</span><span class="ui large label">play</span><span class="ui large label">children</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Xiyue Wang<!-- -->, <!-- -->Kazuki Takashima<!-- -->, <!-- -->Tomoaki Adachi<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Yoshifumi Kitamura<!-- -->. <b>AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</b>. <i>In Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->29<!-- -->.  DOI: <a href="https://doi.org/10.1145/3381016" target="_blank">https://doi.org/10.1145/3381016</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-suzuki" class="ui large modal"><div class="header"><a href="/publications/chi-2020-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-suzuki</a>  -  <a href="/publications/chi-2020-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-suzuki" target="_blank">RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Hooman Hedayati</span> , <span>Clement Zheng</span> , <span>James Bohn</span> , <span>Daniel Szafir</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D Gross</span> , <span>Daniel Leithinger</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.</p><div class="ui labels">Keywords:  <span class="ui large label">virtual reality</span><span class="ui large label">room-scale haptics</span><span class="ui large label">haptic interfaces</span><span class="ui large label">swarm robots</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Hooman Hedayati<!-- -->, <!-- -->Clement Zheng<!-- -->, <!-- -->James Bohn<!-- -->, <!-- -->Daniel Szafir<!-- -->, <!-- -->Ellen Yi-Luen Do<!-- -->, <!-- -->Mark D Gross<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->11<!-- -->.  DOI: <a href="https://doi.org/10.1145/3313831.3376523" target="_blank">https://doi.org/10.1145/3313831.3376523</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-goffin" class="ui large modal"><div class="header"><a href="/publications/chi-2020-goffin" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-goffin</a>  -  <a href="/publications/chi-2020-goffin.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-goffin.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-goffin" target="_blank">Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</a></h1><p class="meta"><span>Pascal Goffin</span> , <span>Tanja Blascheck</span> , <span>Petra Isenberg</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/wPaVdSWM8hU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/wPaVdSWM8hU?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/wPaVdSWM8hU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We describe a design space of view manipulation interactions for small data-driven contextual visualizations (word-scale visualizations). These interaction techniques support an active reading experience and engage readers through exploration of embedded visualizations whose placement and content connect them to specific terms in a document. A reader could, for example, use our proposed interaction techniques to explore word-scale visualizations of stock market trends for companies listed in a market overview article. When readers wish to engage more deeply with the data, they can collect, arrange, compare, and navigate the document using the embedded word-scale visualizations, permitting more visualization-centric analyses. We support our design space with a concrete implementation, illustrate it with examples from three application domains, and report results from two experiments. The experiments show how view manipulation interactions helped readers examine embedded visualizations more quickly and with less scrolling and yielded qualitative feedback on usability and future opportunities.</p><div class="ui labels">Keywords:  <span class="ui large label">glyphs</span><span class="ui large label">word-scale visualization</span><span class="ui large label">information visualization</span><span class="ui large label">interaction techniques</span><span class="ui large label">text visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Pascal Goffin<!-- -->, <!-- -->Tanja Blascheck<!-- -->, <!-- -->Petra Isenberg<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3313831.3376842" target="_blank">https://doi.org/10.1145/3313831.3376842</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-anjani" class="ui large modal"><div class="header"><a href="/publications/chi-2020-anjani" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-anjani</a>  -  <a href="/publications/chi-2020-anjani.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-anjani" target="_blank">Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</a></h1><p class="meta"><span>Laurensia Anjani</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><strong>Terrance Mok</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Wooi Boon Goh</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present a mixed-methods study of viewers on their practices and motivations around watching mukbang — video streams of people eating large quantities of food. Viewers&#x27; experiences provide insight on future technologies for multisensorial video streams and technology-supported commensality (eating with others). We surveyed 104 viewers and interviewed 15 of them about their attitudes and reflections on their mukbang viewing habits, their physiological aspects of watching someone eat, and their perceived social relationship with mukbangers. Based on our findings, we propose design implications for remote commensality, and for synchronized multisensorial video streaming content.</p><div class="ui labels">Keywords:  <span class="ui large label">video streams</span><span class="ui large label">mukbang</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Laurensia Anjani<!-- -->, <!-- -->Terrance Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Wooi Boon Goh<!-- -->. <b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3313831.3376567" target="_blank">https://doi.org/10.1145/3313831.3376567</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-hou" class="ui large modal"><div class="header"><a href="/publications/chi-2020-hou" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-hou</a>  -  <a href="/publications/chi-2020-hou.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-hou" target="_blank">Autonomous Vehicle-Cyclist Interaction: Peril and Promise</a></h1><p class="meta"><span>Ming Hou</span> , <a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><strong>Karthik Mahadevan</strong></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p></p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ming Hou<!-- -->, <!-- -->Karthik Mahadevan<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2020-suzuki" class="ui large modal"><div class="header"><a href="/publications/tei-2020-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2020-suzuki</a>  -  <a href="/publications/tei-2020-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2020-suzuki" target="_blank">LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Ryosuke Nakayama</span> , <span>Dan Liu</span> , <span>Yasuaki Kakehi</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/0LHeTkOMR84" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/0LHeTkOMR84?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/0LHeTkOMR84/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore interactions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust, making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various applications. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.</p><div class="ui labels">Keywords:  <span class="ui large label">shape-changing interfaces</span><span class="ui large label">inflatables</span><span class="ui large label">large-scale interactions</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Ryosuke Nakayama<!-- -->, <!-- -->Dan Liu<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces</b>. <i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->9<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mobilehci-2019-hung" class="ui large modal"><div class="header"><a href="/publications/mobilehci-2019-hung" target="_blank"><i class="fas fa-link fa-fw"></i>mobilehci-2019-hung</a>  -  <a href="/publications/mobilehci-2019-hung.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">MobileHCI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/mobilehci-2019-hung.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/mobilehci-2019-hung" target="_blank">WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</a></h1><p class="meta"><a href="/people/michael-hung"><img src="/static/images/people/michael-hung.jpg" class="ui circular spaced image mini-profile"/><strong>Michael Hung</strong></a> , <a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Pen-based input is often treated as auxiliary to mobile devices. We posit that cross-device interactions can inspire and extend the design space of pen-based interactions into new, expressive directions. We realize this through WatchPen, a smartwatch mounted on a passive, capacitive stylus that: (1) senses the usage context and leverages it for expression (e.g., changing colour), (2) contains tools and parameters within the display, and (3) acts as an on-demand output. As a result, it provides users with a dynamic relationship between inputs and outputs, awareness of current tool selection and parameters, and increased expressive match (e.g., added ability to mimic physical tools, showing clipboard contents). We discuss and reflect upon a series of interaction techniques that demonstrate WatchPen within a drawing application. We highlight the expressive power of leveraging multiple sensing and output capabilities across both the watch-augmented stylus and the tablet surface.</p><div class="ui labels">Keywords:  <span class="ui large label">smartwatch</span><span class="ui large label">cross-device interaction</span><span class="ui large label">pen interaction</span><span class="ui large label">interaction techniques</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Michael Hung<!-- -->, <!-- -->David Ledo<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</b>. <i>In Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->8<!-- -->.  DOI: <a href="https://doi.org/10.1145/3338286.3340122" target="_blank">https://doi.org/10.1145/3338286.3340122</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2019-suzuki" class="ui large modal"><div class="header"><a href="/publications/uist-2019-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2019-suzuki</a>  -  <a href="/publications/uist-2019-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2019-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2019-suzuki" target="_blank">ShapeBots: Shape-changing Swarm Robots</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Clement Zheng</span> , <span>Yasuaki Kakehi</span> , <span>Tom Yeh</span> , <span>Ellen Yi-Luen Do</span> , <span>Mark D. Gross</span> , <span>Daniel Leithinger</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We introduce shape-changing swarm robots. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.</p><div class="ui labels">Keywords:  <span class="ui large label">swarm user interfaces</span><span class="ui large label">shape-changing user interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Clement Zheng<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->, <!-- -->Tom Yeh<!-- -->, <!-- -->Ellen Yi-Luen Do<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>ShapeBots: Shape-changing Swarm Robots</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3332165.3347911" target="_blank">https://doi.org/10.1145/3332165.3347911</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2019-walny" class="ui large modal"><div class="header"><a href="/publications/tvcg-2019-walny" target="_blank"><i class="fas fa-link fa-fw"></i>tvcg-2019-walny</a>  -  <a href="/publications/tvcg-2019-walny.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TVCG 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2019-walny.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2019-walny" target="_blank">Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff</a></h1><p class="meta"><span>Jagoda Walny</span> , <span>Christian Frisson</span> , <span>Mieka West</span> , <span>Doris Kosminsky</span> , <a href="/people/soren-knudsen"><img src="/static/images/people/soren-knudsen.jpg" class="ui circular spaced image mini-profile"/><strong>Søren Knudsen</strong></a> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/360483702" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/360483702?autoplay=1&gt;&lt;img src=https://i.vimeocdn.com/video/814665539_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.</p><div class="ui labels">Keywords:  <span class="ui large label">information visualization</span><span class="ui large label">design handoff</span><span class="ui large label">data mapping</span><span class="ui large label">design process</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jagoda Walny<!-- -->, <!-- -->Christian Frisson<!-- -->, <!-- -->Mieka West<!-- -->, <!-- -->Doris Kosminsky<!-- -->, <!-- -->Søren Knudsen<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff</b>. <i>In IEEE Transactions on Visualization and Computer Graphics (TVCG &#x27;19)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1109/TVCG.2019.2934538" target="_blank">https://doi.org/10.1109/TVCG.2019.2934538</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="cnc-2019-hammad" class="ui large modal"><div class="header"><a href="/publications/cnc-2019-hammad" target="_blank"><i class="fas fa-link fa-fw"></i>cnc-2019-hammad</a>  -  <a href="/publications/cnc-2019-hammad.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">C&amp;C 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/cnc-2019-hammad.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/cnc-2019-hammad" target="_blank">Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning</a></h1><p class="meta"><a href="/people/nour-hammad"><img src="/static/images/people/nour-hammad.jpg" class="ui circular spaced image mini-profile"/><strong>Nour Hammad</strong></a> , <span>Elaheh Sanoubari</span> , <span>Patrick Finn</span> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <span>James E. Young</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p></p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nour Hammad<!-- -->, <!-- -->Elaheh Sanoubari<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->James E. Young<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning</b>. <i>In Proceedings of the ACM on Creativity and Cognition (C&amp;C &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-bressa" class="ui large modal"><div class="header"><a href="/publications/dis-2019-bressa" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2019-bressa</a>  -  <a href="/publications/dis-2019-bressa.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-bressa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-bressa" target="_blank">Sketching and Ideation Activities for Situated Visualization Design</a></h1><p class="meta"><a href="/people/nathalie-bressa"><img src="/static/images/people/nathalie-bressa.jpg" class="ui circular spaced image mini-profile"/><strong>Nathalie Bressa</strong></a> , <a href="/people/kendra-wannamaker"><img src="/static/images/people/kendra-wannamaker.jpg" class="ui circular spaced image mini-profile"/><strong>Kendra Wannamaker</strong></a> , <span>Henrik Korsgaard</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <span>Jo Vermeulen</span></p></div></div></div><div class="block"><h1>Abstract</h1><p></p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nathalie Bressa<!-- -->, <!-- -->Kendra Wannamaker<!-- -->, <!-- -->Henrik Korsgaard<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Jo Vermeulen<!-- -->. <b>Sketching and Ideation Activities for Situated Visualization Design</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-mahadevan" class="ui large modal"><div class="header"><a href="/publications/dis-2019-mahadevan" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2019-mahadevan</a>  -  <a href="/publications/dis-2019-mahadevan.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-mahadevan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-mahadevan" target="_blank">AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator</a></h1><p class="meta"><a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><strong>Karthik Mahadevan</strong></a> , <span>Elaheh Sanoubari</span> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <span>James E. Young</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>AV-pedestrian interaction will impact pedestrian safety, etiquette, and overall acceptance of AV technology. Evaluating AV-pedestrian interaction is challenging given limited availability of AVs and safety concerns. These challenges are compounded by &quot;mixed traffic&quot; conditions: studying AV-pedestrian interaction will be difficult in traffic consisting of vehicles varying in autonomy level. We propose immersive pedestrian simulators as design tools to study AV-pedestrian interaction, allowing rapid prototyping and evaluation of future AV-pedestrian interfaces. We present OnFoot: a VR-based simulator that immerses participants in mixed traffic conditions and allows examination of their behavior while controlling vehicles&#x27; autonomy-level, traffic and street characteristics, behavior of other virtual pedestrians, and integration of novel AV-pedestrian interfaces. We validated OnFoot against prior simulators and Wizard-of-Oz studies, and conducted a user study, manipulating vehicles&#x27; autonomy level, interfaces, and pedestrian group behavior. Our findings highlight the potential to use VR simulators as powerful tools for AV-pedestrian interaction design in mixed traffic.</p><div class="ui labels">Keywords:  <span class="ui large label">mixed traffic</span><span class="ui large label">pedestrian simulator</span><span class="ui large label">autonomous vehicle-pedestrian interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Karthik Mahadevan<!-- -->, <!-- -->Elaheh Sanoubari<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->James E. Young<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3322276.3322328" target="_blank">https://doi.org/10.1145/3322276.3322328</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-ledo" class="ui large modal"><div class="header"><a href="/publications/dis-2019-ledo" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2019-ledo</a>  -  <a href="/publications/dis-2019-ledo.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-ledo" target="_blank">Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</a></h1><p class="meta"><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <span>Jo Vermeulen</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><strong>Saul Greenberg</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Sebastian Boring</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Astral is a prototyping tool for authoring mobile and smart object interactive behaviours. It mirrors selected display contents of desktop applications onto mobile devices (smartphones and smartwatches), and streams/remaps mobile sensor data to desktop input events (mouse or keyboard) to manipulate selected desktop contents. This allows designers to use familiar desktop applications (e.g. PowerPoint, AfterEffects) to prototype rich interactive behaviours. Astral combines and integrates display mirroring, sensor streaming and input remapping, where designers can exploit familiar desktop applications to prototype, explore and fine-tune dynamic interactive behaviours. With Astral, designers can visually author rules to test real-time behaviours while interactions take place, as well as after the interaction has occurred. We demonstrate Astral&#x27;s applicability, workflow and expressiveness within the interaction design process through both new examples and replication of prior approaches that illustrate how various familiar desktop applications are leveraged and repurposed.</p><div class="ui labels">Keywords:  <span class="ui large label">smart objects</span><span class="ui large label">mobile interfaces</span><span class="ui large label">prototyping</span><span class="ui large label">design tool</span><span class="ui large label">interactive behaviour</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Saul Greenberg<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Sebastian Boring<!-- -->. <b>Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->14<!-- -->.  DOI: <a href="https://doi.org/10.1145/3322276.3322329" target="_blank">https://doi.org/10.1145/3322276.3322329</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-seyed" class="ui large modal"><div class="header"><a href="/publications/dis-2019-seyed" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2019-seyed</a>  -  <a href="/publications/dis-2019-seyed.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-seyed.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-seyed" target="_blank">Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways</a></h1><p class="meta"><a href="/people/teddy-seyed"><img src="/static/images/people/teddy-seyed.jpg" class="ui circular spaced image mini-profile"/><strong>Teddy Seyed</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Drawing upon multiple disciplines, avant-garde fashion-tech teams push the boundaries between fashion and technology. Many are well trained in envisioning aesthetic qualities of garments, but few have formal training on designing and fabricating technologies themselves. We introduce Mannequette, a prototyping tool for fashion-tech garments that enables teams to experiment with interactive technologies at early stages of their design processes. Mannequette provides an abstraction of light-based outputs and sensor-based inputs for garments through a DJ mixer-like interface that allows for dynamic changes and recording/playback of visual effects. The base of Mannequette can also be incorporated into the final garment, where it is then connected to the final components. We conducted an 8-week deployment study with eight design teams who created new garments for a runway show. Our results revealed Mannequette allowed teams to repeatedly consider new design and technical options early in their creative processes, and to communicate more effectively across disciplinary backgrounds.</p><div class="ui labels">Keywords:  <span class="ui large label">fashion</span><span class="ui large label">haute couture</span><span class="ui large label">e-textiles</span><span class="ui large label">maker culture</span><span class="ui large label">fashion-tech</span><span class="ui large label">wearables</span><span class="ui large label">avant-garde</span><span class="ui large label">haute-tech couture</span><span class="ui large label">modular</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Teddy Seyed<!-- -->, <!-- -->Anthony Tang<!-- -->. <b>Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3322276.3322305" target="_blank">https://doi.org/10.1145/3322276.3322305</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-nakayama" class="ui large modal"><div class="header"><a href="/publications/dis-2019-nakayama" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2019-nakayama</a>  -  <a href="/publications/dis-2019-nakayama.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-nakayama.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-nakayama" target="_blank">MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction</a></h1><p class="meta"><span>Ryosuke Nakayama</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Satoshi Nakamaru</span> , <span>Ryuma Niiyama</span> , <span>Yoshihiro Kawahara</span> , <span>Yasuaki Kakehi</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/ZkCcazfFD-M" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/ZkCcazfFD-M?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/ZkCcazfFD-M/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce MorphIO, entirely soft sensing and actuation modules for programming by demonstration of soft robots and shape-changing interfaces.MorphIO&#x27;s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor.This allows both input and output of three-dimensional deformation of a soft material.Leveraging this capability, MorphIO enables a user to record and later playback physical motion of programmable shape-changing materials.In addition, the modular design of MorphIO&#x27;s unit allows the user to construct various shapes and topologies through magnetic connection.We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects.Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.</p><div class="ui labels">Keywords:  <span class="ui large label">shape-changing interfaces</span><span class="ui large label">programming by demonstration</span><span class="ui large label">soft robots</span><span class="ui large label">pneumatic actuation</span><span class="ui large label">tangible interactions</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryosuke Nakayama<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Satoshi Nakamaru<!-- -->, <!-- -->Ryuma Niiyama<!-- -->, <!-- -->Yoshihiro Kawahara<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->. <b>MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->11<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2019-blascheck" class="ui large modal"><div class="header"><a href="/publications/tvcg-2019-blascheck" target="_blank"><i class="fas fa-link fa-fw"></i>tvcg-2019-blascheck</a>  -  <a href="/publications/tvcg-2019-blascheck.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TVCG 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2019-blascheck.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2019-blascheck" target="_blank">Exploration Strategies for Discovery of Interactivity in Visualizations</a></h1><p class="meta"><span>Tanja Blascheck</span> , <span>Lindsay MacDonald Vermeulen</span> , <span>Jo Vermeulen</span> , <span>Charles Perin</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <span>Thomas Ertl</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We investigate how people discover the functionality of an interactive visualization that was designed for the general public. While interactive visualizations are increasingly available for public use, we still know little about how the general public discovers what they can do with these visualizations and what interactions are available. Developing a better understanding of this discovery process can help inform the design of visualizations for the general public, which in turn can help make data more accessible. To unpack this problem, we conducted a lab study in which participants were free to use their own methods to discover the functionality of a connected set of interactive visualizations of public energy data. We collected eye movement data and interaction logs as well as video and audio recordings. By analyzing this combined data, we extract exploration strategies that the participants employed to discover the functionality in these interactive visualizations. These exploration strategies illuminate possible design directions for improving the discoverability of a visualization&#x27;s functionality.</p><div class="ui labels">Keywords:  <span class="ui large label">discovery</span><span class="ui large label">visualization</span><span class="ui large label">open data</span><span class="ui large label">evaluation</span><span class="ui large label">eye tracking</span><span class="ui large label">interaction logs</span><span class="ui large label">think-aloud</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Tanja Blascheck<!-- -->, <!-- -->Lindsay MacDonald Vermeulen<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Charles Perin<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Thomas Ertl<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->. <b>Exploration Strategies for Discovery of Interactivity in Visualizations</b>. <i>In IEEE Transactions on Visualization and Computer Graphics (TVCG &#x27;19)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1109/TVCG.2018.2802520" target="_blank">https://doi.org/10.1109/TVCG.2018.2802520</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2019-danyluk" class="ui large modal"><div class="header"><a href="/publications/chi-2019-danyluk" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2019-danyluk</a>  -  <a href="/publications/chi-2019-danyluk.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2019-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2019-danyluk" target="_blank">Look-From Camera Control for 3D Terrain Maps</a></h1><p class="meta"><span>Kurtis Thorvald Danyluk</span> , <span>Bernhard Jenny</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We introduce three lightweight interactive camera control techniques for 3D terrain maps on touch devices based on a look-from metaphor (Discrete Look-From-At, Continuous Look-From-Forwards, and Continuous Look-From-Towards). These techniques complement traditional touch screen pan, zoom, rotate, and pitch controls allowing viewers to quickly transition between top-down, oblique, and ground-level views. We present the results of a study in which we asked participants to perform elevation comparison and line-of-sight determination tasks using each technique. Our results highlight how look-from techniques can be integrated on top of current direct manipulation navigation approaches by combining several direct manipulation operations into a single look-from operation. Additionally, they show how look-from techniques help viewers complete a variety of common and challenging map-based tasks.</p><div class="ui labels">Keywords:  <span class="ui large label">terrain</span><span class="ui large label">touch</span><span class="ui large label">map interaction</span><span class="ui large label">look-from camera control</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Thorvald Danyluk<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Look-From Camera Control for 3D Terrain Maps</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3290605.3300594" target="_blank">https://doi.org/10.1145/3290605.3300594</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2019-mikalauskas" class="ui large modal"><div class="header"><a href="/publications/tei-2019-mikalauskas" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2019-mikalauskas</a>  -  <a href="/publications/tei-2019-mikalauskas.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-mikalauskas.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2019-mikalauskas" target="_blank">Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</a></h1><p class="meta"><span>Claire Mikalauskas</span> , <span>April Viczko</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>While improvised theatre (improv) is often performed on a bare stage, improvisers sometimes incorporate physical props to inspire new directions for a scene and to enrich their performance. A tech booth can improvise light and sound technical elements, but coordinating with improvisers&#x27; actions on-stage is challenging. Our goal is to inform the design of an augmented prop that lets improvisers tangibly control light and sound technical elements while performing. We interviewed five professional improvisers about their use of physical props in improv, and their expectations of a possible augmented prop that controls technical theatre elements. We propose a set of guidelines for the design of an augmented prop that fits with the existing world of unpredictable improvised performance.</p><div class="ui labels">Keywords:  <span class="ui large label">props</span><span class="ui large label">performer-controlled technology</span><span class="ui large label">improvisational theatre</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Claire Mikalauskas<!-- -->, <!-- -->April Viczko<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</b>. <i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->9<!-- -->.  DOI: <a href="https://doi.org/10.1145/3294109.3295631" target="_blank">https://doi.org/10.1145/3294109.3295631</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2019-wun" class="ui large modal"><div class="header"><a href="/publications/tei-2019-wun" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2019-wun</a>  -  <a href="/publications/tei-2019-wun.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-wun.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2019-wun" target="_blank">You say Potato, I say Po-Data: Physical Template Tools for Authoring Visualizations</a></h1><p class="meta"><span>Tiffany Wun</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Miriam Sturdee</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Providing data visualization authoring tools for the general public remains an ongoing challenge. Inspired by block-printing, we explore how visualization stamps as a physical visualization authoring tool could leverage both visual freedom and ease of repetition. We conducted a workshop with two groups---visualization experts and non-experts---where participants authored visualizations on paper using hand-carved stamps made from potatoes and sponges. The low-fidelity medium freed participants to test new stamp patterns and accept mistakes. From the created visualizations, we observed several unique traits and uses of block-printing tools for visualization authoring, including: modularity of patterns, annotation guides, creation of multiple patterns from one stamp, and various techniques to apply data onto paper. We discuss the issues around expressivity and effectiveness of block-printed stamps in visualization authoring, and identify implications for the design and assembly of primitives in potential visualization stamp kits, as well as applications for future use in non-digital environments.</p><div class="ui labels">Keywords:  <span class="ui large label">potato</span><span class="ui large label">tangible tools</span><span class="ui large label">authoring visualizations</span><span class="ui large label">block-printing</span><span class="ui large label">physical template tools</span><span class="ui large label">information visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Tiffany Wun<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Miriam Sturdee<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->. <b>You say Potato, I say Po-Data: Physical Template Tools for Authoring Visualizations</b>. <i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/3294109.3295627" target="_blank">https://doi.org/10.1145/3294109.3295627</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2019-tolley" class="ui large modal"><div class="header"><a href="/publications/tei-2019-tolley" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2019-tolley</a>  -  <a href="/publications/tei-2019-tolley.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-tolley.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2019-tolley" target="_blank">WindyWall: Exploring Creative Wind Simulations</a></h1><p class="meta"><span>David Tolley</span> , <span>Thi Ngoc Tram Nguyen</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>Nimesha Ranasinghe</span> , <span>Kensaku Kawauchi</span> , <span>Ching-Chiuan Yen</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Wind simulations are typically one-off implementations for specific applications. We introduce WindyWall, a platform for creative design and exploration of wind simulations. WindyWall is a three-panel 90-fan array that encapsulates users with 270? of wind coverage. We describe the design and implementation of the array panels, discussing how the panels can be re-arranged, where various wind simulations can be realized as simple effects. To understand how people perceive &quot;wind&quot; generated from WindyWall, we conducted a pilot study of wind magnitude perception using different wind activation patterns from WindyWall. Our findings suggest that: horizontal wind activations are perceived more readily than vertical ones, and that people&#x27;s perceptions of wind are highly variable-most individuals will rate airflow differently in subsequent exposures. Based on our findings, we discuss the importance of developing a method for characterizing wind simulations, and provide design directions for others using fan arrays to simulate wind.</p><div class="ui labels">Keywords:  <span class="ui large label">tactile/haptic interaction</span><span class="ui large label">multimodal interaction</span><span class="ui large label">novel actuators/displays</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Tolley<!-- -->, <!-- -->Thi Ngoc Tram Nguyen<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Nimesha Ranasinghe<!-- -->, <!-- -->Kensaku Kawauchi<!-- -->, <!-- -->Ching-Chiuan Yen<!-- -->. <b>WindyWall: Exploring Creative Wind Simulations</b>. <i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/3294109.3295624" target="_blank">https://doi.org/10.1145/3294109.3295624</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="vr-2019-satriadi" class="ui large modal"><div class="header"><a href="/publications/vr-2019-satriadi" target="_blank"><i class="fas fa-link fa-fw"></i>vr-2019-satriadi</a>  -  <a href="/publications/vr-2019-satriadi.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IEEE VR 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/vr-2019-satriadi.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/vr-2019-satriadi" target="_blank">Augmented Reality Map Navigation with Freehand Gestures</a></h1><p class="meta"><span>Kadek Ananta Satriadi</span> , <span>Barrett Ens</span> , <span>Maxime Cordeil</span> , <span>Bernhard Jenny</span> , <span>Tobias Czauderna</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Freehand gesture interaction has long been proposed as a `natural&#x27; input method for Augmented Reality (AR) applications, yet has been little explored for intensive applications like multiscale navigation. In multiscale navigation, such as digital map navigation, pan and zoom are the predominant interactions. A position-based input mapping (e.g. grabbing metaphor) is intuitive for such interactions, but is prone to arm fatigue. This work focuses on improving digital map navigation in AR with mid-air hand gestures, using a horizontal intangible map display. First, we conducted a user study to explore the effects of handedness (unimanual and bimanual) and input mapping (position-based and rate-based). From these findings we designed DiveZoom and TerraceZoom, two novel hybrid techniques that smoothly transition between position- and rate-based mappings. A second user study evaluated these designs. Our results indicate that the introduced input-mapping transitions can reduce perceived arm fatigue with limited impact on performance.</p><div class="ui labels">Keywords:  <span class="ui large label">augmented reality</span><span class="ui large label">gesture recognition</span><span class="ui large label">human computer interaction</span><span class="ui large label">interactive devices</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kadek Ananta Satriadi<!-- -->, <!-- -->Barrett Ens<!-- -->, <!-- -->Maxime Cordeil<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Tobias Czauderna<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Augmented Reality Map Navigation with Freehand Gestures</b>. <i>In undefined (IEEE VR &#x27;19)</i>. <!-- -->  Page: 1-<!-- -->11<!-- -->.  DOI: <a href="https://doi.org/10.1109/VR.2019.8798340" target="_blank">https://doi.org/10.1109/VR.2019.8798340</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2018-suzuki" class="ui large modal"><div class="header"><a href="/publications/uist-2018-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2018-suzuki</a>  -  <a href="/publications/uist-2018-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2018-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2018-suzuki" target="_blank">Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Junichi Yamaoka</span> , <span>Daniel Leithinger</span> , <span>Tom Yeh</span> , <span>Mark D. Gross</span> , <span>Yoshihiro Kawahara</span> , <span>Yasuaki Kakehi</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/7nPlr3O9xu8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/7nPlr3O9xu8?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/7nPlr3O9xu8/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.</p><div class="ui labels">Keywords:  <span class="ui large label">digital materials</span><span class="ui large label">dynamic 3D printing</span><span class="ui large label">shape displays</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Junichi Yamaoka<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Tom Yeh<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Yoshihiro Kawahara<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->. <b>Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3242587.3242659" target="_blank">https://doi.org/10.1145/3242587.3242659</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2018-mikalauskas" class="ui large modal"><div class="header"><a href="/publications/dis-2018-mikalauskas" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2018-mikalauskas</a>  -  <a href="/publications/dis-2018-mikalauskas.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2018-mikalauskas.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2018-mikalauskas" target="_blank">Improvising with an Audience-Controlled Robot Performer</a></h1><p class="meta"><span>Claire Mikalauskas</span> , <span>Tiffany Wun</span> , <span>Kevin Ta</span> , <span>Joshua Horacsek</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In improvisational theatre (improv), actors perform unscripted scenes together, collectively creating a narrative. Audience suggestions introduce randomness and build audience engagement, but can be challenging to mediate at scale. We present Robot Improv Puppet Theatre (RIPT), which includes a performance robot (Pokey) who performs gestures and dialogue in short-form improv scenes based on audience input from a mobile interface. We evaluated RIPT in several initial informal performances, and in a rehearsal with seven professional improvisers. The improvisers noted how audience prompts can have a big impact on the scene - highlighting the delicate balance between ambiguity and constraints in improv. The open structure of RIPT performances allows for multiple interpretations of how to perform with Pokey, including one-on-one conversations or multi-performer scenes. While Pokey lacks key qualities of a good improviser, improvisers found his serendipitous dialogue and gestures particularly rewarding.</p><div class="ui labels">Keywords:  <span class="ui large label">human-robot interaction</span><span class="ui large label">improvised theatre</span><span class="ui large label">creativity-support tools</span><span class="ui large label">crowdsourcing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Claire Mikalauskas<!-- -->, <!-- -->Tiffany Wun<!-- -->, <!-- -->Kevin Ta<!-- -->, <!-- -->Joshua Horacsek<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Improvising with an Audience-Controlled Robot Performer</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/3196709.3196757" target="_blank">https://doi.org/10.1145/3196709.3196757</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2018-pham" class="ui large modal"><div class="header"><a href="/publications/dis-2018-pham" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2018-pham</a>  -  <a href="/publications/dis-2018-pham.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2018-pham.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2018-pham" target="_blank">Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design</a></h1><p class="meta"><span>Tran Pham</span> , <span>Jo Vermeulen</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>Lindsay MacDonald Vermeulen</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Because gesture design for augmented reality (AR) remains idiosyncratic, people cannot necessarily use gestures learned in one AR application in another. To design discoverable gestures, we need to understand what gestures people expect to use. We explore how the scale of AR affects the gestures people expect to use to interact with 3D holograms. Using an elicitation study, we asked participants to generate gestures in response to holographic task referents, where we varied the scale of holograms from desktop-scale to room-scale objects. We found that the scale of objects and scenes in the AR experience moderates the generated gestures. Most gestures were informed by physical interaction, and when people interacted from a distance, they sought a good perspective on the target object before and during the interaction. These results suggest that gesture designers need to account for scale, and should not simply reuse gestures across different hologram sizes.</p><div class="ui labels">Keywords:  <span class="ui large label">augmented reality</span><span class="ui large label">gestures</span><span class="ui large label">gesture elicitation</span><span class="ui large label">hololens</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Tran Pham<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lindsay MacDonald Vermeulen<!-- -->. <b>Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->14<!-- -->.  DOI: <a href="https://doi.org/10.1145/3196709.3196719" target="_blank">https://doi.org/10.1145/3196709.3196719</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2018-ta" class="ui large modal"><div class="header"><a href="/publications/dis-2018-ta" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2018-ta</a>  -  <a href="/publications/dis-2018-ta.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2018-ta.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2018-ta" target="_blank">Bod-IDE: An Augmented Reality Sandbox for eFashion Garments</a></h1><p class="meta"><span>Kevin Ta</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Electronic fashion (eFashion) garments use technology to augment the human body with wearable interaction. In developing ideas, eFashion designers need to prototype the role and behavior of the interactive garment in context; however, current wearable prototyping toolkits require semi-permanent construction with physical materials that cannot easily be altered. We present Bod-IDE, an augmented reality &#x27;mirror&#x27; that allows eFashion designers to create virtual interactive garment prototypes. Designers can quickly build, refine, and test on-the-body interactions without the need to connect or program electronics. By envisioning interaction with the body in mind, eFashion designers can focus more on reimagining the relationship between bodies, clothing, and technology.</p><div class="ui labels">Keywords:  <span class="ui large label">augmented reality</span><span class="ui large label">electronic fashion</span><span class="ui large label">creativity support tool</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kevin Ta<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Bod-IDE: An Augmented Reality Sandbox for eFashion Garments</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->5<!-- -->.  DOI: <a href="https://doi.org/10.1145/3197391.3205408" target="_blank">https://doi.org/10.1145/3197391.3205408</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2016-somanath" class="ui large modal"><div class="header"><a href="/publications/tei-2016-somanath" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2016-somanath</a>  -  <a href="/publications/tei-2016-somanath.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2016</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2016-somanath.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2016-somanath" target="_blank">Engaging &#x27;At-Risk&#x27; Students through Maker Culture Activities</a></h1><p class="meta"><a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <span>Laura Morrison</span> , <span>Janette Hughes</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <span>Mario Costa Sousa</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>This paper presents a set of lessons learnt from introducing maker culture and DIY paradigms to &#x27;at-risk&#x27; students (age 12-14). Our goal is to engage &#x27;at-risk&#x27; students through maker culture activities. While improved technology literacy is one of the outcomes we also wanted the learners to use technology to realize concepts and ideas, and to gain freedom of thinking similar to creators, artists and designers. We present our study and a set of high level suggestions to enable thinking about how maker culture activities can facilitate engagement and creative use of technology by 1) thinking about creativity in task, 2) facilitating different entry points, 3) the importance of personal relevance, and 4) relevance to education.</p><div class="ui labels">Keywords:  <span class="ui large label">DIY</span><span class="ui large label">&#x27;at-risk&#x27; students</span><span class="ui large label">maker culture</span><span class="ui large label">education</span><span class="ui large label">young learners</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sowmya Somanath<!-- -->, <!-- -->Laura Morrison<!-- -->, <!-- -->Janette Hughes<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Mario Costa Sousa<!-- -->. <b>Engaging &#x27;At-Risk&#x27; Students through Maker Culture Activities</b>. <i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;16)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->9<!-- -->.  DOI: <a href="https://doi.org/10.1145/2839462.2839482" target="_blank">https://doi.org/10.1145/2839462.2839482</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-oh" class="ui large modal"><div class="header"><a href="/publications/chi-2018-oh" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-oh</a>  -  <a href="/publications/chi-2018-oh.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-oh.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-oh" target="_blank">PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices</a></h1><p class="meta"><span>Hyunjoo Oh</span> , <span>Tung D. Ta</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Mark D. Gross</span> , <span>Yoshihiro Kawahara</span> , <span>Lining Yao</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/DTd863suDN0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/DTd863suDN0?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/DTd863suDN0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present PEP (Printed Electronic Papercrafts), a set of design and fabrication techniques to integrate electronic based interactivities into printed papercrafts via 3D sculpting. We explore the design space of PEP, integrating four functions into 3D paper products: actuation, sensing, display, and communication, leveraging the expressive and technical opportunities enabled by paper-like functional layers with a stack of paper. We outline a seven-step workflow, introduce a design tool we developed as an add-on to an existing CAD environment, and demonstrate example applications that combine the electronic enabled functionality, the capability of 3D sculpting, and the unique creative affordances by the materiality of paper.</p><div class="ui labels">Keywords:  <span class="ui large label">paper electronics</span><span class="ui large label">3d sculpting</span><span class="ui large label">paper craft</span><span class="ui large label">fabrication techniques</span><span class="ui large label">prototyping</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hyunjoo Oh<!-- -->, <!-- -->Tung D. Ta<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Yoshihiro Kawahara<!-- -->, <!-- -->Lining Yao<!-- -->. <b>PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3174015" target="_blank">https://doi.org/10.1145/3173574.3174015</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-suzuki" class="ui large modal"><div class="header"><a href="/publications/chi-2018-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-suzuki</a>  -  <a href="/publications/chi-2018-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-suzuki" target="_blank">Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Jun Kato</span> , <span>Mark D. Gross</span> , <span>Tom Yeh</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Gb7brajKCVE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Gb7brajKCVE?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/Gb7brajKCVE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high-level interface design. Inspired by current UI programming practices, we introduce a four-step workflow-create elements, abstract attributes, specify behaviors, and propagate changes-for Swarm UI programming. We propose a set of direct physical manipulation techniques to support each step in this workflow. To demonstrate these concepts, we developed Reactile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies-an in-class survey with 148 students and a lab interview with eight participants-confirm that our approach is intuitive and understandable for programming Swarm UIs.</p><div class="ui labels">Keywords:  <span class="ui large label">direct manipulation</span><span class="ui large label">tangible programming</span><span class="ui large label">swarm user interfaces</span><span class="ui large label">programming by demonstration</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Jun Kato<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Tom Yeh<!-- -->. <b>Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173773" target="_blank">https://doi.org/10.1145/3173574.3173773</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-dillman" class="ui large modal"><div class="header"><a href="/publications/chi-2018-dillman" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-dillman</a>  -  <a href="/publications/chi-2018-dillman.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-dillman.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-dillman" target="_blank">A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</a></h1><p class="meta"><span>Kody R. Dillman</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><strong>Terrance Mok</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Alex Mitchell</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences.</p><div class="ui labels">Keywords:  <span class="ui large label">game design</span><span class="ui large label">guidance</span><span class="ui large label">interaction cues</span><span class="ui large label">augmented reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kody R. Dillman<!-- -->, <!-- -->Terrance Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Alex Mitchell<!-- -->. <b>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173714" target="_blank">https://doi.org/10.1145/3173574.3173714</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-feick" class="ui large modal"><div class="header"><a href="/publications/chi-2018-feick" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-feick</a>  -  <a href="/publications/chi-2018-feick.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-feick.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-feick" target="_blank">Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</a></h1><p class="meta"><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><strong>Martin Feick</strong></a> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/sfxTHsPJWHY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/sfxTHsPJWHY?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/sfxTHsPJWHY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Remote collaborators working together on physical objects have difficulty building a shared understanding of what each person is talking about. Conventional video chat systems are insufficient for many situations because they present a single view of the object in a flattened image. To understand how this limited perspective affects collaboration, we designed the Remote Manipulator (ReMa), which can reproduce orientation manipulations on a proxy object at a remote site. We conducted two studies with ReMa, with two main findings. First, a shared perspective is more effective and preferred compared to the opposing perspective offered by conventional video chat systems. Second, the physical proxy and video chat complement one another in a combined system: people used the physical proxy to understand objects, and used video chat to perform gestures and confirm remote actions.</p><div class="ui labels">Keywords:  <span class="ui large label">cscw</span><span class="ui large label">remote collaboration</span><span class="ui large label">object-focused collaboration</span><span class="ui large label">physical telepresence</span><span class="ui large label">collaborative physical tasks</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Terrance Tin Hoi Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173855" target="_blank">https://doi.org/10.1145/3173574.3173855</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-ledo" class="ui large modal"><div class="header"><a href="/publications/chi-2018-ledo" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-ledo</a>  -  <a href="/publications/chi-2018-ledo.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-ledo" target="_blank">Evaluation Strategies for HCI Toolkit Research</a></h1><p class="meta"><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <span>Steven Houben</span> , <span>Jo Vermeulen</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><strong>Nicolai Marquardt</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><strong>Saul Greenberg</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/3lAwhCk60C4" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/3lAwhCk60C4?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/3lAwhCk60C4/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what &#x27;evaluating&#x27; a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.</p><div class="ui labels">Keywords:  <span class="ui large label">user interfaces</span><span class="ui large label">design</span><span class="ui large label">evaluation</span><span class="ui large label">prototyping</span><span class="ui large label">toolkits</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Steven Houben<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Nicolai Marquardt<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Saul Greenberg<!-- -->. <b>Evaluation Strategies for HCI Toolkit Research</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->17<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173610" target="_blank">https://doi.org/10.1145/3173574.3173610</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-mahadevan" class="ui large modal"><div class="header"><a href="/publications/chi-2018-mahadevan" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-mahadevan</a>  -  <a href="/publications/chi-2018-mahadevan.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-mahadevan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-mahadevan" target="_blank">Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction</a></h1><p class="meta"><a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><strong>Karthik Mahadevan</strong></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/D_hhcGVREGA" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/D_hhcGVREGA?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/D_hhcGVREGA/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Drivers use nonverbal cues such as vehicle speed, eye gaze, and hand gestures to communicate awareness and intent to pedestrians. Conversely, in autonomous vehicles, drivers can be distracted or absent, leaving pedestrians to infer awareness and intent from the vehicle alone. In this paper, we investigate the usefulness of interfaces (beyond vehicle movement) that explicitly communicate awareness and intent of autonomous vehicles to pedestrians, focusing on crosswalk scenarios. We conducted a preliminary study to gain insight on designing interfaces that communicate autonomous vehicle awareness and intent to pedestrians. Based on study outcomes, we developed four prototype interfaces and deployed them in studies involving a Segway and a car. We found interfaces communicating vehicle awareness and intent: (1) can help pedestrians attempting to cross; (2) are not limited to the vehicle and can exist in the environment; and (3) should use a combination of modalities such as visual, auditory, and physical.</p><div class="ui labels">Keywords:  <span class="ui large label">autonomous vehicle-pedestrian interaction</span><span class="ui large label">perceived awareness and intent in autonomous vehicles</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Karthik Mahadevan<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3174003" target="_blank">https://doi.org/10.1145/3173574.3174003</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-heshmat" class="ui large modal"><div class="header"><a href="/publications/chi-2018-heshmat" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-heshmat</a>  -  <a href="/publications/chi-2018-heshmat.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-heshmat.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-heshmat" target="_blank">Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing</a></h1><p class="meta"><span>Yasamin Heshmat</span> , <a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><strong>Brennan Jones</strong></a> , <span>Xiaoxuan Xiong</span> , <span>Carman Neustaedter</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>Bernhard E. Riecke</span> , <span>Lillian Yang</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>People often enjoy sharing outdoor activities together such as walking and hiking. However, when family and friends are separated by distance it can be difficult if not impossible to share such activities. We explore this design space by investigating the benefits and challenges of using a telepresence robot to support outdoor leisure activities. In our study, participants participated in the outdoor activity of geocaching where one person geocached with the help of a remote partner via a telepresence robot. We compared a wide field of view (WFOV) camera to a 360° camera. Results show the benefits of having a physical embodiment and a sense of immersion with the 360° view. Yet challenges related to a lack of environmental awareness, safety issues, and privacy concerns resulting from bystander interactions. These findings illustrate the need to design telepresence robots with the environment and public in mind to provide an enhanced sensory experience while balancing safety and privacy issues resulting from being amongst the general public.</p><div class="ui labels">Keywords:  <span class="ui large label">video communication</span><span class="ui large label">telepresence robots</span><span class="ui large label">leisure activities</span><span class="ui large label">social presence</span><span class="ui large label">geocaching</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Yasamin Heshmat<!-- -->, <!-- -->Brennan Jones<!-- -->, <!-- -->Xiaoxuan Xiong<!-- -->, <!-- -->Carman Neustaedter<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Bernhard E. Riecke<!-- -->, <!-- -->Lillian Yang<!-- -->. <b>Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173933" target="_blank">https://doi.org/10.1145/3173574.3173933</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-neustaedter" class="ui large modal"><div class="header"><a href="/publications/chi-2018-neustaedter" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-neustaedter</a>  -  <a href="/publications/chi-2018-neustaedter.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-neustaedter.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-neustaedter" target="_blank">The Benefits and Challenges of Video Calling for Emergency Situations</a></h1><p class="meta"><span>Carman Neustaedter</span> , <a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><strong>Brennan Jones</strong></a> , <span>Kenton O&#x27;Hara</span> , <span>Abigail Sellen</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In the coming years, emergency calling services in North America will begin to incorporate new modalities for reporting emergencies, including video-based calling. The challenge is that we know little of how video calling systems should be designed and what benefits or challenges video calling might bring. We conducted observations and contextual interviews within three emergency response call centres to investigate these points. We focused on the work practices of call takers and dispatchers. Results show that video calls could provide valuable contextual information about a situation and help to overcome call taker challenges with information ambiguity, location, deceit, and communication issues. Yet video calls have the potential to introduce issues around control, information overload, and privacy if systems are not designed well. These results point to the need to think about emergency video calling along a continuum of visual modalities ranging from audio calls accompanied with images or video clips to one-way video streams to two-way video streams where camera control and camera work need to be carefully designed.</p><div class="ui labels">Keywords:  <span class="ui large label">collaboration</span><span class="ui large label">situation awareness</span><span class="ui large label">emergency calling</span><span class="ui large label">call takers</span><span class="ui large label">mobile video calling</span><span class="ui large label">dispatchers</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Carman Neustaedter<!-- -->, <!-- -->Brennan Jones<!-- -->, <!-- -->Kenton O&#x27;Hara<!-- -->, <!-- -->Abigail Sellen<!-- -->. <b>The Benefits and Challenges of Video Calling for Emergency Situations</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3174231" target="_blank">https://doi.org/10.1145/3173574.3174231</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-wuertz" class="ui large modal"><div class="header"><a href="/publications/chi-2018-wuertz" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-wuertz</a>  -  <a href="/publications/chi-2018-wuertz.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-wuertz.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-wuertz" target="_blank">A Design Framework for Awareness Cues in Distributed Multiplayer Games</a></h1><p class="meta"><span>Jason Wuertz</span> , <span>Sultan A. Alharthi</span> , <span>William A. Hamilton</span> , <span>Scott Bateman</span> , <span>Carl Gutwin</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>Zachary O. Toups</span> , <span>Jessica Hammer</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>In the physical world, teammates develop situation awareness about each other&#x27;s location, status, and actions through cues such as gaze direction and ambient noise. To support situation awareness, distributed multiplayer games provide awareness cues - information that games automatically make available to players to support cooperative gameplay. The design of awareness cues can be extremely complex, impacting how players experience games and work with teammates. Despite the importance of awareness cues, designers have little beyond experiential knowledge to guide their design. In this work, we describe a design framework for awareness cues, providing insight into what information they provide, how they communicate this information, and how design choices can impact play experience. Our research, based on a grounded theory analysis of current games, is the first to provide a characterization of awareness cues, providing a palette for game designers to improve design practice and a starting point for deeper research into collaborative play.</p><div class="ui labels">Keywords:  <span class="ui large label">workspace awareness</span><span class="ui large label">situation awareness</span><span class="ui large label">game design</span><span class="ui large label">distributed multiplayer games</span><span class="ui large label">awareness cues</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jason Wuertz<!-- -->, <!-- -->Sultan A. Alharthi<!-- -->, <!-- -->William A. Hamilton<!-- -->, <!-- -->Scott Bateman<!-- -->, <!-- -->Carl Gutwin<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Zachary O. Toups<!-- -->, <!-- -->Jessica Hammer<!-- -->. <b>A Design Framework for Awareness Cues in Distributed Multiplayer Games</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->14<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173817" target="_blank">https://doi.org/10.1145/3173574.3173817</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="hri-2018-feick" class="ui large modal"><div class="header"><a href="/publications/hri-2018-feick" target="_blank"><i class="fas fa-link fa-fw"></i>hri-2018-feick</a>  -  <a href="/publications/hri-2018-feick.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">HRI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/hri-2018-feick" target="_blank">The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</a></h1><p class="meta"><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><strong>Martin Feick</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>André Miede</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we discuss the role of the movement trajectory and velocity enabled by our tele-robotic system (ReMa) for remote collaboration on physical tasks. Our system reproduces changes in object orientation and position at a remote location using a humanoid robotic arm. However, even minor kinematics differences between robot and human arm can result in awkward or exaggerated robot movements. As a result, user communication with the robotic system can become less efficient, less fluent and more time intensive.</p><div class="ui labels">Keywords:  <span class="ui large label">movement trajectory &amp; velocity</span><span class="ui large label">remote collaboration</span><span class="ui large label">robot surrogate</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->André Miede<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b>. <i>In Adjunct Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction (HRI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->2<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173386.3176959" target="_blank">https://doi.org/10.1145/3173386.3176959</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="sui-2017-li" class="ui large modal"><div class="header"><a href="/publications/sui-2017-li" target="_blank"><i class="fas fa-link fa-fw"></i>sui-2017-li</a>  -  <a href="/publications/sui-2017-li.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">SUI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/sui-2017-li.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/sui-2017-li" target="_blank">Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models</a></h1><p class="meta"><span>Nico Li</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <span>Mario Costa Sousa</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We compare the effectiveness of 2D maps and 3D terrain models for visibility tasks and demonstrate how interactive dynamic viewsheds can improve performance for both types of terrain representations. In general, the two-dimensional nature of classic topographic maps limits their legibility and can make complex yet typical cartographic tasks like determining the visibility between locations difficult. Both 3D physical models and interactive techniques like dynamic viewsheds have the potential to improve viewers&#x27; understanding of topography, but their impact has not been deeply explored. We evaluate the effectiveness of 2D maps, 3D models, and interactive viewsheds for both simple and complex visibility tasks. Our results demonstrate the benefits of the dynamic viewshed technique and highlight opportunities for additional tactile interactions. Based on these findings we present guidelines for improving the design and usability of future topographic maps and models.</p><div class="ui labels">Keywords:  <span class="ui large label">terrain visualization</span><span class="ui large label">geospatial visualization</span><span class="ui large label">dynamic viewshed</span><span class="ui large label">topographic maps</span><span class="ui large label">tangible user interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nico Li<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Mario Costa Sousa<!-- -->. <b>Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models</b>. <i>In undefined (SUI &#x27;17)</i>. <!-- -->  Page: 1-<!-- -->9<!-- -->.  DOI: <a href="https://doi.org/10.1145/3131277.3132178" target="_blank">https://doi.org/10.1145/3131277.3132178</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="assets-2017-suzuki" class="ui large modal"><div class="header"><a href="/publications/assets-2017-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>assets-2017-suzuki</a>  -  <a href="/publications/assets-2017-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">ASSETS 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/assets-2017-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/assets-2017-suzuki" target="_blank">FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Abigale Stangl</span> , <span>Mark D. Gross</span> , <span>Tom Yeh</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/VbwIZ9V6i_g" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/VbwIZ9V6i_g?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/VbwIZ9V6i_g/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>For people with visual impairments, tactile graphics are an important means to learn and explore information. However, raised line tactile graphics created with traditional materials such as embossing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dynamic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily reconfigured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, feature identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as education and data exploration.</p><div class="ui labels">Keywords:  <span class="ui large label">visual impairment</span><span class="ui large label">dynamic tactile markers</span><span class="ui large label">tangible interfaces</span><span class="ui large label">interactive tactile graphics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Abigale Stangl<!-- -->, <!-- -->Mark D. Gross<!-- -->, <!-- -->Tom Yeh<!-- -->. <b>FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers</b>. <i>In undefined (ASSETS &#x27;20)</i>. <!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/3132525.3132548" target="_blank">https://doi.org/10.1145/3132525.3132548</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2017-mok" class="ui large modal"><div class="header"><a href="/publications/dis-2017-mok" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2017-mok</a>  -  <a href="/publications/dis-2017-mok.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2017-mok.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2017-mok" target="_blank">Critiquing Physical Prototypes for a Remote Audience</a></h1><p class="meta"><a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><strong>Terrance Mok</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present an observational study of physical prototype critique that highlights some of the challenges of communicating physical behaviors and materiality at a distance. Geographically distributed open hardware communities often conduct user feedback and peer critique sessions via video conference. However, people have difficulty using current video conferencing tools to demonstrate and critique physical designs. To examine the challenges of remote critique, we conducted an observational lab study in which participants critiqued pairs of physical prototypes (prosthetic hands) for a face-to-face or remote collaborator. In both conditions, participants&#x27; material experiences were an important part of their critique, however their attention was divided between interacting with the prototype and finding strategies to communicate `invisible&#x27; features. Based on our findings, we propose design implications for remote collaboration tools that support the sharing of material experiences and prototype critique.</p><div class="ui labels">Keywords:  <span class="ui large label">design review</span><span class="ui large label">prototype critique</span><span class="ui large label">remote collaboration</span><span class="ui large label">material experience</span><span class="ui large label">open hardware</span><span class="ui large label">video conferencing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Terrance Mok<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Critiquing Physical Prototypes for a Remote Audience</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;17)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3064663.3064722" target="_blank">https://doi.org/10.1145/3064663.3064722</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-aoki" class="ui large modal"><div class="header"><a href="/publications/chi-2017-aoki" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2017-aoki</a>  -  <a href="/publications/chi-2017-aoki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2017-aoki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-aoki" target="_blank">Environmental Protection and Agency: Motivations, Capacity, and Goals in Participatory Sensing</a></h1><p class="meta"><span>Paul Aoki</span> , <span>Allison Woodruff</span> , <span>Baladitya Yellapragada</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper we consider various genres of citizen science from the perspective of citizen participants. As a mode of scientific inquiry, citizen science has the potential to &quot;scale up&quot; scientific data collection efforts and increase lay engagement with science. However, current technological directions risk losing sight of the ways in which citizen science is actually practiced. As citizen science is increasingly used to describe a wide range of activities, we begin by presenting a framework of citizen science genres. We then present findings from four interlocking qualitative studies and technological interventions of community air quality monitoring efforts, examining the motivations and capacities of citizen participants and characterizing their alignment with different types of citizen science. Based on these studies, we suggest that data acquisition involves complex multi-dimensional tradeoffs, and the commonly held view that citizen science systems are a win-win for citizens and science may be overstated.</p><div class="ui labels">Keywords:  <span class="ui large label">citizen science</span><span class="ui large label">environmental sensing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Paul Aoki<!-- -->, <!-- -->Allison Woodruff<!-- -->, <!-- -->Baladitya Yellapragada<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Environmental Protection and Agency: Motivations, Capacity, and Goals in Participatory Sensing</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;17)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3025453.3025667" target="_blank">https://doi.org/10.1145/3025453.3025667</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-hull" class="ui large modal"><div class="header"><a href="/publications/chi-2017-hull" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2017-hull</a>  -  <a href="/publications/chi-2017-hull.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2017-hull.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-hull" target="_blank">Building with Data: Architectural Models as Inspiration for Data Physicalization</a></h1><p class="meta"><a href="/people/carmen-hull"><img src="/static/images/people/carmen-hull.jpg" class="ui circular spaced image mini-profile"/><strong>Carmen Hull</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/bkqLNgYIXek" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/bkqLNgYIXek?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/bkqLNgYIXek/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>In this paper we analyze the role of physical scale models in the architectural design process and apply insights from architecture for the creation and use of data physicalizations. Based on a survey of the architecture literature on model making and ten interviews with practicing architects, we describe the role of physical models as a tool for exploration and communication. From these observations, we identify trends in the use of physical models in architecture, which have the potential to inform the design of data physicalizations. We identify four functions of architectural modeling that can be directly adapted for use in the process of building rich data models. Finally, we discuss how the visualization community can apply observations from architecture to the design of new data physicalizations.</p><div class="ui labels">Keywords:  <span class="ui large label">design process</span><span class="ui large label">architectural models</span><span class="ui large label">data physicalization</span><span class="ui large label">embodied interaction</span><span class="ui large label">data visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Carmen Hull<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Building with Data: Architectural Models as Inspiration for Data Physicalization</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;17)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3025453.3025850" target="_blank">https://doi.org/10.1145/3025453.3025850</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-ledo" class="ui large modal"><div class="header"><a href="/publications/chi-2017-ledo" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2017-ledo</a>  -  <a href="/publications/chi-2017-ledo.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2017-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-ledo" target="_blank">Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices</a></h1><p class="meta"><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <span>Fraser Anderson</span> , <span>Ryan Schmidt</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><strong>Saul Greenberg</strong></a> , <span>Tovi Grossman</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Interactive, smart objects – customized to individuals and uses – are central to many movements, such as tangibles, the internet of things (IoT), and ubiquitous computing. Yet, rapid prototyping both the form and function of these custom objects can be problematic, particularly for those with limited electronics or programming experience. Designers often need to embed custom circuitry; program its workings; and create a form factor that not only reflects the desired user experience but can also house the required circuitry and electronics. To mitigate this, we created Pineal, a design tool that lets end-users: (1) modify 3D models to include a smart watch or phone as its heart; (2) specify high-level interactive behaviours through visual programming; and (3) have the phone or watch act out such behaviours as the objects&#x27; &quot;smarts&quot;. Furthermore, a series of prototypes show how Pineal exploits mobile sensing and output, and automatically generates 3D printed form-factors for rich, interactive, objects.</p><div class="ui labels">Keywords:  <span class="ui large label">fabrication</span><span class="ui large label">3d printing</span><span class="ui large label">smart objects</span><span class="ui large label">rapid prototyping</span><span class="ui large label">toolkits</span><span class="ui large label">prototyping tool</span><span class="ui large label">interaction design</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Fraser Anderson<!-- -->, <!-- -->Ryan Schmidt<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Saul Greenberg<!-- -->, <!-- -->Tovi Grossman<!-- -->. <b>Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;17)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/3025453.3025652" target="_blank">https://doi.org/10.1145/3025453.3025652</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-somanath" class="ui large modal"><div class="header"><a href="/publications/chi-2017-somanath" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2017-somanath</a>  -  <a href="/publications/chi-2017-somanath.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2017-somanath.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-somanath" target="_blank">&#x27;Maker&#x27; within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India</a></h1><p class="meta"><a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Janette Hughes</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <span>Mario Costa Sousa</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/NpIME1h1mH8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/NpIME1h1mH8?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/NpIME1h1mH8/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Do-it-yourself (DIY) inspired activities have gained popularity as a means of creative expression and self-directed learning. However, DIY culture is difficult to implement in places with limited technology infrastructure and traditional learning cultures. Our goal is to understand how learners in such a setting react to DIY activities. We present observations from a physical computing workshop with 12 students (13-15 years old) conducted at a high school in India. We observed unique challenges for these students when tackling DIY activities: a high monetary and psychological cost to exploration, limited independent learning resources, difficulties with finding intellectual courage and assumed technical language proficiency. Our participants, however, overcome some of these challenges by adopting their own local strategies: resilience, nonverbal and verbal learning techniques, and creating documentation and fallback circuit versions. Based on our findings, we discuss a set of lessons learned about makerspaces in a context with socio-technical challenges.</p><div class="ui labels">Keywords:  <span class="ui large label">India</span><span class="ui large label">HCI4D</span><span class="ui large label">physical computing</span><span class="ui large label">DIY</span><span class="ui large label">young learners</span><span class="ui large label">maker culture</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sowmya Somanath<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Janette Hughes<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Mario Costa Sousa<!-- -->. <b>&#x27;Maker&#x27; within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;17)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3025453.3025849" target="_blank">https://doi.org/10.1145/3025453.3025849</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2017-goffin" class="ui large modal"><div class="header"><a href="/publications/tvcg-2017-goffin" target="_blank"><i class="fas fa-link fa-fw"></i>tvcg-2017-goffin</a>  -  <a href="/publications/tvcg-2017-goffin.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TVCG 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2017-goffin.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2017-goffin" target="_blank">An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents</a></h1><p class="meta"><span>Pascal Goffin</span> , <span>Jeremy Boy</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <span>Petra Isenberg</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/230834366" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/230834366?autoplay=1&gt;&lt;img src=undefined&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We contribute an investigation of the design and function of word-scale graphics and visualizations embedded in text documents. Word-scale graphics include both data-driven representations such as word-scale visualizations and sparklines, and non-data-driven visual marks. Their design, function, and use has so far received little research attention. We present the results of an open ended exploratory study with nine graphic designers. The study resulted in a rich collection of different types of graphics, data provenance, and relationships between text, graphics, and data. Based on this corpus, we present a systematic overview of word-scale graphic designs, and examine how designers used them. We also discuss the designers&#x27; goals in creating their graphics, and characterize how they used word-scale graphics to visualize data, add emphasis, and create alternative narratives. Building on these examples, we discuss implications for the design of authoring tools for word-scale graphics and visualizations, and explore how new authoring environments could make it easier for designers to integrate them into documents.</p><div class="ui labels">Keywords:  <span class="ui large label">word-scale visualization</span><span class="ui large label">word-scale graphic</span><span class="ui large label">text visualization</span><span class="ui large label">sparklines</span><span class="ui large label">authoring tool</span><span class="ui large label">information visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Pascal Goffin<!-- -->, <!-- -->Jeremy Boy<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Petra Isenberg<!-- -->. <b>An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents</b>. <i>In IEEE Transactions on Visualization and Computer Graphics (TVCG &#x27;17)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1109/TVCG.2016.2618797" target="_blank">https://doi.org/10.1109/TVCG.2016.2618797</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2017-willett" class="ui large modal"><div class="header"><a href="/publications/tvcg-2017-willett" target="_blank"><i class="fas fa-link fa-fw"></i>tvcg-2017-willett</a>  -  <a href="/publications/tvcg-2017-willett.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TVCG 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2017-willett.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2017-willett" target="_blank">Embedded Data Representations</a></h1><p class="meta"><a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <span>Yvonne Jansen</span> , <span>Pierre Dragicevic</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/182971005" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/182971005?autoplay=1&gt;&lt;img src=undefined&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce embedded data representations, the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents – the real-world entities and spaces to which data corresponds – and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations, which display data in proximity to data referents, and embedded representations, which display data so that it spatially coincides with data referents. Drawing on examples from visualization, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and physicalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.</p><div class="ui labels">Keywords:  <span class="ui large label">information visualization</span><span class="ui large label">data physicalization</span><span class="ui large label">ambient displays</span><span class="ui large label">ubiquitous computing</span><span class="ui large label">augmented reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Wesley Willett<!-- -->, <!-- -->Yvonne Jansen<!-- -->, <!-- -->Pierre Dragicevic<!-- -->. <b>Embedded Data Representations</b>. <i>In IEEE Transactions on Visualization and Computer Graphics (TVCG &#x27;17)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1109/TVCG.2016.2598608" target="_blank">https://doi.org/10.1109/TVCG.2016.2598608</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2016-jones" class="ui large modal"><div class="header"><a href="/publications/dis-2016-jones" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2016-jones</a>  -  <a href="/publications/dis-2016-jones.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2016</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2016-jones.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2016-jones" target="_blank">Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</a></h1><p class="meta"><a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><strong>Brennan Jones</strong></a> , <span>Kody Dillman</span> , <span>Richard Tang</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Carman Neustaedter</span> , <span>Scott Bateman</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/10hbJHIQVX8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/10hbJHIQVX8?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/10hbJHIQVX8/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>People are increasingly using mobile video to communicate, collaborate, and share experiences while on the go. Yet this presents challenges in adequately sharing camera views with remote users. In this paper, we study the use of semi-autonomous drones for video conferencing, where an outdoor user (using a smartphone) is connected to a desktop user who can explore the environment from the drone&#x27;s perspective. We describe findings from a study where pairs collaborated to complete shared navigation and search tasks. We illustrate the benefits of providing the desktop user with a view that is elevated, manipulable, and decoupled from the outdoor user. In addition, we articulate how participants overcame challenges in communicating environmental information and navigational cues, negotiated control of the view, and used the drone as a tool for sharing experiences. This provides a new way of thinking about mobile video conferencing where cameras that are decoupled from both users play an integral role in communication, collaboration, and sharing experiences.</p><div class="ui labels">Keywords:  <span class="ui large label">cscw</span><span class="ui large label">telepresence</span><span class="ui large label">video communication</span><span class="ui large label">shared experiences</span><span class="ui large label">teleoperation</span><span class="ui large label">drones</span><span class="ui large label">collaboration</span><span class="ui large label">hri</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Brennan Jones<!-- -->, <!-- -->Kody Dillman<!-- -->, <!-- -->Richard Tang<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Carman Neustaedter<!-- -->, <!-- -->Scott Bateman<!-- -->. <b>Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;16)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/2901790.2901847" target="_blank">https://doi.org/10.1145/2901790.2901847</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2016-lopez" class="ui large modal"><div class="header"><a href="/publications/tvcg-2016-lopez" target="_blank"><i class="fas fa-link fa-fw"></i>tvcg-2016-lopez</a>  -  <a href="/publications/tvcg-2016-lopez.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TVCG 2016</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2016-lopez.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2016-lopez" target="_blank">Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration</a></h1><p class="meta"><span>David Lopez</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Candemir Doger</span> , <span>Tobias Isenberg</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/jBtHgTYpJl0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/jBtHgTYpJl0?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/jBtHgTYpJl0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We discuss touch-based navigation of 3D visualizations in a combined monoscopic and stereoscopic viewing environment. We identify a set of interaction modes, and a workflow that helps users transition between these modes to improve their interaction experience. In our discussion we analyze, in particular, the control-display space mapping between the different reference frames of the stereoscopic and monoscopic displays. We show how this mapping supports interactive data exploration, but may also lead to conflicts between the stereoscopic and monoscopic views due to users&#x27; movement in space; we resolve these problems through synchronization. To support our discussion, we present results from an exploratory observational evaluation with domain experts in fluid mechanics and structural biology. These experts explored domain-specific datasets using variations of a system that embodies the interaction modes and workflows; we report on their interactions and qualitative feedback on the system and its workflow.</p><div class="ui labels">Keywords:  <span class="ui large label">visualization of 3D data</span><span class="ui large label">human-computer interaction</span><span class="ui large label">expert interaction</span><span class="ui large label">direct-touch input</span><span class="ui large label">mobile displays</span><span class="ui large label">stereoscopic environments</span><span class="ui large label">VR</span><span class="ui large label">AR</span><span class="ui large label">conceptual model of interaction</span><span class="ui large label">interaction reference frame mapping</span><span class="ui large label">observational study</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Lopez<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Candemir Doger<!-- -->, <!-- -->Tobias Isenberg<!-- -->. <b>Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration</b>. <i>In IEEE Transactions on Visualization and Computer Graphics (TVCG &#x27;16)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1109/TVCG.2015.2440233" target="_blank">https://doi.org/10.1109/TVCG.2015.2440233</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mobilehci-2015-ledo" class="ui large modal"><div class="header"><a href="/publications/mobilehci-2015-ledo" target="_blank"><i class="fas fa-link fa-fw"></i>mobilehci-2015-ledo</a>  -  <a href="/publications/mobilehci-2015-ledo.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">MobileHCI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/mobilehci-2015-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/mobilehci-2015-ledo" target="_blank">Proxemic-Aware Controls: Designing Remote Controls for Ubiquitous Computing Ecologies</a></h1><p class="meta"><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><strong>Saul Greenberg</strong></a> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><strong>Nicolai Marquardt</strong></a> , <span>Sebastian Boring</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/1AlMUmD6E3U" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/1AlMUmD6E3U?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/1AlMUmD6E3U/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Remote controls facilitate interactions at-a-distance with appliances. However, the complexity, diversity, and increasing number of digital appliances in ubiquitous computing ecologies make it increasingly difficult to: (1) discover which appliances are controllable; (2) select a particular appliance from the large number available; (3) view information about its status; and (4) control the appliance in a pertinent manner. To mitigate these problems we contribute proxemic-aware controls, which exploit the spatial relationships between a person&#x27;s handheld device and all surrounding appliances to create a dynamic appliance control interface. Specifically, a person can discover and select an appliance by the way one orients a mobile device around the room, and then progressively view the appliance&#x27;s status and control its features in increasing detail by simply moving towards it. We illustrate proxemic-aware controls of assorted appliances through various scenarios. We then provide a generalized conceptual framework that informs future designs of proxemic-aware controls.</p><div class="ui labels">Keywords:  <span class="ui large label">ubiquitous computing</span><span class="ui large label">proxemic-interaction</span><span class="ui large label">mobile interaction</span><span class="ui large label">control of appliances</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Saul Greenberg<!-- -->, <!-- -->Nicolai Marquardt<!-- -->, <!-- -->Sebastian Boring<!-- -->. <b>Proxemic-Aware Controls: Designing Remote Controls for Ubiquitous Computing Ecologies</b>. <i>In Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &#x27;15)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/2785830.2785871" target="_blank">https://doi.org/10.1145/2785830.2785871</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-oehlberg" class="ui large modal"><div class="header"><a href="/publications/chi-2015-oehlberg" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2015-oehlberg</a>  -  <a href="/publications/chi-2015-oehlberg.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-oehlberg.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-oehlberg" target="_blank">Patterns of Physical Design Remixing in Online Maker Communities</a></h1><p class="meta"><a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <span>Wendy E. Mackay</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/bkqLNgYIXek" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/bkqLNgYIXek?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/bkqLNgYIXek/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Makers participate in remixing culture by drawing inspiration from, combining, and adapting designs for physical objects. To examine how makers remix each others&#x27; designs on a community scale, we analyzed metadata from over 175,000 digital designs from Thingiverse, the largest online design community for digital fabrication. Remixed designs on Thingiverse are predominantly generated designs from Customizer a built-in web app for adjusting parametric designs. However, we find that these designs do not elicit subsequent user activity and the authors who generate them tend not to contribute additional content to Thingiverse. Outside of Customizer, influential sources of remixing include complex assemblies and design primitives, as well as non-physical resources posing as physical designs. Building on our findings, we discuss ways in which online maker communities could become more than just design repositories and better support collaborative remixing.</p><div class="ui labels">Keywords:  <span class="ui large label">customization</span><span class="ui large label">maker communities</span><span class="ui large label">user innovation</span><span class="ui large label">collaboration</span><span class="ui large label">hacking</span><span class="ui large label">remixing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Lora Oehlberg<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Wendy E. Mackay<!-- -->. <b>Patterns of Physical Design Remixing in Online Maker Communities</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;15)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/2702123.2702175" target="_blank">https://doi.org/10.1145/2702123.2702175</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-aseniero" class="ui large modal"><div class="header"><a href="/publications/chi-2015-aseniero" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2015-aseniero</a>  -  <a href="/publications/chi-2015-aseniero.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-aseniero.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-aseniero" target="_blank">Stratos: Using Visualization to Support Decisions in Strategic Software Release Planning</a></h1><p class="meta"><a href="/people/bon-adriel-aseniero"><img src="/static/images/people/bon-adriel-aseniero.jpg" class="ui circular spaced image mini-profile"/><strong>Bon Adriel Aseniero</strong></a> , <span>Tiffany Wun</span> , <a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <span>Guenther Ruhe</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/qm57aHjTAYc" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/qm57aHjTAYc?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/qm57aHjTAYc/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Software is typically developed incrementally and released in stages. Planning these releases involves deciding which features of the system should be implemented for each release. This is a complex planning process involving numerous trade-offs-constraints and factors that often make decisions difficult. Since the success of a product depends on this plan, it is important to understand the trade-offs between different release plans in order to make an informed choice. We present STRATOS, a tool that simultaneously visualizes several software release plans. The visualization shows several attributes about each plan that are important to planners. Multiple plans are shown in a single layout to help planners find and understand the trade-offs between alternative plans. We evaluated our tool via a qualitative study and found that STRATOS enables a range of decision-making processes, helping participants decide on which plan is most optimal.</p><div class="ui labels">Keywords:  <span class="ui large label">software engineering</span><span class="ui large label">information visualization</span><span class="ui large label">release planning</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Bon Adriel Aseniero<!-- -->, <!-- -->Tiffany Wun<!-- -->, <!-- -->David Ledo<!-- -->, <!-- -->Guenther Ruhe<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->. <b>Stratos: Using Visualization to Support Decisions in Strategic Software Release Planning</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;15)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/2702123.2702426" target="_blank">https://doi.org/10.1145/2702123.2702426</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-jones" class="ui large modal"><div class="header"><a href="/publications/chi-2015-jones" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2015-jones</a>  -  <a href="/publications/chi-2015-jones.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-jones.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-jones" target="_blank">Mechanics of Camera Work in Mobile Video Collaboration</a></h1><p class="meta"><a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><strong>Brennan Jones</strong></a> , <span>Anna Witcraft</span> , <span>Scott Bateman</span> , <span>Carman Neustaedter</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Mobile video conferencing, where one or more participants are moving about in the real world, enables entirely new interaction scenarios (e.g., asking for help to construct or repair an object, or showing a physical location). While we have a good understanding of the challenges of video conferencing in office or home environments, we do not fully understand the mechanics of camera work-how people use mobile devices to communicate with one another-during mobile video calls. To provide an understanding of what people do in mobile video collaboration, we conducted an observational study where pairs of participants completed tasks using a mobile video conferencing system. Our analysis suggests that people use the camera view deliberately to support their interactions-for example, to convey a message or to ask questions-but the limited field of view, and the lack of camera control can make it a frustrating experience.</p><div class="ui labels">Keywords:  <span class="ui large label">video communication</span><span class="ui large label">collaboration</span><span class="ui large label">mobile computing</span><span class="ui large label">handheld devices</span><span class="ui large label">cscw</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Brennan Jones<!-- -->, <!-- -->Anna Witcraft<!-- -->, <!-- -->Scott Bateman<!-- -->, <!-- -->Carman Neustaedter<!-- -->, <!-- -->Anthony Tang<!-- -->. <b>Mechanics of Camera Work in Mobile Video Collaboration</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;15)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/2702123.2702345" target="_blank">https://doi.org/10.1145/2702123.2702345</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-willett" class="ui large modal"><div class="header"><a href="/publications/chi-2015-willett" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2015-willett</a>  -  <a href="/publications/chi-2015-willett.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-willett.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-willett" target="_blank">Lightweight Relief Shearing for Enhanced Terrain Perception on Interactive Maps</a></h1><p class="meta"><a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <span>Bernhard Jenny</span> , <span>Tobias Isenberg</span> , <span>Pierre Dragicevic</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We explore interactive relief shearing, a set of non-intrusive, direct manipulation interactions that expose depth and shape information in terrain maps using ephemeral animations. Reading and interpreting topography and relief on terrain maps is an important aspect of map use, but extracting depth information from 2D maps is notoriously difficult. Modern mapping software attempts to alleviate this limitation by presenting digital terrain using 3D views. However, 3D views introduce occlusion, complicate distance estimations, and typically require more complex interactions. In contrast, our approach reveals depth information via shearing animations on 2D maps, and can be paired with existing interactions such as pan and zoom. We examine explicit, integrated, and hybrid interactions for triggering relief shearing and present a version that uses device tilt to control depth effects. Our evaluation shows that these interactive techniques improve depth perception when compared to standard 2D and perspective views.</p><div class="ui labels">Keywords:  <span class="ui large label">plan oblique relief</span><span class="ui large label">interaction</span><span class="ui large label">depth perception</span><span class="ui large label">terrain maps</span><span class="ui large label">relief shearing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Wesley Willett<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Tobias Isenberg<!-- -->, <!-- -->Pierre Dragicevic<!-- -->. <b>Lightweight Relief Shearing for Enhanced Terrain Perception on Interactive Maps</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;15)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/2702123.2702172" target="_blank">https://doi.org/10.1145/2702123.2702172</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img style="max-width:180px;margin:30px auto" src="/static/images/logo-6.png"/><div class="content"><img style="max-width:200px;margin:0px auto" src="/static/images/logo-4.png"/><div class="sub header">Department of Computer Science</div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"id":"publications","title":"Publications"}},"page":"/page","query":{"id":"publications"},"buildId":"6bjRBjipLrjz-covNxi6J","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/page" src="/_next/static/6bjRBjipLrjz-covNxi6J/pages/page.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/6bjRBjipLrjz-covNxi6J/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.98f1a4e4c94db9943918.js" async=""></script><script src="/_next/static/runtime/main-22d58f57abab872d6e70.js" async=""></script></body></html>