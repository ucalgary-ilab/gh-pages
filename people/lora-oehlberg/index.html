<!DOCTYPE html><html><head><link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.css" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/css/lightbox.css" rel="stylesheet"/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link href="/static/css/style.css" rel="stylesheet"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox-plus-jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              $('.ui.sidebar')
                .sidebar('attach events', '.sidebar.icon')

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><meta charSet="utf-8" class="next-head"/><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><link rel="preload" href="/_next/static/nRxFzcfYt4k13GmMYtYpY/pages/person.js" as="script"/><link rel="preload" href="/_next/static/nRxFzcfYt4k13GmMYtYpY/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.98f1a4e4c94db9943918.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-22d58f57abab872d6e70.js" as="script"/></head><body><div id="__next"><div><title>Lora Oehlberg - Interactions Lab | University of Calgary HCI Group</title><div><div class="ui right vertical sidebar menu"><a class="item" href="/">Home</a><a class="item" href="/publications">Publications</a><a class="item active" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/news">News</a><a class="item" href="/location">Location</a></div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/"><b>UCalgary iLab</b></a></div><div class="right menu"><a class="item" href="/publications">Publications</a><a class="item active" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/news">News</a><a class="item" href="/location">Location</a><div class="toc item"><a href="/"><b>UCalgary iLab</b></a><i style="float:right" class="sidebar icon"></i></div></div></div></div><div class="ui stackable grid"><div class="one wide column"></div><div class="eleven wide column centered"><div id="person" class="category" style="text-align:center"><img class="ui circular image large-profile" src="/static/images/people/lora-oehlberg.jpg" style="margin:auto"/><h1>Lora Oehlberg</h1><p>Associate Professor</p><p><a href="https://pages.cpsc.ucalgary.ca/~lora.oehlberg/" target="_blank"><i class="fas fa-link fa-fw"></i>https://pages.cpsc.ucalgary.ca/~lora.oehlberg/</a></p><p><a href="https://scholar.google.ca/citations?hl=en&amp;user=8GzaBdwAAAAJ" target="_blank"><i class="fas fa-graduation-cap fa-fw"></i>Google Scholar</a></p><div class="ui horizontal small divided link list"></div></div><div id="publications" class="category"><h1 class="ui horizontal divider header"><i class="file alternate outline icon"></i>Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="chi-2020-anjani"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b></p><p><span>Laurensia Anjani</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Terrance Mok</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Wooi Boon Goh</span></p><p><div class="ui labels"><span class="ui label">video streams</span><span class="ui label">mukbang</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-hou"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2020</span></p><p class="color" style="font-size:1.3em"><b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b></p><p><span>Ming Hou</span> , <a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Karthik Mahadevan</span></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p></p></div></div><div class="publication ui vertical segment stackable grid" data-id="mobilehci-2019-hung"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/mobilehci-2019-hung.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">MobileHCI 2019</span></p><p class="color" style="font-size:1.3em"><b>WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</b></p><p><a href="/people/michael-hung"><img src="/static/images/people/michael-hung.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Michael Hung</span></a> , <a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui labels"><span class="ui label">smartwatch</span><span class="ui label">cross-device interaction</span><span class="ui label">pen interaction</span><span class="ui label">interaction techniques</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-ledo"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2019</span></p><p class="color" style="font-size:1.3em"><b>Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</b></p><p><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <span>Jo Vermeulen</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Saul Greenberg</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Sebastian Boring</span></p><p><div class="ui labels"><span class="ui label">smart objects</span><span class="ui label">mobile interfaces</span><span class="ui label">prototyping</span><span class="ui label">design tool</span><span class="ui label">interactive behaviour</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2019-mikalauskas"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-mikalauskas.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2019</span></p><p class="color" style="font-size:1.3em"><b>Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</b></p><p><span>Claire Mikalauskas</span> , <span>April Viczko</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui labels"><span class="ui label">props</span><span class="ui label">performer-controlled technology</span><span class="ui label">improvisational theatre</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2019-wun"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-wun.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TEI 2019</span></p><p class="color" style="font-size:1.3em"><b>You say Potato, I say Po-Data: Physical Template Tools for Authoring Visualizations</b></p><p><span>Tiffany Wun</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Miriam Sturdee</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a></p><p><div class="ui labels"><span class="ui label">potato</span><span class="ui label">tangible tools</span><span class="ui label">authoring visualizations</span><span class="ui label">block-printing</span><span class="ui label">physical template tools</span><span class="ui label">information visualization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2018-mikalauskas"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2018-mikalauskas.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2018</span></p><p class="color" style="font-size:1.3em"><b>Improvising with an Audience-Controlled Robot Performer</b></p><p><span>Claire Mikalauskas</span> , <span>Tiffany Wun</span> , <span>Kevin Ta</span> , <span>Joshua Horacsek</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui labels"><span class="ui label">human-robot interaction</span><span class="ui label">improvised theatre</span><span class="ui label">creativity-support tools</span><span class="ui label">crowdsourcing</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2018-ta"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2018-ta.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2018</span></p><p class="color" style="font-size:1.3em"><b>Bod-IDE: An Augmented Reality Sandbox for eFashion Garments</b></p><p><span>Kevin Ta</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui labels"><span class="ui label">augmented reality</span><span class="ui label">electronic fashion</span><span class="ui label">creativity support tool</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-dillman"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-dillman.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</b></p><p><span>Kody R. Dillman</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Terrance Mok</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Alex Mitchell</span></p><p><div class="ui labels"><span class="ui label">game design</span><span class="ui label">guidance</span><span class="ui label">interaction cues</span><span class="ui label">augmented reality</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-feick"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span><span class="ui big label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</b></p><p><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Martin Feick</span></a> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p><div class="ui labels"><span class="ui label">cscw</span><span class="ui label">remote collaboration</span><span class="ui label">object-focused collaboration</span><span class="ui label">physical telepresence</span><span class="ui label">collaborative physical tasks</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-ledo"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2018</span></p><p class="color" style="font-size:1.3em"><b>Evaluation Strategies for HCI Toolkit Research</b></p><p><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <span>Steven Houben</span> , <span>Jo Vermeulen</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nicolai Marquardt</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Saul Greenberg</span></a></p><p><div class="ui labels"><span class="ui label">user interfaces</span><span class="ui label">design</span><span class="ui label">evaluation</span><span class="ui label">prototyping</span><span class="ui label">toolkits</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="hri-2018-feick"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">HRI 2018</span></p><p class="color" style="font-size:1.3em"><b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b></p><p><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Martin Feick</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>André Miede</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p><p><div class="ui labels"><span class="ui label">movement trajectory &amp; velocity</span><span class="ui label">remote collaboration</span><span class="ui label">robot surrogate</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2017-mok"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2017-mok.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2017</span></p><p class="color" style="font-size:1.3em"><b>Critiquing Physical Prototypes for a Remote Audience</b></p><p><a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Terrance Mok</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui labels"><span class="ui label">design review</span><span class="ui label">prototype critique</span><span class="ui label">remote collaboration</span><span class="ui label">material experience</span><span class="ui label">open hardware</span><span class="ui label">video conferencing</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-somanath"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2017-somanath.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>&#x27;Maker&#x27; within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India</b></p><p><a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Janette Hughes</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <span>Mario Costa Sousa</span></p><p><div class="ui labels"><span class="ui label">India</span><span class="ui label">HCI4D</span><span class="ui label">physical computing</span><span class="ui label">DIY</span><span class="ui label">young learners</span><span class="ui label">maker culture</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2017-ledo"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2017-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2017</span></p><p class="color" style="font-size:1.3em"><b>Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices</b></p><p><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <span>Fraser Anderson</span> , <span>Ryan Schmidt</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Saul Greenberg</span></a> , <span>Tovi Grossman</span></p><p><div class="ui labels"><span class="ui label">fabrication</span><span class="ui label">3d printing</span><span class="ui label">smart objects</span><span class="ui label">rapid prototyping</span><span class="ui label">toolkits</span><span class="ui label">prototyping tool</span><span class="ui label">interaction design</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2016-jones"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2016-jones.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2016</span></p><p class="color" style="font-size:1.3em"><b>Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</b></p><p><a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Brennan Jones</span></a> , <span>Kody Dillman</span> , <span>Richard Tang</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Carman Neustaedter</span> , <span>Scott Bateman</span></p><p><div class="ui labels"><span class="ui label">cscw</span><span class="ui label">telepresence</span><span class="ui label">video communication</span><span class="ui label">shared experiences</span><span class="ui label">teleoperation</span><span class="ui label">drones</span><span class="ui label">collaboration</span><span class="ui label">hri</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2016-lopez"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2016-lopez.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TVCG 2016</span></p><p class="color" style="font-size:1.3em"><b>Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration</b></p><p><span>David Lopez</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Candemir Doger</span> , <span>Tobias Isenberg</span></p><p><div class="ui labels"><span class="ui label">visualization of 3D data</span><span class="ui label">human-computer interaction</span><span class="ui label">expert interaction</span><span class="ui label">direct-touch input</span><span class="ui label">mobile displays</span><span class="ui label">stereoscopic environments</span><span class="ui label">VR</span><span class="ui label">AR</span><span class="ui label">conceptual model of interaction</span><span class="ui label">interaction reference frame mapping</span><span class="ui label">observational study</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2015-oehlberg"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-oehlberg.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2015</span></p><p class="color" style="font-size:1.3em"><b>Patterns of Physical Design Remixing in Online Maker Communities</b></p><p><a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <span>Wendy E. Mackay</span></p><p><div class="ui labels"><span class="ui label">customization</span><span class="ui label">maker communities</span><span class="ui label">user innovation</span><span class="ui label">collaboration</span><span class="ui label">hacking</span><span class="ui label">remixing</span></div></p></div></div></div><div id="publications-modal"><div id="chi-2020-anjani" class="ui large modal"><div class="header"><a href="/publications/chi-2020-anjani" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-anjani</a>  -  <a href="/publications/chi-2020-anjani.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-anjani" target="_blank">Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</a></h1><p class="meta"><span>Laurensia Anjani</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><strong>Terrance Mok</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Wooi Boon Goh</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present a mixed-methods study of viewers on their practices and motivations around watching mukbang — video streams of people eating large quantities of food. Viewers&#x27; experiences provide insight on future technologies for multisensorial video streams and technology-supported commensality (eating with others). We surveyed 104 viewers and interviewed 15 of them about their attitudes and reflections on their mukbang viewing habits, their physiological aspects of watching someone eat, and their perceived social relationship with mukbangers. Based on our findings, we propose design implications for remote commensality, and for synchronized multisensorial video streaming content.</p><div class="ui labels">Keywords:  <span class="ui large label">video streams</span><span class="ui large label">mukbang</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Laurensia Anjani<!-- -->, <!-- -->Terrance Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Wooi Boon Goh<!-- -->. <b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3313831.3376567" target="_blank">https://doi.org/10.1145/3313831.3376567</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2020-hou" class="ui large modal"><div class="header"><a href="/publications/chi-2020-hou" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-hou</a>  -  <a href="/publications/chi-2020-hou.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2020-hou" target="_blank">Autonomous Vehicle-Cyclist Interaction: Peril and Promise</a></h1><p class="meta"><span>Ming Hou</span> , <a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><strong>Karthik Mahadevan</strong></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p></p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ming Hou<!-- -->, <!-- -->Karthik Mahadevan<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="mobilehci-2019-hung" class="ui large modal"><div class="header"><a href="/publications/mobilehci-2019-hung" target="_blank"><i class="fas fa-link fa-fw"></i>mobilehci-2019-hung</a>  -  <a href="/publications/mobilehci-2019-hung.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">MobileHCI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/mobilehci-2019-hung.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/mobilehci-2019-hung" target="_blank">WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</a></h1><p class="meta"><a href="/people/michael-hung"><img src="/static/images/people/michael-hung.jpg" class="ui circular spaced image mini-profile"/><strong>Michael Hung</strong></a> , <a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Pen-based input is often treated as auxiliary to mobile devices. We posit that cross-device interactions can inspire and extend the design space of pen-based interactions into new, expressive directions. We realize this through WatchPen, a smartwatch mounted on a passive, capacitive stylus that: (1) senses the usage context and leverages it for expression (e.g., changing colour), (2) contains tools and parameters within the display, and (3) acts as an on-demand output. As a result, it provides users with a dynamic relationship between inputs and outputs, awareness of current tool selection and parameters, and increased expressive match (e.g., added ability to mimic physical tools, showing clipboard contents). We discuss and reflect upon a series of interaction techniques that demonstrate WatchPen within a drawing application. We highlight the expressive power of leveraging multiple sensing and output capabilities across both the watch-augmented stylus and the tablet surface.</p><div class="ui labels">Keywords:  <span class="ui large label">smartwatch</span><span class="ui large label">cross-device interaction</span><span class="ui large label">pen interaction</span><span class="ui large label">interaction techniques</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Michael Hung<!-- -->, <!-- -->David Ledo<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</b>. <i>In Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->8<!-- -->.  DOI: <a href="https://doi.org/10.1145/3338286.3340122" target="_blank">https://doi.org/10.1145/3338286.3340122</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2019-ledo" class="ui large modal"><div class="header"><a href="/publications/dis-2019-ledo" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2019-ledo</a>  -  <a href="/publications/dis-2019-ledo.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2019-ledo" target="_blank">Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</a></h1><p class="meta"><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <span>Jo Vermeulen</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><strong>Saul Greenberg</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Sebastian Boring</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Astral is a prototyping tool for authoring mobile and smart object interactive behaviours. It mirrors selected display contents of desktop applications onto mobile devices (smartphones and smartwatches), and streams/remaps mobile sensor data to desktop input events (mouse or keyboard) to manipulate selected desktop contents. This allows designers to use familiar desktop applications (e.g. PowerPoint, AfterEffects) to prototype rich interactive behaviours. Astral combines and integrates display mirroring, sensor streaming and input remapping, where designers can exploit familiar desktop applications to prototype, explore and fine-tune dynamic interactive behaviours. With Astral, designers can visually author rules to test real-time behaviours while interactions take place, as well as after the interaction has occurred. We demonstrate Astral&#x27;s applicability, workflow and expressiveness within the interaction design process through both new examples and replication of prior approaches that illustrate how various familiar desktop applications are leveraged and repurposed.</p><div class="ui labels">Keywords:  <span class="ui large label">smart objects</span><span class="ui large label">mobile interfaces</span><span class="ui large label">prototyping</span><span class="ui large label">design tool</span><span class="ui large label">interactive behaviour</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Saul Greenberg<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Sebastian Boring<!-- -->. <b>Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->14<!-- -->.  DOI: <a href="https://doi.org/10.1145/3322276.3322329" target="_blank">https://doi.org/10.1145/3322276.3322329</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2019-mikalauskas" class="ui large modal"><div class="header"><a href="/publications/tei-2019-mikalauskas" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2019-mikalauskas</a>  -  <a href="/publications/tei-2019-mikalauskas.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-mikalauskas.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2019-mikalauskas" target="_blank">Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</a></h1><p class="meta"><span>Claire Mikalauskas</span> , <span>April Viczko</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>While improvised theatre (improv) is often performed on a bare stage, improvisers sometimes incorporate physical props to inspire new directions for a scene and to enrich their performance. A tech booth can improvise light and sound technical elements, but coordinating with improvisers&#x27; actions on-stage is challenging. Our goal is to inform the design of an augmented prop that lets improvisers tangibly control light and sound technical elements while performing. We interviewed five professional improvisers about their use of physical props in improv, and their expectations of a possible augmented prop that controls technical theatre elements. We propose a set of guidelines for the design of an augmented prop that fits with the existing world of unpredictable improvised performance.</p><div class="ui labels">Keywords:  <span class="ui large label">props</span><span class="ui large label">performer-controlled technology</span><span class="ui large label">improvisational theatre</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Claire Mikalauskas<!-- -->, <!-- -->April Viczko<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</b>. <i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->9<!-- -->.  DOI: <a href="https://doi.org/10.1145/3294109.3295631" target="_blank">https://doi.org/10.1145/3294109.3295631</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tei-2019-wun" class="ui large modal"><div class="header"><a href="/publications/tei-2019-wun" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2019-wun</a>  -  <a href="/publications/tei-2019-wun.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2019</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-wun.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tei-2019-wun" target="_blank">You say Potato, I say Po-Data: Physical Template Tools for Authoring Visualizations</a></h1><p class="meta"><span>Tiffany Wun</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Miriam Sturdee</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Providing data visualization authoring tools for the general public remains an ongoing challenge. Inspired by block-printing, we explore how visualization stamps as a physical visualization authoring tool could leverage both visual freedom and ease of repetition. We conducted a workshop with two groups---visualization experts and non-experts---where participants authored visualizations on paper using hand-carved stamps made from potatoes and sponges. The low-fidelity medium freed participants to test new stamp patterns and accept mistakes. From the created visualizations, we observed several unique traits and uses of block-printing tools for visualization authoring, including: modularity of patterns, annotation guides, creation of multiple patterns from one stamp, and various techniques to apply data onto paper. We discuss the issues around expressivity and effectiveness of block-printed stamps in visualization authoring, and identify implications for the design and assembly of primitives in potential visualization stamp kits, as well as applications for future use in non-digital environments.</p><div class="ui labels">Keywords:  <span class="ui large label">potato</span><span class="ui large label">tangible tools</span><span class="ui large label">authoring visualizations</span><span class="ui large label">block-printing</span><span class="ui large label">physical template tools</span><span class="ui large label">information visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Tiffany Wun<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Miriam Sturdee<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->. <b>You say Potato, I say Po-Data: Physical Template Tools for Authoring Visualizations</b>. <i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/3294109.3295627" target="_blank">https://doi.org/10.1145/3294109.3295627</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2018-mikalauskas" class="ui large modal"><div class="header"><a href="/publications/dis-2018-mikalauskas" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2018-mikalauskas</a>  -  <a href="/publications/dis-2018-mikalauskas.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2018-mikalauskas.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2018-mikalauskas" target="_blank">Improvising with an Audience-Controlled Robot Performer</a></h1><p class="meta"><span>Claire Mikalauskas</span> , <span>Tiffany Wun</span> , <span>Kevin Ta</span> , <span>Joshua Horacsek</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In improvisational theatre (improv), actors perform unscripted scenes together, collectively creating a narrative. Audience suggestions introduce randomness and build audience engagement, but can be challenging to mediate at scale. We present Robot Improv Puppet Theatre (RIPT), which includes a performance robot (Pokey) who performs gestures and dialogue in short-form improv scenes based on audience input from a mobile interface. We evaluated RIPT in several initial informal performances, and in a rehearsal with seven professional improvisers. The improvisers noted how audience prompts can have a big impact on the scene - highlighting the delicate balance between ambiguity and constraints in improv. The open structure of RIPT performances allows for multiple interpretations of how to perform with Pokey, including one-on-one conversations or multi-performer scenes. While Pokey lacks key qualities of a good improviser, improvisers found his serendipitous dialogue and gestures particularly rewarding.</p><div class="ui labels">Keywords:  <span class="ui large label">human-robot interaction</span><span class="ui large label">improvised theatre</span><span class="ui large label">creativity-support tools</span><span class="ui large label">crowdsourcing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Claire Mikalauskas<!-- -->, <!-- -->Tiffany Wun<!-- -->, <!-- -->Kevin Ta<!-- -->, <!-- -->Joshua Horacsek<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Improvising with an Audience-Controlled Robot Performer</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/3196709.3196757" target="_blank">https://doi.org/10.1145/3196709.3196757</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2018-ta" class="ui large modal"><div class="header"><a href="/publications/dis-2018-ta" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2018-ta</a>  -  <a href="/publications/dis-2018-ta.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2018-ta.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2018-ta" target="_blank">Bod-IDE: An Augmented Reality Sandbox for eFashion Garments</a></h1><p class="meta"><span>Kevin Ta</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Electronic fashion (eFashion) garments use technology to augment the human body with wearable interaction. In developing ideas, eFashion designers need to prototype the role and behavior of the interactive garment in context; however, current wearable prototyping toolkits require semi-permanent construction with physical materials that cannot easily be altered. We present Bod-IDE, an augmented reality &#x27;mirror&#x27; that allows eFashion designers to create virtual interactive garment prototypes. Designers can quickly build, refine, and test on-the-body interactions without the need to connect or program electronics. By envisioning interaction with the body in mind, eFashion designers can focus more on reimagining the relationship between bodies, clothing, and technology.</p><div class="ui labels">Keywords:  <span class="ui large label">augmented reality</span><span class="ui large label">electronic fashion</span><span class="ui large label">creativity support tool</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kevin Ta<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Bod-IDE: An Augmented Reality Sandbox for eFashion Garments</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->5<!-- -->.  DOI: <a href="https://doi.org/10.1145/3197391.3205408" target="_blank">https://doi.org/10.1145/3197391.3205408</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-dillman" class="ui large modal"><div class="header"><a href="/publications/chi-2018-dillman" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-dillman</a>  -  <a href="/publications/chi-2018-dillman.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-dillman.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-dillman" target="_blank">A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</a></h1><p class="meta"><span>Kody R. Dillman</span> , <a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><strong>Terrance Mok</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Alex Mitchell</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences.</p><div class="ui labels">Keywords:  <span class="ui large label">game design</span><span class="ui large label">guidance</span><span class="ui large label">interaction cues</span><span class="ui large label">augmented reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kody R. Dillman<!-- -->, <!-- -->Terrance Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Alex Mitchell<!-- -->. <b>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173714" target="_blank">https://doi.org/10.1145/3173574.3173714</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-feick" class="ui large modal"><div class="header"><a href="/publications/chi-2018-feick" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-feick</a>  -  <a href="/publications/chi-2018-feick.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-feick.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-feick" target="_blank">Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</a></h1><p class="meta"><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><strong>Martin Feick</strong></a> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/sfxTHsPJWHY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/sfxTHsPJWHY?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/sfxTHsPJWHY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Remote collaborators working together on physical objects have difficulty building a shared understanding of what each person is talking about. Conventional video chat systems are insufficient for many situations because they present a single view of the object in a flattened image. To understand how this limited perspective affects collaboration, we designed the Remote Manipulator (ReMa), which can reproduce orientation manipulations on a proxy object at a remote site. We conducted two studies with ReMa, with two main findings. First, a shared perspective is more effective and preferred compared to the opposing perspective offered by conventional video chat systems. Second, the physical proxy and video chat complement one another in a combined system: people used the physical proxy to understand objects, and used video chat to perform gestures and confirm remote actions.</p><div class="ui labels">Keywords:  <span class="ui large label">cscw</span><span class="ui large label">remote collaboration</span><span class="ui large label">object-focused collaboration</span><span class="ui large label">physical telepresence</span><span class="ui large label">collaborative physical tasks</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Terrance Tin Hoi Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173855" target="_blank">https://doi.org/10.1145/3173574.3173855</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2018-ledo" class="ui large modal"><div class="header"><a href="/publications/chi-2018-ledo" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-ledo</a>  -  <a href="/publications/chi-2018-ledo.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2018-ledo" target="_blank">Evaluation Strategies for HCI Toolkit Research</a></h1><p class="meta"><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <span>Steven Houben</span> , <span>Jo Vermeulen</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><strong>Nicolai Marquardt</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><strong>Saul Greenberg</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/3lAwhCk60C4" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/3lAwhCk60C4?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/3lAwhCk60C4/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what &#x27;evaluating&#x27; a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.</p><div class="ui labels">Keywords:  <span class="ui large label">user interfaces</span><span class="ui large label">design</span><span class="ui large label">evaluation</span><span class="ui large label">prototyping</span><span class="ui large label">toolkits</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Steven Houben<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Nicolai Marquardt<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Saul Greenberg<!-- -->. <b>Evaluation Strategies for HCI Toolkit Research</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->17<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173610" target="_blank">https://doi.org/10.1145/3173574.3173610</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="hri-2018-feick" class="ui large modal"><div class="header"><a href="/publications/hri-2018-feick" target="_blank"><i class="fas fa-link fa-fw"></i>hri-2018-feick</a>  -  <a href="/publications/hri-2018-feick.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">HRI 2018</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/hri-2018-feick" target="_blank">The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</a></h1><p class="meta"><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><strong>Martin Feick</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>André Miede</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we discuss the role of the movement trajectory and velocity enabled by our tele-robotic system (ReMa) for remote collaboration on physical tasks. Our system reproduces changes in object orientation and position at a remote location using a humanoid robotic arm. However, even minor kinematics differences between robot and human arm can result in awkward or exaggerated robot movements. As a result, user communication with the robotic system can become less efficient, less fluent and more time intensive.</p><div class="ui labels">Keywords:  <span class="ui large label">movement trajectory &amp; velocity</span><span class="ui large label">remote collaboration</span><span class="ui large label">robot surrogate</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->André Miede<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b>. <i>In Adjunct Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction (HRI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->2<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173386.3176959" target="_blank">https://doi.org/10.1145/3173386.3176959</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2017-mok" class="ui large modal"><div class="header"><a href="/publications/dis-2017-mok" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2017-mok</a>  -  <a href="/publications/dis-2017-mok.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2017-mok.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2017-mok" target="_blank">Critiquing Physical Prototypes for a Remote Audience</a></h1><p class="meta"><a href="/people/terrance-mok"><img src="/static/images/people/terrance-mok.jpg" class="ui circular spaced image mini-profile"/><strong>Terrance Mok</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present an observational study of physical prototype critique that highlights some of the challenges of communicating physical behaviors and materiality at a distance. Geographically distributed open hardware communities often conduct user feedback and peer critique sessions via video conference. However, people have difficulty using current video conferencing tools to demonstrate and critique physical designs. To examine the challenges of remote critique, we conducted an observational lab study in which participants critiqued pairs of physical prototypes (prosthetic hands) for a face-to-face or remote collaborator. In both conditions, participants&#x27; material experiences were an important part of their critique, however their attention was divided between interacting with the prototype and finding strategies to communicate `invisible&#x27; features. Based on our findings, we propose design implications for remote collaboration tools that support the sharing of material experiences and prototype critique.</p><div class="ui labels">Keywords:  <span class="ui large label">design review</span><span class="ui large label">prototype critique</span><span class="ui large label">remote collaboration</span><span class="ui large label">material experience</span><span class="ui large label">open hardware</span><span class="ui large label">video conferencing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Terrance Mok<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Critiquing Physical Prototypes for a Remote Audience</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;17)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3064663.3064722" target="_blank">https://doi.org/10.1145/3064663.3064722</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-somanath" class="ui large modal"><div class="header"><a href="/publications/chi-2017-somanath" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2017-somanath</a>  -  <a href="/publications/chi-2017-somanath.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2017-somanath.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-somanath" target="_blank">&#x27;Maker&#x27; within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India</a></h1><p class="meta"><a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Janette Hughes</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <span>Mario Costa Sousa</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/NpIME1h1mH8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/NpIME1h1mH8?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/NpIME1h1mH8/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Do-it-yourself (DIY) inspired activities have gained popularity as a means of creative expression and self-directed learning. However, DIY culture is difficult to implement in places with limited technology infrastructure and traditional learning cultures. Our goal is to understand how learners in such a setting react to DIY activities. We present observations from a physical computing workshop with 12 students (13-15 years old) conducted at a high school in India. We observed unique challenges for these students when tackling DIY activities: a high monetary and psychological cost to exploration, limited independent learning resources, difficulties with finding intellectual courage and assumed technical language proficiency. Our participants, however, overcome some of these challenges by adopting their own local strategies: resilience, nonverbal and verbal learning techniques, and creating documentation and fallback circuit versions. Based on our findings, we discuss a set of lessons learned about makerspaces in a context with socio-technical challenges.</p><div class="ui labels">Keywords:  <span class="ui large label">India</span><span class="ui large label">HCI4D</span><span class="ui large label">physical computing</span><span class="ui large label">DIY</span><span class="ui large label">young learners</span><span class="ui large label">maker culture</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sowmya Somanath<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Janette Hughes<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Mario Costa Sousa<!-- -->. <b>&#x27;Maker&#x27; within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;17)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3025453.3025849" target="_blank">https://doi.org/10.1145/3025453.3025849</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2017-ledo" class="ui large modal"><div class="header"><a href="/publications/chi-2017-ledo" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2017-ledo</a>  -  <a href="/publications/chi-2017-ledo.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2017</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2017-ledo.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2017-ledo" target="_blank">Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices</a></h1><p class="meta"><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <span>Fraser Anderson</span> , <span>Ryan Schmidt</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><strong>Saul Greenberg</strong></a> , <span>Tovi Grossman</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Interactive, smart objects – customized to individuals and uses – are central to many movements, such as tangibles, the internet of things (IoT), and ubiquitous computing. Yet, rapid prototyping both the form and function of these custom objects can be problematic, particularly for those with limited electronics or programming experience. Designers often need to embed custom circuitry; program its workings; and create a form factor that not only reflects the desired user experience but can also house the required circuitry and electronics. To mitigate this, we created Pineal, a design tool that lets end-users: (1) modify 3D models to include a smart watch or phone as its heart; (2) specify high-level interactive behaviours through visual programming; and (3) have the phone or watch act out such behaviours as the objects&#x27; &quot;smarts&quot;. Furthermore, a series of prototypes show how Pineal exploits mobile sensing and output, and automatically generates 3D printed form-factors for rich, interactive, objects.</p><div class="ui labels">Keywords:  <span class="ui large label">fabrication</span><span class="ui large label">3d printing</span><span class="ui large label">smart objects</span><span class="ui large label">rapid prototyping</span><span class="ui large label">toolkits</span><span class="ui large label">prototyping tool</span><span class="ui large label">interaction design</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Fraser Anderson<!-- -->, <!-- -->Ryan Schmidt<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Saul Greenberg<!-- -->, <!-- -->Tovi Grossman<!-- -->. <b>Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;17)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1145/3025453.3025652" target="_blank">https://doi.org/10.1145/3025453.3025652</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2016-jones" class="ui large modal"><div class="header"><a href="/publications/dis-2016-jones" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2016-jones</a>  -  <a href="/publications/dis-2016-jones.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2016</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2016-jones.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2016-jones" target="_blank">Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</a></h1><p class="meta"><a href="/people/brennan-jones"><img src="/static/images/people/brennan-jones.jpg" class="ui circular spaced image mini-profile"/><strong>Brennan Jones</strong></a> , <span>Kody Dillman</span> , <span>Richard Tang</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Carman Neustaedter</span> , <span>Scott Bateman</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/10hbJHIQVX8" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/10hbJHIQVX8?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/10hbJHIQVX8/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>People are increasingly using mobile video to communicate, collaborate, and share experiences while on the go. Yet this presents challenges in adequately sharing camera views with remote users. In this paper, we study the use of semi-autonomous drones for video conferencing, where an outdoor user (using a smartphone) is connected to a desktop user who can explore the environment from the drone&#x27;s perspective. We describe findings from a study where pairs collaborated to complete shared navigation and search tasks. We illustrate the benefits of providing the desktop user with a view that is elevated, manipulable, and decoupled from the outdoor user. In addition, we articulate how participants overcame challenges in communicating environmental information and navigational cues, negotiated control of the view, and used the drone as a tool for sharing experiences. This provides a new way of thinking about mobile video conferencing where cameras that are decoupled from both users play an integral role in communication, collaboration, and sharing experiences.</p><div class="ui labels">Keywords:  <span class="ui large label">cscw</span><span class="ui large label">telepresence</span><span class="ui large label">video communication</span><span class="ui large label">shared experiences</span><span class="ui large label">teleoperation</span><span class="ui large label">drones</span><span class="ui large label">collaboration</span><span class="ui large label">hri</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Brennan Jones<!-- -->, <!-- -->Kody Dillman<!-- -->, <!-- -->Richard Tang<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Carman Neustaedter<!-- -->, <!-- -->Scott Bateman<!-- -->. <b>Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;16)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/2901790.2901847" target="_blank">https://doi.org/10.1145/2901790.2901847</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tvcg-2016-lopez" class="ui large modal"><div class="header"><a href="/publications/tvcg-2016-lopez" target="_blank"><i class="fas fa-link fa-fw"></i>tvcg-2016-lopez</a>  -  <a href="/publications/tvcg-2016-lopez.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TVCG 2016</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2016-lopez.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tvcg-2016-lopez" target="_blank">Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration</a></h1><p class="meta"><span>David Lopez</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Candemir Doger</span> , <span>Tobias Isenberg</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/jBtHgTYpJl0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/jBtHgTYpJl0?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/jBtHgTYpJl0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We discuss touch-based navigation of 3D visualizations in a combined monoscopic and stereoscopic viewing environment. We identify a set of interaction modes, and a workflow that helps users transition between these modes to improve their interaction experience. In our discussion we analyze, in particular, the control-display space mapping between the different reference frames of the stereoscopic and monoscopic displays. We show how this mapping supports interactive data exploration, but may also lead to conflicts between the stereoscopic and monoscopic views due to users&#x27; movement in space; we resolve these problems through synchronization. To support our discussion, we present results from an exploratory observational evaluation with domain experts in fluid mechanics and structural biology. These experts explored domain-specific datasets using variations of a system that embodies the interaction modes and workflows; we report on their interactions and qualitative feedback on the system and its workflow.</p><div class="ui labels">Keywords:  <span class="ui large label">visualization of 3D data</span><span class="ui large label">human-computer interaction</span><span class="ui large label">expert interaction</span><span class="ui large label">direct-touch input</span><span class="ui large label">mobile displays</span><span class="ui large label">stereoscopic environments</span><span class="ui large label">VR</span><span class="ui large label">AR</span><span class="ui large label">conceptual model of interaction</span><span class="ui large label">interaction reference frame mapping</span><span class="ui large label">observational study</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Lopez<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Candemir Doger<!-- -->, <!-- -->Tobias Isenberg<!-- -->. <b>Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration</b>. <i>In IEEE Transactions on Visualization and Computer Graphics (TVCG &#x27;16)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1109/TVCG.2015.2440233" target="_blank">https://doi.org/10.1109/TVCG.2015.2440233</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2015-oehlberg" class="ui large modal"><div class="header"><a href="/publications/chi-2015-oehlberg" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2015-oehlberg</a>  -  <a href="/publications/chi-2015-oehlberg.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2015</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2015-oehlberg.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2015-oehlberg" target="_blank">Patterns of Physical Design Remixing in Online Maker Communities</a></h1><p class="meta"><a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <span>Wendy E. Mackay</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/bkqLNgYIXek" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/bkqLNgYIXek?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/bkqLNgYIXek/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Makers participate in remixing culture by drawing inspiration from, combining, and adapting designs for physical objects. To examine how makers remix each others&#x27; designs on a community scale, we analyzed metadata from over 175,000 digital designs from Thingiverse, the largest online design community for digital fabrication. Remixed designs on Thingiverse are predominantly generated designs from Customizer a built-in web app for adjusting parametric designs. However, we find that these designs do not elicit subsequent user activity and the authors who generate them tend not to contribute additional content to Thingiverse. Outside of Customizer, influential sources of remixing include complex assemblies and design primitives, as well as non-physical resources posing as physical designs. Building on our findings, we discuss ways in which online maker communities could become more than just design repositories and better support collaborative remixing.</p><div class="ui labels">Keywords:  <span class="ui large label">customization</span><span class="ui large label">maker communities</span><span class="ui large label">user innovation</span><span class="ui large label">collaboration</span><span class="ui large label">hacking</span><span class="ui large label">remixing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Lora Oehlberg<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Wendy E. Mackay<!-- -->. <b>Patterns of Physical Design Remixing in Online Maker Communities</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;15)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/2702123.2702175" target="_blank">https://doi.org/10.1145/2702123.2702175</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div></div></div><div class="one wide column"></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img style="max-width:180px;margin:30px auto" src="/static/images/logo-6.png"/><div class="content"><img style="max-width:200px;margin:0px auto" src="/static/images/logo-4.png"/><div class="sub header">Department of Computer Science</div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"id":"lora-oehlberg"}},"page":"/person","query":{"id":"lora-oehlberg"},"buildId":"nRxFzcfYt4k13GmMYtYpY","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/person" src="/_next/static/nRxFzcfYt4k13GmMYtYpY/pages/person.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/nRxFzcfYt4k13GmMYtYpY/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.98f1a4e4c94db9943918.js" async=""></script><script src="/_next/static/runtime/main-22d58f57abab872d6e70.js" async=""></script></body></html>