<!DOCTYPE html><html><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="format-detection" content="telephone=no"/><link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.css" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/css/lightbox.css" rel="stylesheet"/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link href="/static/css/style.css" rel="stylesheet"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox-plus-jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              $('.ui.sidebar')
                .sidebar('attach events', '.sidebar.icon')

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><meta charSet="utf-8" class="next-head"/><title class="next-head">Seminar | Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="Human-Computer Interaction, HCI, Information Visualization, University of Calgary, CHI, UIST" class="next-head"/><meta name="description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta property="og:title" content="Seminar | Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta property="og:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta property="og:site_name" content="University of Calgary Interactions Lab" class="next-head"/><meta property="og:url" content="https://ilab.ucalgary.ca/" class="next-head"/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/cover.jpg" class="next-head"/><meta property="og:type" content="website" class="next-head"/><meta name="twitter:title" content="Seminar | Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta name="twitter:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/cover.jpg" class="next-head"/><meta name="twitter:card" content="summary" class="next-head"/><meta name="twitter:site" content="@ucalgary" class="next-head"/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" class="next-head"/><link rel="preload" href="/_next/static/V3KxGhDJB2qAz0iVcQl2X/pages/page.js" as="script"/><link rel="preload" href="/_next/static/V3KxGhDJB2qAz0iVcQl2X/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.a61bf59ddcaa0993bba1.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-d690ee4b7e1fbfe47058.js" as="script"/></head><body><div id="__next"><div><div><div class="ui right vertical sidebar menu"><a class="item" href="/">Home</a><a class="item" href="/publications">Publications</a><a class="item" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item active" href="/seminar">Seminar</a><a class="item" href="/location">Location</a></div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/"><b>UCalgary iLab</b></a></div><div class="right menu"><a class="item" href="/publications">Publications</a><a class="item" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item active" href="/seminar">Seminar</a><a class="item" href="/location">Location</a><div class="toc item"><a href="/"><b>UCalgary iLab</b></a><i style="float:right" class="sidebar icon"></i></div></div></div></div><div class="ui stackable grid"><div class="eleven wide column centered"><div id="seminar" class="category"><h1 class="ui horizontal divider header"><i class="calendar alternate outline icon"></i>iLab Invited Talk Series</h1><div class="ui " style="margin-top:50px"><div style="margin:40px 0"><p class="ui horizontal divider header">2021-05-03</p><div class="publication ui vertical segment grid" data-id="2021-05-03-stefanie-mueller"><div class="three wide column" style="margin:auto;text-align:center"><a href="https://hcie.csail.mit.edu/stefanie-mueller.html" target="_blank"><img class="photo" src="/static/images/seminar/stefanie-mueller.jpg"/><h1>Stefanie Mueller</h1><h3>MIT</h3></a></div><div class="twelve wide column"><h1>Advancing Personal Fabrication by Making Physical Objects as Reprogrammable as Digital Data</h1><h2>Abstract</h2><p>Computing has revolutionized how we process and interact with data today, unfortunately, these capabilities are constraint to the digital realm and cannot yet be applied to physical matter. For instance, today, we can already quickly update the appearance of a digital photo by applying a filter or adding and removing elements. However, updating physical objects in the same way is not possible today. In this talk, I will show my research group’s latest developments that bring us closer to a future in which physical objects are as reprogrammable as data is today. As a first example of this, I will show our research on a new reprogrammable material that can be applied to the surface of physical objects and that allows them to change their appearance within a few minutes. This allows us to update the color of clothing, shoes, and even entire rooms in the same way as we can update a digital photo today. I will then show additional developments that extend this concept to further integrate computing capabilities into physical objects, show our research on how we can print functional objects in one go without the need for assembly, and demonstrate how we can create unified prototyping environments that support engineers and designers in fabricating new types of physical objects.</p><h2>Bio</h2><p>Stefanie Mueller is the X-Career Development Assistant Professor in the MIT EECS department joint with MIT Mechanical Engineering and Head of the HCI Engineering Group at MIT CSAIL. For her research, Stefanie has received an NSF CAREER Award, an Alfred P. Sloan Fellowship, a Microsoft Research Faculty Fellowship, and was also named a Forbes 30 under 30 in Science. In addition, Stefanie’s work has been awarded several Best Paper and Honorable Mention Awards at the ACM CHI and ACM UIST conferences, the premier venues in Human-Computer Interaction. Stefanie has also served as the Program Chair of the ACM UIST 2020 conference and was a Subcommittee Chair for ACM CHI 2019 and 2020. At MIT, Stefanie served as a Program Co-Chair for the MIT EECS Rising Star Workshop in 2018 and is currently serving as the Head of the Human Computer Interaction Communities of Research (HCI CoR) at MIT CSAIL.</p></div></div></div><div style="margin:40px 0"><p class="ui horizontal divider header">2021-04-26</p><div class="publication ui vertical segment grid" data-id="2021-04-26-charles-perin"><div class="three wide column" style="margin:auto;text-align:center"><a href="http://charlesperin.net/" target="_blank"><img class="photo" src="/static/images/seminar/charles-perin.jpg"/><h1>Charles Perin</h1><h3>University of Victoria</h3></a></div><div class="twelve wide column"><h1>Beyond Visualization Wizardry: The Role of Interaction in Data Visualization</h1><h2>Abstract</h2><p>Visualization is not just a way of creating pretty pictures and &quot;intuitive dashboards&quot;. It is not a magic wand that you can apply to your dataset to automatically turn a data mess into &quot;actionable insights for transformative results&quot;. Far from this wizardry, I argue that understanding data comes at the cost of interacting with it. I will go through several research projects - ranging from manual reordering of matrices to active reading of visualizations to interaction discoverability to composite physicalizations to direct manipulation of graphical encodings - in an attempt to convince you that we can, and should, find better ways for people to interact with data visualizations.</p><h2>Bio</h2><p>Charles Perin is an Assistant Professor of Computer Science at the University of Victoria, where he co-leads the Victoria Interactive eXperiences with Information research group specializing in Human-Computer Interaction and Information Visualization. He and his students are particularly interested in designing and studying new interactions for visualizations and in understanding how people may make use of and interact with visualizations in their everyday lives; in designing visualization tools for authoring personal visualizations and for exploring and communicating open data; in sports visualization; and in visualization beyond the desktop. Before joining the University of Victoria in 2018, Charles was a Lecturer at City, University of London, before that a post-doctoral researcher at the University of Calgary, before that a PhD student at University Paris-Sud/INRIA, and long before that a kid in Brittany.</p></div></div></div><div style="margin:40px 0"><p class="ui horizontal divider header">2021-04-19</p><div class="publication ui vertical segment grid" data-id="2021-04-19-nicolai-marquardt"><div class="three wide column" style="margin:auto;text-align:center"><a href="http://www.nicolaimarquardt.com/" target="_blank"><img class="photo" src="/static/images/seminar/nicolai-marquardt.jpg"/><h1>Nicolai Marquardt</h1><h3>University College London</h3></a></div><div class="twelve wide column"><h1>Journey through the Design Space of Cross-Device Interactions</h1><h2>Abstract</h2><p>Designing interfaces or applications that move beyond the bounds of a single device screen enables new ways to engage with digital content. In this talk, I will guide you through the design space of cross-device interactions. In particular, I will give an overview of what the research field of cross-device interactions looks like, and what kind of techniques we can use for designing fluid cross-device interactions. I’ll also discuss a few of the open issues in the research field and suggest opportunities of where we can go next.</p><h2>Bio</h2><p>Nicolai Marquardt is Associate Professor at the University College London, where he is part of the Department of Computer Science, Faculty of Engineering and the Faculty of Brain Sciences. At the UCL Interaction Centre, he works on projects in the research areas of cross-device interaction, sensor-based systems, prototyping toolkits, and design methods. He received his PhD in Computer Science from the University of Calgary, Canada. Nicolai is co-author of the Sketching User Experiences Workbook (Morgan Kaufmann 2011) and the Proxemic Interactions textbook (Morgan &amp; Claypool 2015).</p></div></div></div><div style="margin:40px 0"><p class="ui horizontal divider header">2021-04-12</p><div class="publication ui vertical segment grid" data-id="2021-04-12-benjamin-bach"><div class="three wide column" style="margin:auto;text-align:center"><a href="https://www.designinformatics.org/person/benjaminbach/" target="_blank"><img class="photo" src="/static/images/seminar/benjamin-bach.jpg"/><h1>Benjamin Bach</h1><h3>University of Edinburgh</h3></a></div><div class="twelve wide column"><h1>The Immersive Canvas: Data Visualization and Interaction for Immersive Analytics</h1><h2>Abstract</h2><p>This talk explores the role of interactive data visualization for immersive analytics. Immersive analytics is becoming a complex field that combines many fields of expertise: analytics, big data, infrastructure, virtual and augmented systems, image recognition and many others, as well as human-computer interaction and visualization. In order to make sense of complex data, we need visualization interfaces: in immersive environments, data visualization is freed of the limitedness of the traditional desktop screen; able to expand into an infinite canvas and the third dimension. What potential does this bring for data visualization and immersive visualization? How can we leverage this potential? How is this changing our approach to visualizing and interacting with data?</p><h2>Bio</h2><p>Dr Benjamin Bach is a Lecturer in Design Informatics and Visualization at the University of Edinburgh. His research designs and investigates interactive information visualization interfaces to help people explore, communicate, and understand data. Before joining the University of Edinburgh in 2017, Benjamin worked as a postdoc at Harvard University (Visual Computing Group), Monash University, as well as the Microsoft-Research Inria Joint Centre. Benjamin was visiting researcher at the University of Washington and Microsoft Research in 2015. He obtained his PhD in 2014 from the Université Paris Sud where he worked at the Aviz Group at Inria. The PhD thesis entitled Connections, Changes, and Cubes: Unfolding Dynamic Networks for Visual Exploration got awarded an honorable mention as the Best Thesis by the IEEE Visualization Committee.</p></div></div></div><div style="margin:40px 0"><p class="ui horizontal divider header">2021-03-29</p><div class="publication ui vertical segment grid" data-id="2021-03-29-james-young"><div class="three wide column" style="margin:auto;text-align:center"><a href="http://hci.cs.umanitoba.ca/people/bio/james-e.young" target="_blank"><img class="photo" src="/static/images/seminar/james-young.jpg"/><h1>James Young</h1><h3>University of Manitoba</h3></a></div><div class="twelve wide column"><h1>Warning, This robot is not what it seems! A discussion on deception and the future of social robots</h1><h2>Abstract</h2><p>Social robots are designed to interact with people using human- or animal-like language, gestures, or other techniques. This approach promises intuitive interaction, and can be designed to shape a person&#x27;s mood and behavior; social robots can even serve as companions. However, I argue that social robots - by design - are fundamentally rooted in deception, which highlights real potential dangers as these robots enter society. On the flip side, considering this deception also provides a positive way forward, a path for developing social robots that can be successful in both our everyday lives.</p><h2>Bio</h2><p>Dr. James Young is a professor of computer science at the University of Manitoba.</p></div></div></div><div style="margin:40px 0"><p class="ui horizontal divider header">2021-03-15</p><div class="publication ui vertical segment grid" data-id="2021-03-15-jessica-cauchard"><div class="three wide column" style="margin:auto;text-align:center"><a href="https://scholar.google.com/citations?user=Rk1SDB8AAAAJ&amp;hl=en" target="_blank"><img class="photo" src="/static/images/seminar/jessica-cauchard.jpg"/><h1>Jessica Cauchard</h1><h3>Ben Gurion University</h3></a></div><div class="twelve wide column"><h1>On Body &amp; Out of Body Interactions</h1><h2>Abstract</h2><p>Mobile devices have become ubiquitous over the last decade, changing the way we interact with technology and with one another. Mobile devices were at first personal devices carried in our hands or pockets. They are now changing form to fit our lifestyles and an increasingly demanding amount and diversity of information to display. My research focuses on the design, development, and evaluation of novel interaction techniques with mobile devices using a human-centered approach. In this presentation, I will in particular focus on two types of mobile technologies: wearables and drones. I will discuss the use of multiple modalities to interact with technology and in particular how haptics on wearables can support long-term tasks without interrupting the user’s attention. I will then discuss how autonomous devices such as drones re-invent our understanding of ubiquitous computing and present my current research on collocated natural human-drone interaction.</p><h2>Bio</h2><p>Dr. Jessica Cauchard is a lecturer in the department of Industrial Engineering and Management at Ben Gurion University of the Negev in Israel, where she recently founded the Magic Lab. Her research is rooted in the fields of Human-Computer and Human-Robot Interaction with a focus on novel interaction techniques and ubiquitous computing. Previously, she was faculty of Computer Science at the Interdisciplinary Center Herzliya between 2017 and 2019. Before moving to Israel, Dr. Cauchard worked as a postdoctoral scholar at Stanford University. She has a strong interest in autonomous vehicles and intelligent devices and how they change our device ecology. She completed her PhD in Computer Science at the University of Bristol, UK in 2013 and received a Magic Grant for her work on interacting with drones by the Brown Institute for Media Innovation in 2015</p></div></div></div><div style="margin:40px 0"><p class="ui horizontal divider header">2021-03-08</p><div class="publication ui vertical segment grid" data-id="2021-03-08-alberto-de-salvatierra"><div class="three wide column" style="margin:auto;text-align:center"><a href="https://sapl.ucalgary.ca/about/people/alberto-de-salvatierra" target="_blank"><img class="photo" src="/static/images/seminar/alberto-de-salvatierra.jpg"/><h1>Alberto de Salvatierra</h1><h3>UCalgary School of Architecture, Landscape and Planning</h3></a></div><div class="twelve wide column"><h1>Soft Infrastructures</h1><h2>Abstract</h2><p>The COVID-19 pandemic has exposed the fragility of existing models of housing, collective life, and infrastructure. The 99% have been disproportionately marginalized by shelter-in-place orders and quarantines that assume they have the resources to weather this moment of extreme instability. The transition from a quarantine to a post-pandemic city will not only be a fight for collective human health and wellbeing, but will also be the staging ground for our last stand to prevent a forthcoming climate catastrophe. New paradigms of urban design and civic infrastructure must be decoupled from society’s carbon-intensive practices and archaic fetishes for &quot;solidity&quot; in building. &quot;Soft Infrastructures&quot; present an alternative modality for urban design. This lecture will discuss three on-going projects by the Center for Civilization: Civic Commons Catalyst, Eternal Ephemera, and Soft City/Soft Haus.</p><h2>Bio</h2><p>Alberto de Salvatierra is an assistant professor of urbanism and data in architecture at the University of Calgary&#x27;s School of Architecture, Planning and Landscape, director of the Center for Civilization—a design research lab and international think tank, the founding principal of PROXIIMA, and a Global Shaper at the Calgary Hub of the Global Shapers Community—an initiative by the World Economic Forum based in Geneva, Switzerland. An interdisciplinary polymath, architectural designer, and landscape urbanist, Alberto’s research and work focuses on material flows as infrastructure at the urban and civilizational scales, while his collaborative research agenda centers on fostering, developing and writing on interdisciplinary pedagogy and practices. His work has been published widely and exhibited both domestically and abroad, such as in the United States, the United Kingdom, Mexico, Italy, Japan, Sweden and Serbia, and in such venues as the Priscilla Fowler Fine Art Gallery in Las Vegas, NV, Calatrava-designed Milwaukee Art Museum in Milwaukee, WI, and the National Building Museum in Washington, D.C. In 2019, he was part of the Harvard Kennedy School’s inaugural STS (Science, Technology and Society) program on Expertise, Trust and Democracy, and an invited panelist and delegate to the United Nations.</p></div></div></div><div style="margin:40px 0"><p class="ui horizontal divider header">2021-03-01</p><div class="publication ui vertical segment grid" data-id="2021-03-01-alicia-nahmad-vazquez"><div class="three wide column" style="margin:auto;text-align:center"><a href="https://sapl.ucalgary.ca/about/people/alicia-nahmad" target="_blank"><img class="photo" src="/static/images/seminar/alicia-nahmad-vazquez.jpg"/><h1>Alicia Nahmad Vazquez</h1><h3>UCalgary School of Architecture, Landscape and Planning</h3></a></div><div class="twelve wide column"><h1>Design in the Age of Intelligent Machines</h1><h2>Abstract</h2><p></p><h2>Bio</h2><p>Alicia Nahmad Vazquez is the founder of Architecture Extrapolated (R-Ex) and an assistant professor in robotics and AI at the University of Calgary School of Architecture Planning and Landscape (SAPL) . She is also co-director of the Laboratory for Integrative Design at UofC. For the past 5 years, Alicia worked as studio master at the Architectural Associational Design Research Laboratory (DRL) master’s program. As a research-based practising architect, Alicia explores materials and digital design and fabrication technologies along with the digitization of building trades and the wisdom of traditional building cultures. Her projects include the construction of award-winning ‘Knit-Candela’ and diverse collaborations with practice and academic institutions. She holds a PhD in human-robot collaborative (HRC) design from Cardiff University and a MArch from the AADRL. Alicia previously worked on developing design tools for practices like Populous and Zaha Hadid Architects. Alicia has also been an Artist-In-Residence at Autodesk Pier 9 and has taught and lectured extensively in Latin America and Europe. Her research has been widely published internationally in journals and conference proceedings.</p></div></div></div><div style="margin:40px 0"><p class="ui horizontal divider header">2021-02-08</p><div class="publication ui vertical segment grid" data-id="2021-02-08-rubaiat-habib"><div class="three wide column" style="margin:auto;text-align:center"><a href="https://rubaiathabib.me/" target="_blank"><img class="photo" src="/static/images/seminar/rubaiat-habib.jpg"/><h1>Rubaiat Habib</h1><h3>Adobe Research</h3></a></div><div class="twelve wide column"><h1>Dynamic Graphics as a Language</h1><h2>Abstract</h2><p>How can we make animation as easy as sketching? How can we create dynamic contents in real-time, in the speed of thought? How will dynamic graphics shape our real-time communications and language? In this talk, I&#x27;m going to present my research on animation, storytelling, and design, including the design of Sketchbook Motion that was crowned as &quot;The best iPad app of the year 2016&quot; by Apple. Most of us experience the power of animated media every day: animation makes it easy to communicate complex ideas beyond verbal language. However, only few of us have the skills to express ourselves through this medium. By making animation as easy, accessible, and fluid as sketching, I intend to make dynamic graphics a powerful medium to think, create, and communicate rapidly.</p><h2>Bio</h2><p>Rubaiat Habib is a Sr. Research Scientist at Adobe Research. His research interest lies at the intersection of Computer Graphics and HCI for creative thinking, design, and storytelling. His research in dynamic drawings and animations turned into products that reach a global audience. Rubaiat received several awards for his work including Apple App of the year 2016, three ACM CHI Best Paper Nominations, ACM CHI and ACM UIST Peoples choice best talk awards, and ACM CHI Golden Mouse awards for best research videos. For his PhD at the National University of Singapore, Rubaiat also received a Microsoft Research Asia PhD fellowship. Prior to Adobe, he worked at Autodesk Research and Microsoft Research. rubaiathabib.me</p></div></div></div><div style="margin:40px 0"><p class="ui horizontal divider header">2021-01-25</p><div class="publication ui vertical segment grid" data-id="2021-01-25-xing-dong-yang"><div class="three wide column" style="margin:auto;text-align:center"><a href="https://www.cs.dartmouth.edu/~xingdong/" target="_blank"><img class="photo" src="/static/images/seminar/xing-dong-yang.jpg"/><h1>Xing-Dong Yang</h1><h3>Dartmouth College</h3></a></div><div class="twelve wide column"><h1>Creating Smart Everyday Things</h1><h2>Abstract</h2><p>In my vision, the user interfaces of the future are in a blend of smart physical and virtual environments. My research focuses on the physical side by bringing interactivity to everyday things. I believe this vision is only achievable if people with varying backgrounds and abilities can work together in an accessible and collaborative environment. In this talk, I will describe two major threads of research in interactive everyday things and hardware prototyping tools. The first thread investigates interactive systems to sense the context of use of the things or estimate a user’s intention when touch input data is noisy. For example, I will demonstrate a tablecloth augmented with a fabric sensor that can sense and recognize non-metallic objects placed on a table, such as food, different types of fruits, liquids, plastic, and paper products. I will also show examples of how this technique can be used for contextual applications. The second thread investigates tools to lower the bar of entry to prototyping electronics, which is an essential skill needed to create smart everyday things. The goal of this line of work is to enable more people with varying backgrounds and abilities to create smart everyday things and eventually a better user experience of smart environments. For example, I will demonstrate an audio-tactile tutorial system for blind or low vision learners to understand circuit diagrams, which is an important task in the circuit prototyping pipeline. Both of these threads share a common goal that is to create a better user experience in smart environments.</p><h2>Bio</h2><p>Xing-Dong Yang is an Assistant Professor of Computer Science at Dartmouth College. His research is broadly in Human-Computer Interaction (HCI), where he creates interactive systems using sensing techniques and haptics to enable new applications in smart physical and virtual environments. Xing-Dong’s work is recognized through a Best Paper award at UIST 2019, eight Honorable Mention awards with one at UIST 2020, six at CHI (2010, 2016, 2018, 2019 × 2, 2020), and one at MobileHCI 2009. Aside from academic publications, Xing-Dong’s work attracts major public interest via news coverage from a variety of media outlets with different mediums, including TV (e.g., Discovery Daily Planet), print (e.g., The Wall Street Journal, Forbes), and Internet News (e.g., MIT Technology Review, New Scientist).</p></div></div></div><div style="margin:40px 0"><p class="ui horizontal divider header">2021-01-15</p><div class="publication ui vertical segment grid" data-id="2021-01-15-ken-nakagaki"><div class="three wide column" style="margin:auto;text-align:center"><a href="https://www.ken-nakagaki.com/" target="_blank"><img class="photo" src="/static/images/seminar/ken-nakagaki.jpg"/><h1>Ken Nakagaki</h1><h3>MIT Media Lab</h3></a></div><div class="twelve wide column"><h1>&#x27;Mechanical Shells&#x27; for Actuated Tangible UIs - Hybrid Architecture of Active and Passive Machines for Interaction Design</h1><h2>Abstract</h2><p>Research on actuated and shape-changing Tangible User Interfaces (TUIs) in the field of HCI has been explored widely in the past few decades to enrich interaction with digital information in physical and dynamic ways. In this effort, various types of generic devices of actuated TUIs have been investigated including pin-based shape displays, actuated curve interfaces, and swarm user interfaces. While these approaches are intended to be dynamically reconfigurable to offer generic interactivity, each hardware is inherently limited to the fixed configurations. How can we further expand the versatility of the actuated TUIs for fully expanding their capability for tangible interactions and motion / shape representations?

In my talk, I propose a ‘mechanical shell’, a design concept for actuated TUIs with modular interchangeable components that extends and converts the shape, motion, and interactivity of the hardware. By doing so, compared with the actuated TUI itself, each mechanical shell would bring much more specialized and customized interactivity, while, as the whole architecture, the system can adapt to much more versatile interactions. I present two research instances that demonstrate this concept based-on pin-based shape display and swarm user interface, and introduce proof-of-concept implementation as well as a range of applications. By introducing the novel interaction architecture, my research envisions the future of the physical environment where active and passive machines exist together for enriching tangible and embodied interactions.</p><h2>Bio</h2><p>Ken is an interaction designer and HCI researcher from Japan. Currently, he is a Ph.D. Candidate of Tangible Media Group, MIT Media Lab. He is interested in developing interfaces that combine digital information or computational aids into daily physical tools and materials, to develop novel physical and perceptual experiences. His research has been presented in top HCI conferences (ACM CHI, UIST, TEI, etc), and demonstrated in various exhibitions and awards including Ars Electronica, A&#x27; Design Award, and Japan Media Arts Festival.</p></div></div></div></div></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img style="max-width:180px;margin:30px auto" src="/static/images/logo-6.png"/><div class="content"><img style="max-width:200px;margin:0px auto" src="/static/images/logo-4.png"/><div class="sub header">Department of Computer Science</div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"id":"seminar","title":"Seminar"}},"page":"/page","query":{"id":"seminar"},"buildId":"V3KxGhDJB2qAz0iVcQl2X","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/page" src="/_next/static/V3KxGhDJB2qAz0iVcQl2X/pages/page.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/V3KxGhDJB2qAz0iVcQl2X/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.a61bf59ddcaa0993bba1.js" async=""></script><script src="/_next/static/runtime/main-d690ee4b7e1fbfe47058.js" async=""></script></body></html>