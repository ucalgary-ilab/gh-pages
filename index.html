<!DOCTYPE html><html><head><link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.css" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/css/lightbox.css" rel="stylesheet"/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link href="/static/css/style.css" rel="stylesheet"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox-plus-jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-1"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-1');
          </script><script>
            $(window).ready(function() {
              $('.publication').on('click', function(event) {
                if (event.target.className !== 'author-link') {
                  const id = this.dataset.id
                  $('#'+id).modal('show')
                }
              })
            })
          </script><meta charSet="utf-8" class="next-head"/><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><link rel="preload" href="/_next/static/8g1RMSDT1CNRhC_UbmcOV/pages/index.js" as="script"/><link rel="preload" href="/_next/static/8g1RMSDT1CNRhC_UbmcOV/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.98f1a4e4c94db9943918.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-22d58f57abab872d6e70.js" as="script"/></head><body><div id="__next"><div><title>Interactions Lab | University of Calgary HCI Group</title><div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/"><b style="color:#00716C">UCalgary iLab</b></a></div><div class="right menu"><a class="item" href="/publications">Publications</a><a class="item" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/news">News</a><a class="item" href="/location">Location</a></div></div></div><div id="top-video-container"><video id="top-video" poster="/static/posters/top.png" preload="metadata" autoplay="" loop="" muted="" playsinline="" webkit-playsinline=""><source src="/static/videos/top.mp4" type="video/mp4"/></video></div><div class="ui stackable grid"><div class="one wide column"></div><div class="eleven wide column centered"><div id="header-logo"><div><img src="/static/images/logo-3.png" style="height:100px"/></div><div><img src="/static/images/logo-1.png" style="height:200px;margin-top:-20px"/></div></div><div id="header" class="category"><h1>University of Calgary - Interactions Lab</h1><p>Human-Computer Interaction and Information Visualization Group</p></div><div><h1 class="ui horizontal divider header">Research Labs</h1><div id="labs" class="ui stackable four column grid" style="text-align:center"><div class="column"><a href="https://utouch.cpsc.ucalgary.ca/" target="_blank" class="ui segment"><div class="img"><img src="/static/images/labs/utouch.png"/></div><h3>Physical Interaction and Human-Robot Interaction</h3><p class="header">Prof. <!-- -->Ehud Sharlin</p></a></div><div class="column"><a href="http://pages.cpsc.ucalgary.ca/~lora.oehlberg/" target="_blank" class="ui segment"><div class="img"><img src="/static/images/labs/curiosity.png"/></div><h3>Human-Centered Design for Creativity &amp; Curiosity</h3><p class="header">Prof. <!-- -->Lora Oehlberg</p></a></div><div class="column"><a href="https://dataexperience.cpsc.ucalgary.ca/" target="_blank" class="ui segment"><div class="img"><img src="/static/images/labs/dataexperience.png"/></div><h3>Visual Data-driven Tools and Experiences</h3><p class="header">Prof. <!-- -->Wesley Willet</p></a></div><div class="column"><a href="https://ryosuzuki.org/" target="_blank" class="ui segment"><div class="img"><img src="/static/images/labs/suzuki.png"/></div><h3>Tangible and Shape-changing Interfaces</h3><p class="header">Prof. <!-- -->Ryo Suzuki</p></a></div><div class="column"><a href="http://grouplab.cpsc.ucalgary.ca/" target="_blank" class="ui segment"><div class="img"><img src="/static/images/labs/grouplab.png"/></div><h3>Research in HCI, CSCW, and UbiComp</h3><p class="header">Prof. <!-- -->Saul Greenberg (Emeritus)</p></a></div><div class="column"><a href="https://ricelab.github.io/" target="_blank" class="ui segment"><div class="img"><img src="/static/images/labs/ricelab.png"/></div><h3>Rethinking Interaction, Collaboration, &amp; Engagement</h3><p class="header">Prof. <!-- -->Anthony Tang (Adjunct - University of Toronto)</p></a></div><div class="column"><a href="http://sheelaghcarpendale.ca/" target="_blank" class="ui segment"><div class="img"><img src="/static/images/labs/innovis.png"/></div><h3>Innovations in Visualization Laboratory</h3><p class="header">Prof. <!-- -->Sheelagh Carpendale (Adjunct - Simon Fraser University)</p></a></div></div></div><div id="news" class="category"><h1 class="ui horizontal divider header"><i class="paper plane outline icon"></i>News</h1><div class="ui segment"><div class="ui divided items"><div class="item"><div class="ui tiny image" style="margin:auto;text-align:center;font-size:2.2em;background:#eee;height:80px;padding-top:20px"><i class="fas fa-graduation-cap fa-fw"></i></div><div class="content"><div class="meta">2020-08-07</div><div class="middle aligned content">David Ledo defended his PhD dissertation</div></div></div><div class="item"><div class="ui tiny image"><img src="/static/images/news/uist-2020.jpg" style="padding:5px"/></div><div class="content"><div class="meta">2020-07-10</div><div class="middle aligned content">One paper accepted to UIST 2020</div></div></div><div class="item"><div class="ui tiny image"><img src="/static/images/news/iros-2020.jpg" style="padding:5px"/></div><div class="content"><div class="meta">2020-06-07</div><div class="middle aligned content">One paper accepted to IROS 2020</div></div></div><div class="item"><div class="ui tiny image"><img src="/static/images/news/imwut.jpg" style="padding:5px"/></div><div class="content"><div class="meta">2020-03-07</div><div class="middle aligned content">One paper accepted to IMWUT 2020</div></div></div><div class="item"><div class="ui tiny image"><img src="/static/images/news/chi-2020.jpg" style="padding:5px"/></div><div class="content"><div class="meta">2020-02-05</div><div class="middle aligned content">Four papers accepted to CHI 2020</div></div></div></div></div><div class="ui vertical segment stackable" style="text-align:center"><a class="ui button" href="/news">+ see more updates</a></div></div><div id="publications" class="category"><h1 class="ui horizontal divider header"><i class="file alternate outline icon"></i>Recent Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="uist-2020-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">UIST 2020</span></p><p style="font-size:1.3em;color:#00716C"><b>RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Rubaiat Habib Kazi</span> , <span>Li-Yi Wei</span> , <span>Stephen DiVerdi</span> , <span>Wilmot Li</span> , <span>Daniel Leithinger</span></p></div></div><div class="publication ui vertical segment stackable grid" data-id="iros-2020-hedayati"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/iros-2020-hedayati.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">IROS 2020</span></p><p style="font-size:1.3em;color:#00716C"><b>PufferBot: Actuated Expandable Structures for Aerial Robots</b></p><p><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p></div></div><div class="publication ui vertical segment stackable grid" data-id="imwut-2020-wang"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/imwut-2020-wang.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">IMWUT 2020</span></p><p style="font-size:1.3em;color:#00716C"><b>AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</b></p><p><span>Xiyue Wang</span> , <span>Kazuki Takashima</span> , <span>Tomoaki Adachi</span> , <span>Patrick Finn</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <span>Yoshifumi Kitamura</span></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-anjani"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">CHI 2020</span></p><p style="font-size:1.3em;color:#00716C"><b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b></p><p><span>Laurensia Anjani</span> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Wooi Boon Goh</span></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-goffin"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-goffin.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">CHI 2020</span></p><p style="font-size:1.3em;color:#00716C"><b>Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</b></p><p><span>Pascal Goffin</span> , <span>Tanja Blascheck</span> , <span>Petra Isenberg</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2020-hou"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">CHI 2020</span></p><p style="font-size:1.3em;color:#00716C"><b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b></p><p><span>Ming Hou</span> , <a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Karthik Mahadevan</span></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="mobilehci-2019-hung"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/mobilehci-2019-hung.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">MobileHCI 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</b></p><p><span>Michael Hung</span> , <a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2019-walny"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2019-walny.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">TVCG 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff</b></p><p><span>Jagoda Walny</span> , <span>Christian Frisson</span> , <span>Mieka West</span> , <span>Doris Kosminsky</span> , <a href="/people/soren-knudsen"><img src="/static/images/people/soren-knudsen.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Søren Knudsen</span></a> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="cnc-2019-hammad"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/cnc-2019-hammad.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">C&amp;C 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning</b></p><p><a href="/people/nour-hammad"><img src="/static/images/people/nour-hammad.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nour Hammad</span></a> , <span>Elaheh Sanoubari</span> , <span>Patrick Finn</span> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <span>James E. Young</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-mahadevan"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-mahadevan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">DIS 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator</b></p><p><a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Karthik Mahadevan</span></a> , <span>Elaheh Sanoubari</span> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <span>James E. Young</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tvcg-2019-blascheck"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2019-blascheck.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">TVCG 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>Exploration Strategies for Discovery of Interactivity in Visualizations</b></p><p><span>Tanja Blascheck</span> , <span>Lindsay MacDonald Vermeulen</span> , <span>Jo Vermeulen</span> , <span>Charles Perin</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <span>Thomas Ertl</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-ledo"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">DIS 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</b></p><p><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <span>Jo Vermeulen</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Saul Greenberg</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Sebastian Boring</span></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2019-bressa"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-bressa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">DIS 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>Sketching and Ideation Activities for Situated Visualization Design</b></p><p><a href="/people/nathalie-bressa"><img src="/static/images/people/nathalie-bressa.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nathalie Bressa</span></a> , <a href="/people/kendra-wannamaker"><img src="/static/images/people/kendra-wannamaker.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kendra Wannamaker</span></a> , <span>Henrik Korsgaard</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <span>Jo Vermeulen</span></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2019-danyluk"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2019-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">CHI 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>Look-From Camera Control for 3D Terrain Maps</b></p><p><span>Kurtis Thorvald Danyluk</span> , <span>Bernhard Jenny</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tei-2019-mikalauskas"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-mikalauskas.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">TEI 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</b></p><p><span>Claire Mikalauskas</span> , <span>April Viczko</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="vr-2019-satriadi"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/vr-2019-satriadi.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">IEEE VR 2019</span></p><p style="font-size:1.3em;color:#00716C"><b>Augmented Reality Map Navigation with Freehand Gestures</b></p><p><span>Kadek Ananta Satriadi</span> , <span>Barrett Ens</span> , <span>Maxime Cordeil</span> , <span>Bernhard Jenny</span> , <span>Tobias Czauderna</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="hri-2018-feick"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">HRI 2018</span></p><p style="font-size:1.3em;color:#00716C"><b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b></p><p><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Martin Feick</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <span>André Miede</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-dillman"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-dillman.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">CHI 2018</span></p><p style="font-size:1.3em;color:#00716C"><b>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</b></p><p><span>Kody R. Dillman</span> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Anthony Tang</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <span>Alex Mitchell</span></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-ledo"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-ledo.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">CHI 2018</span></p><p style="font-size:1.3em;color:#00716C"><b>Evaluation Strategies for HCI Toolkit Research</b></p><p><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">David Ledo</span></a> , <span>Steven Houben</span> , <span>Jo Vermeulen</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nicolai Marquardt</span></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Saul Greenberg</span></a></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2018-mahadevan"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-mahadevan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted teal label">CHI 2018</span></p><p style="font-size:1.3em;color:#00716C"><b>Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction</b></p><p><a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Karthik Mahadevan</span></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sowmya Somanath</span></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ehud Sharlin</span></a></p></div></div></div><div id="publications-modal"><div id="uist-2020-suzuki" class="ui large modal"><div class="header"><a href="/publications/uist-2020-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2020-suzuki</a>  -  <a href="/publications/uist-2020-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2020</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2020-suzuki.jpg"/></div><div class="thirteen wide column"><h1>RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Rubaiat Habib Kazi</span> , <span>Li-Yi Wei</span> , <span>Stephen DiVerdi</span> , <span>Wilmot Li</span> , <span>Daniel Leithinger</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.</p><div class="ui labels">Keywords:  <span class="ui large grey label">augmented reality</span><span class="ui large grey label">embedded data visualization</span><span class="ui large grey label">real-time authoring</span><span class="ui large grey label">sketching interfaces</span><span class="ui large grey label">tangible interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Li-Yi Wei<!-- -->, <!-- -->Stephen DiVerdi<!-- -->, <!-- -->Wilmot Li<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div></div><div id="iros-2020-hedayati" class="ui large modal"><div class="header"><a href="/publications/iros-2020-hedayati" target="_blank"><i class="fas fa-link fa-fw"></i>iros-2020-hedayati</a>  -  <a href="/publications/iros-2020-hedayati.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IROS 2020</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/iros-2020-hedayati.jpg"/></div><div class="thirteen wide column"><h1>PufferBot: Actuated Expandable Structures for Aerial Robots</h1><p class="meta"><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drone’s propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique charac- terizations which may be useful in different situations. Finally, we present three motivating scenarios in which PufferBot might be useful beyond existing static propeller guard structures.</p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hooman Hedayati<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Daniel Szafir<!-- -->. <b>PufferBot: Actuated Expandable Structures for Aerial Robots</b>. <i>In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS &#x27;20)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->6<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div></div><div id="imwut-2020-wang" class="ui large modal"><div class="header"><a href="/publications/imwut-2020-wang" target="_blank"><i class="fas fa-link fa-fw"></i>imwut-2020-wang</a>  -  <a href="/publications/imwut-2020-wang.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IMWUT 2020</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/imwut-2020-wang.jpg"/></div><div class="thirteen wide column"><h1>AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</h1><p class="meta"><span>Xiyue Wang</span> , <span>Kazuki Takashima</span> , <span>Tomoaki Adachi</span> , <span>Patrick Finn</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <span>Yoshifumi Kitamura</span></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/fxxvZBY80ug?" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/fxxvZBY80ug?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/fxxvZBY80ug/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Natural disasters cause long-lasting mental health problems such as PTSD in children. Following the 2011 Earthquake and Tsunami in Japan, we witnessed a shift of toy block play behavior in young children who suffered from stress after the disaster. The behavior reflected their emotional responses to the traumatic event. In this paper, we explore the feasibility of using data captured from block-play to assess children&#x27;s stress after a major natural disaster. We prototyped sets of sensor-embedded toy blocks, AssessBlocks, that automate quantitative play data acquisition. During a three-year period, the blocks were dispatched to fifty-two post-disaster children. Within a free play session, we captured block features, a child&#x27;s playing behavior, and stress evaluated by several methods. The result from our analysis reveal correlations between block play features and stress measurements and show initial promise of using the effectiveness of using AssessBlocks to assess children&#x27;s stress after a disaster. We provide detailed insights into the potential as well as the challenges of our approach and unique conditions. From these insights we summarize guidelines for future research in automated play assessment systems that support children&#x27;s mental health.</p><div class="ui labels">Keywords:  <span class="ui large grey label">well being</span><span class="ui large grey label">toy blocks</span><span class="ui large grey label">PTSD</span><span class="ui large grey label">tangibles for health</span><span class="ui large grey label">stress assessment</span><span class="ui large grey label">play</span><span class="ui large grey label">children</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Xiyue Wang<!-- -->, <!-- -->Kazuki Takashima<!-- -->, <!-- -->Tomoaki Adachi<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Yoshifumi Kitamura<!-- -->. <b>AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters</b>. <i>In Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->29<!-- -->.  DOI: <a href="https://doi.org/10.1145/3381016" target="_blank">https://doi.org/10.1145/3381016</a></p></div></div></div></div></div><div id="chi-2020-anjani" class="ui large modal"><div class="header"><a href="/publications/chi-2020-anjani" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-anjani</a>  -  <a href="/publications/chi-2020-anjani.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-anjani.jpg"/></div><div class="thirteen wide column"><h1>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</h1><p class="meta"><span>Laurensia Anjani</span> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Wooi Boon Goh</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present a mixed-methods study of viewers on their practices and motivations around watching mukbang — video streams of people eating large quantities of food. Viewers&#x27; experiences provide insight on future technologies for multisensorial video streams and technology-supported commensality (eating with others). We surveyed 104 viewers and interviewed 15 of them about their attitudes and reflections on their mukbang viewing habits, their physiological aspects of watching someone eat, and their perceived social relationship with mukbangers. Based on our findings, we propose design implications for remote commensality, and for synchronized multisensorial video streaming content.</p><div class="ui labels">Keywords:  <span class="ui large grey label">video streams</span><span class="ui large grey label">mukbang</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Laurensia Anjani<!-- -->, <!-- -->Terrance Tin Hoi Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Wooi Boon Goh<!-- -->. <b>Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3313831.3376567" target="_blank">https://doi.org/10.1145/3313831.3376567</a></p></div></div></div></div></div><div id="chi-2020-goffin" class="ui large modal"><div class="header"><a href="/publications/chi-2020-goffin" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-goffin</a>  -  <a href="/publications/chi-2020-goffin.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-goffin.jpg"/></div><div class="thirteen wide column"><h1>Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</h1><p class="meta"><span>Pascal Goffin</span> , <span>Tanja Blascheck</span> , <span>Petra Isenberg</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/wPaVdSWM8hU?" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/wPaVdSWM8hU?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/wPaVdSWM8hU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We describe a design space of view manipulation interactions for small data-driven contextual visualizations (word-scale visualizations). These interaction techniques support an active reading experience and engage readers through exploration of embedded visualizations whose placement and content connect them to specific terms in a document. A reader could, for example, use our proposed interaction techniques to explore word-scale visualizations of stock market trends for companies listed in a market overview article. When readers wish to engage more deeply with the data, they can collect, arrange, compare, and navigate the document using the embedded word-scale visualizations, permitting more visualization-centric analyses. We support our design space with a concrete implementation, illustrate it with examples from three application domains, and report results from two experiments. The experiments show how view manipulation interactions helped readers examine embedded visualizations more quickly and with less scrolling and yielded qualitative feedback on usability and future opportunities.</p><div class="ui labels">Keywords:  <span class="ui large grey label">glyphs</span><span class="ui large grey label">word-scale visualization</span><span class="ui large grey label">information visualization</span><span class="ui large grey label">interaction techniques</span><span class="ui large grey label">text visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Pascal Goffin<!-- -->, <!-- -->Tanja Blascheck<!-- -->, <!-- -->Petra Isenberg<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3313831.3376842" target="_blank">https://doi.org/10.1145/3313831.3376842</a></p></div></div></div></div></div><div id="chi-2020-hou" class="ui large modal"><div class="header"><a href="/publications/chi-2020-hou" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2020-hou</a>  -  <a href="/publications/chi-2020-hou.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2020</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2020-hou.jpg"/></div><div class="thirteen wide column"><h1>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</h1><p class="meta"><span>Ming Hou</span> , <a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><strong>Karthik Mahadevan</strong></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p></p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ming Hou<!-- -->, <!-- -->Karthik Mahadevan<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Autonomous Vehicle-Cyclist Interaction: Peril and Promise</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;20)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div></div><div id="mobilehci-2019-hung" class="ui large modal"><div class="header"><a href="/publications/mobilehci-2019-hung" target="_blank"><i class="fas fa-link fa-fw"></i>mobilehci-2019-hung</a>  -  <a href="/publications/mobilehci-2019-hung.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">MobileHCI 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/mobilehci-2019-hung.jpg"/></div><div class="thirteen wide column"><h1>WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</h1><p class="meta"><span>Michael Hung</span> , <a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Pen-based input is often treated as auxiliary to mobile devices. We posit that cross-device interactions can inspire and extend the design space of pen-based interactions into new, expressive directions. We realize this through WatchPen, a smartwatch mounted on a passive, capacitive stylus that: (1) senses the usage context and leverages it for expression (e.g., changing colour), (2) contains tools and parameters within the display, and (3) acts as an on-demand output. As a result, it provides users with a dynamic relationship between inputs and outputs, awareness of current tool selection and parameters, and increased expressive match (e.g., added ability to mimic physical tools, showing clipboard contents). We discuss and reflect upon a series of interaction techniques that demonstrate WatchPen within a drawing application. We highlight the expressive power of leveraging multiple sensing and output capabilities across both the watch-augmented stylus and the tablet surface.</p><div class="ui labels">Keywords:  <span class="ui large grey label">smartwatch</span><span class="ui large grey label">cross-device interaction</span><span class="ui large grey label">pen interaction</span><span class="ui large grey label">interaction techniques</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Michael Hung<!-- -->, <!-- -->David Ledo<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction</b>. <i>In Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->8<!-- -->.  DOI: <a href="https://doi.org/10.1145/3338286.3340122" target="_blank">https://doi.org/10.1145/3338286.3340122</a></p></div></div></div></div></div><div id="tvcg-2019-walny" class="ui large modal"><div class="header"><a href="/publications/tvcg-2019-walny" target="_blank"><i class="fas fa-link fa-fw"></i>tvcg-2019-walny</a>  -  <a href="/publications/tvcg-2019-walny.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TVCG 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2019-walny.jpg"/></div><div class="thirteen wide column"><h1>Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff</h1><p class="meta"><span>Jagoda Walny</span> , <span>Christian Frisson</span> , <span>Mieka West</span> , <span>Doris Kosminsky</span> , <a href="/people/soren-knudsen"><img src="/static/images/people/soren-knudsen.jpg" class="ui circular spaced image mini-profile"/><strong>Søren Knudsen</strong></a> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://player.vimeo.com/video/360483702?" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://player.vimeo.com/video/360483702?autoplay=1&gt;&lt;img src=https://i.vimeocdn.com/video/814665539_640.webp&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.</p><div class="ui labels">Keywords:  <span class="ui large grey label">information visualization</span><span class="ui large grey label">design handoff</span><span class="ui large grey label">data mapping</span><span class="ui large grey label">design process</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jagoda Walny<!-- -->, <!-- -->Christian Frisson<!-- -->, <!-- -->Mieka West<!-- -->, <!-- -->Doris Kosminsky<!-- -->, <!-- -->Søren Knudsen<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff</b>. <i>In IEEE Transactions on Visualization and Computer Graphics (TVCG &#x27;19)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="https://doi.org/10.1109/TVCG.2019.2934538" target="_blank">https://doi.org/10.1109/TVCG.2019.2934538</a></p></div></div></div></div></div><div id="cnc-2019-hammad" class="ui large modal"><div class="header"><a href="/publications/cnc-2019-hammad" target="_blank"><i class="fas fa-link fa-fw"></i>cnc-2019-hammad</a>  -  <a href="/publications/cnc-2019-hammad.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">C&amp;C 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/cnc-2019-hammad.jpg"/></div><div class="thirteen wide column"><h1>Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning</h1><p class="meta"><a href="/people/nour-hammad"><img src="/static/images/people/nour-hammad.jpg" class="ui circular spaced image mini-profile"/><strong>Nour Hammad</strong></a> , <span>Elaheh Sanoubari</span> , <span>Patrick Finn</span> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <span>James E. Young</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p></p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nour Hammad<!-- -->, <!-- -->Elaheh Sanoubari<!-- -->, <!-- -->Patrick Finn<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->James E. Young<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning</b>. <i>In Proceedings of the ACM on Creativity and Cognition (C&amp;C &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div></div><div id="dis-2019-mahadevan" class="ui large modal"><div class="header"><a href="/publications/dis-2019-mahadevan" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2019-mahadevan</a>  -  <a href="/publications/dis-2019-mahadevan.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-mahadevan.jpg"/></div><div class="thirteen wide column"><h1>AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator</h1><p class="meta"><a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><strong>Karthik Mahadevan</strong></a> , <span>Elaheh Sanoubari</span> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <span>James E. Young</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>AV-pedestrian interaction will impact pedestrian safety, etiquette, and overall acceptance of AV technology. Evaluating AV-pedestrian interaction is challenging given limited availability of AVs and safety concerns. These challenges are compounded by &quot;mixed traffic&quot; conditions: studying AV-pedestrian interaction will be difficult in traffic consisting of vehicles varying in autonomy level. We propose immersive pedestrian simulators as design tools to study AV-pedestrian interaction, allowing rapid prototyping and evaluation of future AV-pedestrian interfaces. We present OnFoot: a VR-based simulator that immerses participants in mixed traffic conditions and allows examination of their behavior while controlling vehicles&#x27; autonomy-level, traffic and street characteristics, behavior of other virtual pedestrians, and integration of novel AV-pedestrian interfaces. We validated OnFoot against prior simulators and Wizard-of-Oz studies, and conducted a user study, manipulating vehicles&#x27; autonomy level, interfaces, and pedestrian group behavior. Our findings highlight the potential to use VR simulators as powerful tools for AV-pedestrian interaction design in mixed traffic.</p><div class="ui labels">Keywords:  <span class="ui large grey label">mixed traffic</span><span class="ui large grey label">pedestrian simulator</span><span class="ui large grey label">autonomous vehicle-pedestrian interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Karthik Mahadevan<!-- -->, <!-- -->Elaheh Sanoubari<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->James E. Young<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3322276.3322328" target="_blank">https://doi.org/10.1145/3322276.3322328</a></p></div></div></div></div></div><div id="tvcg-2019-blascheck" class="ui large modal"><div class="header"><a href="/publications/tvcg-2019-blascheck" target="_blank"><i class="fas fa-link fa-fw"></i>tvcg-2019-blascheck</a>  -  <a href="/publications/tvcg-2019-blascheck.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TVCG 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tvcg-2019-blascheck.jpg"/></div><div class="thirteen wide column"><h1>Exploration Strategies for Discovery of Interactivity in Visualizations</h1><p class="meta"><span>Tanja Blascheck</span> , <span>Lindsay MacDonald Vermeulen</span> , <span>Jo Vermeulen</span> , <span>Charles Perin</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <span>Thomas Ertl</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We investigate how people discover the functionality of an interactive visualization that was designed for the general public. While interactive visualizations are increasingly available for public use, we still know little about how the general public discovers what they can do with these visualizations and what interactions are available. Developing a better understanding of this discovery process can help inform the design of visualizations for the general public, which in turn can help make data more accessible. To unpack this problem, we conducted a lab study in which participants were free to use their own methods to discover the functionality of a connected set of interactive visualizations of public energy data. We collected eye movement data and interaction logs as well as video and audio recordings. By analyzing this combined data, we extract exploration strategies that the participants employed to discover the functionality in these interactive visualizations. These exploration strategies illuminate possible design directions for improving the discoverability of a visualization&#x27;s functionality.</p><div class="ui labels">Keywords:  <span class="ui large grey label">discovery</span><span class="ui large grey label">visualization</span><span class="ui large grey label">open data</span><span class="ui large grey label">evaluation</span><span class="ui large grey label">eye tracking</span><span class="ui large grey label">interaction logs</span><span class="ui large grey label">think-aloud</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Tanja Blascheck<!-- -->, <!-- -->Lindsay MacDonald Vermeulen<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Charles Perin<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Thomas Ertl<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->. <b>Exploration Strategies for Discovery of Interactivity in Visualizations</b>. <i>In IEEE Transactions on Visualization and Computer Graphics (TVCG &#x27;19)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1109/TVCG.2018.2802520" target="_blank">https://doi.org/10.1109/TVCG.2018.2802520</a></p></div></div></div></div></div><div id="dis-2019-ledo" class="ui large modal"><div class="header"><a href="/publications/dis-2019-ledo" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2019-ledo</a>  -  <a href="/publications/dis-2019-ledo.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-ledo.jpg"/></div><div class="thirteen wide column"><h1>Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</h1><p class="meta"><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <span>Jo Vermeulen</span> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><strong>Saul Greenberg</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Sebastian Boring</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Astral is a prototyping tool for authoring mobile and smart object interactive behaviours. It mirrors selected display contents of desktop applications onto mobile devices (smartphones and smartwatches), and streams/remaps mobile sensor data to desktop input events (mouse or keyboard) to manipulate selected desktop contents. This allows designers to use familiar desktop applications (e.g. PowerPoint, AfterEffects) to prototype rich interactive behaviours. Astral combines and integrates display mirroring, sensor streaming and input remapping, where designers can exploit familiar desktop applications to prototype, explore and fine-tune dynamic interactive behaviours. With Astral, designers can visually author rules to test real-time behaviours while interactions take place, as well as after the interaction has occurred. We demonstrate Astral&#x27;s applicability, workflow and expressiveness within the interaction design process through both new examples and replication of prior approaches that illustrate how various familiar desktop applications are leveraged and repurposed.</p><div class="ui labels">Keywords:  <span class="ui large grey label">smart objects</span><span class="ui large grey label">mobile interfaces</span><span class="ui large grey label">prototyping</span><span class="ui large grey label">design tool</span><span class="ui large grey label">interactive behaviour</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Saul Greenberg<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Sebastian Boring<!-- -->. <b>Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->14<!-- -->.  DOI: <a href="https://doi.org/10.1145/3322276.3322329" target="_blank">https://doi.org/10.1145/3322276.3322329</a></p></div></div></div></div></div><div id="dis-2019-bressa" class="ui large modal"><div class="header"><a href="/publications/dis-2019-bressa" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2019-bressa</a>  -  <a href="/publications/dis-2019-bressa.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2019-bressa.jpg"/></div><div class="thirteen wide column"><h1>Sketching and Ideation Activities for Situated Visualization Design</h1><p class="meta"><a href="/people/nathalie-bressa"><img src="/static/images/people/nathalie-bressa.jpg" class="ui circular spaced image mini-profile"/><strong>Nathalie Bressa</strong></a> , <a href="/people/kendra-wannamaker"><img src="/static/images/people/kendra-wannamaker.jpg" class="ui circular spaced image mini-profile"/><strong>Kendra Wannamaker</strong></a> , <span>Henrik Korsgaard</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <span>Jo Vermeulen</span></p></div></div></div><div class="block"><h1>Abstract</h1><p></p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nathalie Bressa<!-- -->, <!-- -->Kendra Wannamaker<!-- -->, <!-- -->Henrik Korsgaard<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Jo Vermeulen<!-- -->. <b>Sketching and Ideation Activities for Situated Visualization Design</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div></div><div id="chi-2019-danyluk" class="ui large modal"><div class="header"><a href="/publications/chi-2019-danyluk" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2019-danyluk</a>  -  <a href="/publications/chi-2019-danyluk.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2019-danyluk.jpg"/></div><div class="thirteen wide column"><h1>Look-From Camera Control for 3D Terrain Maps</h1><p class="meta"><span>Kurtis Thorvald Danyluk</span> , <span>Bernhard Jenny</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We introduce three lightweight interactive camera control techniques for 3D terrain maps on touch devices based on a look-from metaphor (Discrete Look-From-At, Continuous Look-From-Forwards, and Continuous Look-From-Towards). These techniques complement traditional touch screen pan, zoom, rotate, and pitch controls allowing viewers to quickly transition between top-down, oblique, and ground-level views. We present the results of a study in which we asked participants to perform elevation comparison and line-of-sight determination tasks using each technique. Our results highlight how look-from techniques can be integrated on top of current direct manipulation navigation approaches by combining several direct manipulation operations into a single look-from operation. Additionally, they show how look-from techniques help viewers complete a variety of common and challenging map-based tasks.</p><div class="ui labels">Keywords:  <span class="ui large grey label">terrain</span><span class="ui large grey label">touch</span><span class="ui large grey label">map interaction</span><span class="ui large grey label">look-from camera control</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Thorvald Danyluk<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Look-From Camera Control for 3D Terrain Maps</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3290605.3300594" target="_blank">https://doi.org/10.1145/3290605.3300594</a></p></div></div></div></div></div><div id="tei-2019-mikalauskas" class="ui large modal"><div class="header"><a href="/publications/tei-2019-mikalauskas" target="_blank"><i class="fas fa-link fa-fw"></i>tei-2019-mikalauskas</a>  -  <a href="/publications/tei-2019-mikalauskas.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TEI 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tei-2019-mikalauskas.jpg"/></div><div class="thirteen wide column"><h1>Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</h1><p class="meta"><span>Claire Mikalauskas</span> , <span>April Viczko</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>While improvised theatre (improv) is often performed on a bare stage, improvisers sometimes incorporate physical props to inspire new directions for a scene and to enrich their performance. A tech booth can improvise light and sound technical elements, but coordinating with improvisers&#x27; actions on-stage is challenging. Our goal is to inform the design of an augmented prop that lets improvisers tangibly control light and sound technical elements while performing. We interviewed five professional improvisers about their use of physical props in improv, and their expectations of a possible augmented prop that controls technical theatre elements. We propose a set of guidelines for the design of an augmented prop that fits with the existing world of unpredictable improvised performance.</p><div class="ui labels">Keywords:  <span class="ui large grey label">props</span><span class="ui large grey label">performer-controlled technology</span><span class="ui large grey label">improvisational theatre</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Claire Mikalauskas<!-- -->, <!-- -->April Viczko<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology</b>. <i>In Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction (TEI &#x27;19)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->9<!-- -->.  DOI: <a href="https://doi.org/10.1145/3294109.3295631" target="_blank">https://doi.org/10.1145/3294109.3295631</a></p></div></div></div></div></div><div id="vr-2019-satriadi" class="ui large modal"><div class="header"><a href="/publications/vr-2019-satriadi" target="_blank"><i class="fas fa-link fa-fw"></i>vr-2019-satriadi</a>  -  <a href="/publications/vr-2019-satriadi.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IEEE VR 2019</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/vr-2019-satriadi.jpg"/></div><div class="thirteen wide column"><h1>Augmented Reality Map Navigation with Freehand Gestures</h1><p class="meta"><span>Kadek Ananta Satriadi</span> , <span>Barrett Ens</span> , <span>Maxime Cordeil</span> , <span>Bernhard Jenny</span> , <span>Tobias Czauderna</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Freehand gesture interaction has long been proposed as a `natural&#x27; input method for Augmented Reality (AR) applications, yet has been little explored for intensive applications like multiscale navigation. In multiscale navigation, such as digital map navigation, pan and zoom are the predominant interactions. A position-based input mapping (e.g. grabbing metaphor) is intuitive for such interactions, but is prone to arm fatigue. This work focuses on improving digital map navigation in AR with mid-air hand gestures, using a horizontal intangible map display. First, we conducted a user study to explore the effects of handedness (unimanual and bimanual) and input mapping (position-based and rate-based). From these findings we designed DiveZoom and TerraceZoom, two novel hybrid techniques that smoothly transition between position- and rate-based mappings. A second user study evaluated these designs. Our results indicate that the introduced input-mapping transitions can reduce perceived arm fatigue with limited impact on performance.</p><div class="ui labels">Keywords:  <span class="ui large grey label">augmented reality</span><span class="ui large grey label">gesture recognition</span><span class="ui large grey label">human computer interaction</span><span class="ui large grey label">interactive devices</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kadek Ananta Satriadi<!-- -->, <!-- -->Barrett Ens<!-- -->, <!-- -->Maxime Cordeil<!-- -->, <!-- -->Bernhard Jenny<!-- -->, <!-- -->Tobias Czauderna<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Augmented Reality Map Navigation with Freehand Gestures</b>. <i>In undefined (IEEE VR &#x27;19)</i>. <!-- -->  Page: 1-<!-- -->11<!-- -->.  DOI: <a href="https://doi.org/10.1109/VR.2019.8798340" target="_blank">https://doi.org/10.1109/VR.2019.8798340</a></p></div></div></div></div></div><div id="hri-2018-feick" class="ui large modal"><div class="header"><a href="/publications/hri-2018-feick" target="_blank"><i class="fas fa-link fa-fw"></i>hri-2018-feick</a>  -  <a href="/publications/hri-2018-feick.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">HRI 2018</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/hri-2018-feick.jpg"/></div><div class="thirteen wide column"><h1>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</h1><p class="meta"><a href="/people/martin-feick"><img src="/static/images/people/martin-feick.jpg" class="ui circular spaced image mini-profile"/><strong>Martin Feick</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <span>André Miede</span> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we discuss the role of the movement trajectory and velocity enabled by our tele-robotic system (ReMa) for remote collaboration on physical tasks. Our system reproduces changes in object orientation and position at a remote location using a humanoid robotic arm. However, even minor kinematics differences between robot and human arm can result in awkward or exaggerated robot movements. As a result, user communication with the robotic system can become less efficient, less fluent and more time intensive.</p><div class="ui labels">Keywords:  <span class="ui large grey label">movement trajectory &amp; velocity</span><span class="ui large grey label">remote collaboration</span><span class="ui large grey label">robot surrogate</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Feick<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->André Miede<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration</b>. <i>In Adjunct Adjunct Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction (HRI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->2<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173386.3176959" target="_blank">https://doi.org/10.1145/3173386.3176959</a></p></div></div></div></div></div><div id="chi-2018-dillman" class="ui large modal"><div class="header"><a href="/publications/chi-2018-dillman" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-dillman</a>  -  <a href="/publications/chi-2018-dillman.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-dillman.jpg"/></div><div class="thirteen wide column"><h1>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</h1><p class="meta"><span>Kody R. Dillman</span> , <span>Terrance Tin Hoi Mok</span> , <a href="/people/anthony-tang"><img src="/static/images/people/anthony-tang.jpg" class="ui circular spaced image mini-profile"/><strong>Anthony Tang</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <span>Alex Mitchell</span></p></div></div></div><div class="block"><h1>Abstract</h1><p>Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences.</p><div class="ui labels">Keywords:  <span class="ui large grey label">game design</span><span class="ui large grey label">guidance</span><span class="ui large grey label">interaction cues</span><span class="ui large grey label">augmented reality</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kody R. Dillman<!-- -->, <!-- -->Terrance Tin Hoi Mok<!-- -->, <!-- -->Anthony Tang<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Alex Mitchell<!-- -->. <b>A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173714" target="_blank">https://doi.org/10.1145/3173574.3173714</a></p></div></div></div></div></div><div id="chi-2018-ledo" class="ui large modal"><div class="header"><a href="/publications/chi-2018-ledo" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-ledo</a>  -  <a href="/publications/chi-2018-ledo.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-ledo.jpg"/></div><div class="thirteen wide column"><h1>Evaluation Strategies for HCI Toolkit Research</h1><p class="meta"><a href="/people/david-ledo"><img src="/static/images/people/david-ledo.jpg" class="ui circular spaced image mini-profile"/><strong>David Ledo</strong></a> , <span>Steven Houben</span> , <span>Jo Vermeulen</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><strong>Nicolai Marquardt</strong></a> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a> , <a href="/people/saul-greenberg"><img src="/static/images/people/saul-greenberg.jpg" class="ui circular spaced image mini-profile"/><strong>Saul Greenberg</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/3lAwhCk60C4?" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/3lAwhCk60C4?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/3lAwhCk60C4/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what &#x27;evaluating&#x27; a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.</p><div class="ui labels">Keywords:  <span class="ui large grey label">user interfaces</span><span class="ui large grey label">design</span><span class="ui large grey label">evaluation</span><span class="ui large grey label">prototyping</span><span class="ui large grey label">toolkits</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">David Ledo<!-- -->, <!-- -->Steven Houben<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Nicolai Marquardt<!-- -->, <!-- -->Lora Oehlberg<!-- -->, <!-- -->Saul Greenberg<!-- -->. <b>Evaluation Strategies for HCI Toolkit Research</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->17<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3173610" target="_blank">https://doi.org/10.1145/3173574.3173610</a></p></div></div></div></div></div><div id="chi-2018-mahadevan" class="ui large modal"><div class="header"><a href="/publications/chi-2018-mahadevan" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2018-mahadevan</a>  -  <a href="/publications/chi-2018-mahadevan.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i> pdf</a></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2018</a></div><div class="ui grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2018-mahadevan.jpg"/></div><div class="thirteen wide column"><h1>Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction</h1><p class="meta"><a href="/people/karthik-mahadevan"><img src="/static/images/people/karthik-mahadevan.jpg" class="ui circular spaced image mini-profile"/><strong>Karthik Mahadevan</strong></a> , <a href="/people/sowmya-somanath"><img src="/static/images/people/sowmya-somanath.jpg" class="ui circular spaced image mini-profile"/><strong>Sowmya Somanath</strong></a> , <a href="/people/ehud-sharlin"><img src="/static/images/people/ehud-sharlin.jpg" class="ui circular spaced image mini-profile"/><strong>Ehud Sharlin</strong></a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/D_hhcGVREGA?" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/D_hhcGVREGA?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/D_hhcGVREGA/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Drivers use nonverbal cues such as vehicle speed, eye gaze, and hand gestures to communicate awareness and intent to pedestrians. Conversely, in autonomous vehicles, drivers can be distracted or absent, leaving pedestrians to infer awareness and intent from the vehicle alone. In this paper, we investigate the usefulness of interfaces (beyond vehicle movement) that explicitly communicate awareness and intent of autonomous vehicles to pedestrians, focusing on crosswalk scenarios. We conducted a preliminary study to gain insight on designing interfaces that communicate autonomous vehicle awareness and intent to pedestrians. Based on study outcomes, we developed four prototype interfaces and deployed them in studies involving a Segway and a car. We found interfaces communicating vehicle awareness and intent: (1) can help pedestrians attempting to cross; (2) are not limited to the vehicle and can exist in the environment; and (3) should use a combination of modalities such as visual, auditory, and physical.</p><div class="ui labels">Keywords:  <span class="ui large grey label">autonomous vehicle-pedestrian interaction</span><span class="ui large grey label">perceived awareness and intent in autonomous vehicles</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Karthik Mahadevan<!-- -->, <!-- -->Sowmya Somanath<!-- -->, <!-- -->Ehud Sharlin<!-- -->. <b>Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;18)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3173574.3174003" target="_blank">https://doi.org/10.1145/3173574.3174003</a></p></div></div></div></div></div></div><div class="ui vertical segment stackable" style="text-align:center"><a class="ui button" href="/publications">+ 20 more publications</a></div></div><div id="people" class="category"><h1 class="ui horizontal divider header"><i class="child icon"></i>People</h1><div class="people-category"><h2>Faculty</h2><div class="ui grid"><a class="four wide column person" href="/people/anthony-tang"><img class="ui circular image medium-profile" src="/static/images/people/anthony-tang.jpg"/><p><b>Anthony Tang</b></p><p>Adjunct Associate Professor</p></a><a class="four wide column person" href="/people/ehud-sharlin"><img class="ui circular image medium-profile" src="/static/images/people/ehud-sharlin.jpg"/><p><b>Ehud Sharlin</b></p><p>Professor</p></a><a class="four wide column person" href="/people/lora-oehlberg"><img class="ui circular image medium-profile" src="/static/images/people/lora-oehlberg.jpg"/><p><b>Lora Oehlberg</b></p><p>Assistant Professor</p></a><a class="four wide column person" href="/people/ryo-suzuki"><img class="ui circular image medium-profile" src="/static/images/people/ryo-suzuki.jpg"/><p><b>Ryo Suzuki</b></p><p>Assistant Professor</p></a><a class="four wide column person" href="/people/sheelagh-carpendale"><img class="ui circular image medium-profile" src="/static/images/people/sheelagh-carpendale.jpg"/><p><b>Sheelagh Carpendale</b></p><p>Adjunct Professor</p></a><a class="four wide column person" href="/people/saul-greenberg"><img class="ui circular image medium-profile" src="/static/images/people/saul-greenberg.jpg"/><p><b>Saul Greenberg</b></p><p>Emeritus Professor</p></a><a class="four wide column person" href="/people/wesley-willett"><img class="ui circular image medium-profile" src="/static/images/people/wesley-willett.jpg"/><p><b>Wesley Willett</b></p><p>Assistant Professor</p></a></div></div><div class="people-category"><h2>Post Docs</h2><div class="ui grid"><a class="four wide column person" href="/people/soren-knudsen"><img class="ui circular image medium-profile" src="/static/images/people/soren-knudsen.jpg"/><p><b>Søren Knudsen</b></p><p>Postdocotral Fellow</p></a></div></div><div class="people-category"><h2>PhD Students</h2><div class="ui grid"><a class="four wide column person" href="/people/bon-adriel-aseniero"><img class="ui circular image medium-profile" src="/static/images/people/bon-adriel-aseniero.jpg"/><p><b>Bon Adriel Aseniero</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/carmen-hull"><img class="ui circular image medium-profile" src="/static/images/people/carmen-hull.jpg"/><p><b>Carmen Hull</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/brennan-jones"><img class="ui circular image medium-profile" src="/static/images/people/brennan-jones.jpg"/><p><b>Brennan Jones</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/kurtis-danyluk"><img class="ui circular image medium-profile" src="/static/images/people/kurtis-danyluk.jpg"/><p><b>Kurtis Danyluk</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/roberta-cabral-mota"><img class="ui circular image medium-profile" src="/static/images/people/roberta-cabral-mota.jpg"/><p><b>Roberta Cabral Mota</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/martin-feick"><img class="ui circular image medium-profile" src="/static/images/people/martin-feick.jpg"/><p><b>Martin Feick</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/sydney-pratte"><img class="ui circular image medium-profile" src="/static/images/people/sydney-pratte.jpg"/><p><b>Sydney Pratte</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/terrance-mok"><img class="ui circular image medium-profile" src="/static/images/people/terrance-mok.jpg"/><p><b>Terrance Mok</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/tim-au-yeung"><img class="ui circular image medium-profile" src="/static/images/people/tim-au-yeung.jpg"/><p><b>Tim Au Yeung</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/zachary-mckendrick"><img class="ui circular image medium-profile" src="/static/images/people/zachary-mckendrick.jpg"/><p><b>Zachary McKendrick</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/darcy-norman"><img class="ui circular image medium-profile" src="/static/images/people/darcy-norman.jpg"/><p><b>D&#x27;Arcy Norman</b></p><p>PhD Student</p></a></div></div><div class="people-category"><h2>Masters Students</h2><div class="ui grid"><a class="four wide column person" href="/people/ashratuz-zavin-asha"><img class="ui circular image medium-profile" src="/static/images/people/ashratuz-zavin-asha.jpg"/><p><b>Ashratuz Zavin Asha</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/christopher-smith"><img class="ui circular image medium-profile" src="/static/images/people/christopher-smith.jpg"/><p><b>Christopher Smith</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/kendra-wannamaker"><img class="ui circular image medium-profile" src="/static/images/people/kendra-wannamaker.jpg"/><p><b>Kendra Wannamaker</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/sasha-ivanov"><img class="ui circular image medium-profile" src="/static/images/people/sasha-ivanov.jpg"/><p><b>Sasha Ivanov</b></p><p>MSc Student</p></a></div></div><div class="ui vertical segment stackable" style="text-align:center"><a class="ui button" href="/people">+ see more members</a></div></div><div id="location" class="category"><h1 class="ui horizontal divider header"><i class="map outline icon"></i>Location</h1><div id="map" class="ui grid"><div class="ten wide column"><div class="feature map"><iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d2506.544200524445!2d-114.1279042!3d51.079963549999995!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x53716f0c07993c17%3A0xb8f1352e9e5dfa06!2sMath+Science%2C+Calgary%2C+AB+T2N+4V8%2C+Canada!5e0!3m2!1sen!2sus!4v1439359680603" frameBorder="0" style="border:0"></iframe></div></div><div class="six wide column"><div class="ui segment"><h1>Interactions Lab</h1><p>680 Math Science Building,<br/>University of Calgary<br/>Calgary, AB T2N 4V8, Canada</p></div></div></div><div class="ui vertical segment stackable" style="text-align:center"><a class="ui button" href="/location">+ learn more about our space</a></div></div></div><div class="one wide column"></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img style="max-width:60px;margin:30px auto" src="/static/images/logo-3.png"/><div class="content"><h1 style="font-size:2.2rem">Interactions Lab</h1><div class="sub header">University of Calgary<br/>Department of Computer Science</div></div><img style="max-width:200px;margin:0px auto" src="/static/images/logo-1.png"/></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{}},"page":"/","query":{},"buildId":"8g1RMSDT1CNRhC_UbmcOV","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/" src="/_next/static/8g1RMSDT1CNRhC_UbmcOV/pages/index.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/8g1RMSDT1CNRhC_UbmcOV/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.98f1a4e4c94db9943918.js" async=""></script><script src="/_next/static/runtime/main-22d58f57abab872d6e70.js" async=""></script></body></html>