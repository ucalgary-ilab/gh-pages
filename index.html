<!DOCTYPE html><html><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="format-detection" content="telephone=no"/><link href="https://use.fontawesome.com/releases/v5.1.1/css/all.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.css" rel="stylesheet"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/css/lightbox.css" rel="stylesheet"/><link href="/assets/img/favicon.ico" rel="shortcut icon"/><link href="/static/css/style.css" rel="stylesheet"/><script src="https://code.jquery.com/jquery-3.2.1.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.0/js/lightbox-plus-jquery.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.0/semantic.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-62643728-2"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-62643728-2');
          </script><script>
            $(window).ready(function() {
              $('.ui.sidebar')
                .sidebar('attach events', '.sidebar.icon')

              $('.publication').on('click', function(event) {
                if (event.target.className === 'author-link') return
                const id = this.dataset.id
                $('#'+id).modal({
                  onHidden: function() {
                    const html = $(this).html()
                    $(this).html(html)
                  }
                })
                .modal('show')
              })
            })
          </script><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><meta charSet="utf-8" class="next-head"/><title class="next-head">Interactions Lab - University of Calgary HCI Group</title><meta name="keywords" content="Human-Computer Interaction, HCI, Information Visualization, University of Calgary, CHI, UIST" class="next-head"/><meta name="description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta property="og:title" content="Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta property="og:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta property="og:site_name" content="University of Calgary Interactions Lab" class="next-head"/><meta property="og:url" content="https://ilab.ucalgary.ca/" class="next-head"/><meta property="og:image" content="https://ilab.ucalgary.ca/static/images/cover.jpg" class="next-head"/><meta property="og:type" content="website" class="next-head"/><meta name="twitter:title" content="Interactions Lab - University of Calgary HCI Group" class="next-head"/><meta name="twitter:description" content="Human-Computer Interaction and Information Visualization Group at the University of Calgary" class="next-head"/><meta name="twitter:image" content="https://ilab.ucalgary.ca/static/images/cover.jpg" class="next-head"/><meta name="twitter:card" content="summary" class="next-head"/><meta name="twitter:site" content="@ucalgary" class="next-head"/><meta name="twitter:url" content="https://ilab.ucalgary.ca/" class="next-head"/><link rel="preload" href="/_next/static/IH3t5oTZZKsOXvNbIke9X/pages/index.js" as="script"/><link rel="preload" href="/_next/static/IH3t5oTZZKsOXvNbIke9X/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.2ccf7861fac39e850a30.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-ceb4d64d798d2348aa74.js" as="script"/></head><body><div id="__next"><div><div><div class="ui right vertical sidebar menu"><a class="item" href="/">Home</a><a class="item" href="/publications">Publications</a><a class="item" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/seminar">Seminar</a><a class="item" href="/location">Location</a></div><div class="ui stackable secondary pointing container menu" style="border-bottom:none;margin-right:15%;font-size:1.1em"><div class="left menu"><a class="item" href="/"><b>UCalgary iLab</b></a></div><div class="right menu"><a class="item" href="/publications">Publications</a><a class="item" href="/people">People</a><a class="item" href="/courses">Courses</a><a class="item" href="/facility">Facility</a><a class="item" href="/seminar">Seminar</a><a class="item" href="/location">Location</a><div class="toc item"><a href="/"><b>UCalgary iLab</b></a><i style="float:right" class="sidebar icon"></i></div></div></div></div><div id="top-video-container"><video id="top-video" poster="/static/posters/top.png" preload="metadata" autoplay="" loop="" muted="" playsinline="" webkit-playsinline=""><source src="/static/videos/top.mp4" type="video/mp4"/></video></div><div class="ui stackable grid"><div class="eleven wide column centered"><div id="header-logo"><div><img src="/static/images/logo-5.png" style="height:200px"/></div></div><div id="header" class="category"><img src="/static/images/logo-4.png" style="height:100px;margin-top:0px"/><h1 style="font-size:2em">Interactions Lab</h1><p>Human-Computer Interaction and Information Visualization Group</p></div><div><h1 class="ui horizontal divider header">Research Labs</h1><div id="labs" class="ui three cards" style="text-align:center;margin-top:15px"><div class="card" style="padding:15px"><a href="https://utouch.cpsc.ucalgary.ca/" target="_blank" class="ui "><div class="img card-color-0"><img src="/static/images/labs/utouch.png"/></div><h3>Physical Interaction and Human-Robot Interaction</h3><p class="header">Prof. <!-- -->Ehud Sharlin</p></a></div><div class="card" style="padding:15px"><a href="http://pages.cpsc.ucalgary.ca/~lora.oehlberg/" target="_blank" class="ui "><div class="img card-color-1"><img src="/static/images/labs/curiosity.png"/></div><h3>Human-Centered Design for Creativity &amp; Curiosity</h3><p class="header">Prof. <!-- -->Lora Oehlberg</p></a></div><div class="card" style="padding:15px"><a href="https://dataexperience.cpsc.ucalgary.ca/" target="_blank" class="ui "><div class="img card-color-2"><img src="/static/images/labs/dataexperience.png"/></div><h3>Visual Data-driven Tools and Experiences</h3><p class="header">Prof. <!-- -->Wesley Willett</p></a></div><div class="card" style="padding:15px"><a href="https://programmable-reality-lab.github.io/" target="_blank" class="ui "><div class="img card-color-3"><img src="/static/images/labs/suzuki.png"/></div><h3>Programmable Reality Lab - Tangible, AR/VR, and Robotics</h3><p class="header">Prof. <!-- -->Ryo Suzuki</p></a></div><div class="card" style="padding:15px"><a href="https://helenaihe.com/research/" target="_blank" class="ui "><div class="img card-color-4"><img src="/static/images/labs/c3-lab.png"/></div><h3>Tech to Bridge Cultural Barriers, Improve Collaboration, &amp; Build Community</h3><p class="header">Prof. <!-- -->Helen Ai He</p></a></div><div class="card" style="padding:15px"><a href="https://sites.google.com/site/adityanittala/" target="_blank" class="ui "><div class="img card-color-5"><img src="/static/images/labs/diff_lab.png"/></div><h3>Integrating of Interactive Computing into Everyday Environments</h3><p class="header">Prof. <!-- -->Aditya Shekhar Nittala</p></a></div><div class="card" style="padding:15px"><a href="http://grouplab.cpsc.ucalgary.ca/" target="_blank" class="ui "><div class="img card-color-6"><img src="/static/images/labs/grouplab.png"/></div><h3>Research in HCI, CSCW, and UbiComp</h3><p class="header">Prof. <!-- -->Saul Greenberg (Emeritus)</p></a></div><div class="card" style="padding:15px"><a href="https://ricelab.github.io/" target="_blank" class="ui "><div class="img card-color-7"><img src="/static/images/labs/ricelab.png"/></div><h3>Rethinking Interaction, Collaboration, &amp; Engagement</h3><p class="header">Prof. <!-- -->Anthony Tang (Adjunct - Singapore Management University)</p></a></div><div class="card" style="padding:15px"><a href="http://sheelaghcarpendale.ca/" target="_blank" class="ui "><div class="img card-color-8"><img src="/static/images/labs/innovis.png"/></div><h3>Innovations in Visualization Laboratory</h3><p class="header">Prof. <!-- -->Sheelagh Carpendale (Adjunct - Simon Fraser University)</p></a></div></div></div><div id="people" class="category"><h1 class="ui horizontal divider header"><i class="child icon"></i>Faculty</h1><div class="people-category"><h2></h2><div class="ui grid"><a class="five wide column person" href="/people/ehud-sharlin"><img class="ui circular image medium-profile" src="/static/images/people/ehud-sharlin.jpg"/><p><b>Ehud Sharlin</b></p><p>Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">HRI</span><span class="ui large inverted label label-brown-color">Robots</span><span class="ui large inverted label label-brown-color">Drones</span></div></a><a class="five wide column person" href="/people/lora-oehlberg"><img class="ui circular image medium-profile" src="/static/images/people/lora-oehlberg.jpg"/><p><b>Lora Oehlberg</b></p><p>Associate Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Tangible</span><span class="ui large inverted label label-brown-color">Design Tools</span></div></a><a class="five wide column person" href="/people/wesley-willett"><img class="ui circular image medium-profile" src="/static/images/people/wesley-willett.jpg"/><p><b>Wesley Willett</b></p><p>Associate Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Data Visualization</span><span class="ui large inverted label label-brown-color">Data Phyz</span><span class="ui large inverted label label-brown-color">AR</span></div></a><a class="five wide column person" href="/people/aditya-shekhar-nittala"><img class="ui circular image medium-profile" src="/static/images/people/aditya-shekhar-nittala.jpg"/><p><b>Aditya Shekhar Nittala</b></p><p>Assistant Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Wearable Computing</span><span class="ui large inverted label label-brown-color">Fabrication</span><span class="ui large inverted label label-brown-color">Interaction Techniques</span></div></a><a class="five wide column person" href="/people/saul-greenberg"><img class="ui circular image medium-profile" src="/static/images/people/saul-greenberg.jpg"/><p><b>Saul Greenberg</b></p><p>Emeritus Professor</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">UbiComp</span><span class="ui large inverted label label-brown-color">CSCW</span></div></a><a class="five wide column person" href="/people/ryo-suzuki"><img class="ui circular image medium-profile" src="/static/images/people/ryo-suzuki.jpg"/><p><b>Ryo Suzuki</b></p><p>Assistant Professor (CU Boulder)</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Tangible</span><span class="ui large inverted label label-brown-color">AR x AI</span><span class="ui large inverted label label-brown-color">Robots</span></div></a><a class="five wide column person" href="/people/anthony-tang"><img class="ui circular image medium-profile" src="/static/images/people/anthony-tang.jpg"/><p><b>Anthony Tang</b></p><p>Adjunct Associate Professor (Singapore Management University)</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Mixed Reality</span><span class="ui large inverted label label-brown-color">CSCW</span></div></a><a class="five wide column person" href="/people/sheelagh-carpendale"><img class="ui circular image medium-profile" src="/static/images/people/sheelagh-carpendale.jpg"/><p><b>Sheelagh Carpendale</b></p><p>Adjunct Professor (Simon Fraser University)</p><div class="ui large basic labels"><span class="ui large inverted label label-brown-color">Data Viz</span><span class="ui large inverted label label-brown-color">Data Phyz</span></div></a></div></div></div><div id="publications" class="category"><h1 class="ui horizontal divider header"><i class="file alternate outline icon"></i>Recent Publications</h1><div class="ui segment" style="margin-top:50px"><div class="publication ui vertical segment stackable grid" data-id="uist-2024-gunturu"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2024-gunturu.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2024</span><span class="ui big basic pink label"><b><i class="fas fa-trophy"></i> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>Augmented Physics: Creating Interactive and Embedded Physics Simulations from Static Textbook Diagrams</b></p><p><a href="/people/aditya-gunturu"><img src="/static/images/people/aditya-gunturu.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Gunturu</span></a> , <span>Yi Wen</span> , <a href="/people/nandi-zhang"><img src="/static/images/people/nandi-zhang.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nandi Zhang</span></a> , <a href="/people/jarin-thundathil"><img src="/static/images/people/jarin-thundathil.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Jarin Thundathil</span></a> , <span>Rubaiat Habib Kazi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Physics Education</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2024-danyluk"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2024-danyluk.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2024</span></p><p class="color" style="font-size:1.3em"><b>Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</b></p><p><a href="/people/kurtis-danyluk"><img src="/static/images/people/kurtis-danyluk.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kurtis Danyluk</span></a> , <span>Simon Klueber</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Gestural Input</span><span class="ui brown basic label">Microgestures</span><span class="ui brown basic label">AR</span><span class="ui brown basic label">Maps</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2024-bressa"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2024-bressa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2024</span></p><p class="color" style="font-size:1.3em"><b>Input Visualization: Collecting and Modifying Data with Visual Representations</b></p><p><a href="/people/nathalie-bressa"><img src="/static/images/people/nathalie-bressa.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nathalie Bressa</span></a> , <span>Jordan Louis</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <span>Samuel Huron</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Input Visualization</span><span class="ui brown basic label">Data Physicalization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2024-dhawka"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2024-dhawka.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2024</span></p><p class="color" style="font-size:1.3em"><b>Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</b></p><p><a href="/people/priya-dhawka"><img src="/static/images/people/priya-dhawka.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Priya Dhawka</span></a> , <span>Lauren Perera</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-chulpongsatorn"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</b></p><p><a href="/people/neil-chulpongsatorn"><img src="/static/images/people/neil-chulpongsatorn.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Neil Chulpongsatorn</span></a> , <a href="/people/mille-skovhus-lunding"><img src="/static/images/people/mille-skovhus-lunding.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mille Skovhus Lunding</span></a> , <a href="/people/nishan-soni"><img src="/static/images/people/no-profile-2.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nishan Soni</span></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-ihara"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-ihara.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</b></p><p><a href="/people/keiichi-ihara"><img src="/static/images/people/keiichi-ihara.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Keiichi Ihara</span></a> , <a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mehrad Faridan</span></a> , <span>Ayumi Ichikawa</span> , <span>Ikkaku Kawaguchi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Mobile Robots</span><span class="ui brown basic label">Actuated Tangible Ui</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-xia"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-xia.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects</b></p><p><a href="/people/zhijie-xia"><img src="/static/images/people/zhijie-xia.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Zhijie Xia</span></a> , <a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kyzyl Monteiro</span></a> , <a href="/people/kevin-van"><img src="/static/images/people/kevin-van.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kevin Van</span></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Scribble Animation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Real Time Authoring</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2023-xia2"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-xia2.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2023</span></p><p class="color" style="font-size:1.3em"><b>CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration</b></p><p><span>Haijun Xia</span> , <span>Tony Wang</span> , <a href="/people/aditya-gunturu"><img src="/static/images/people/aditya-gunturu.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Gunturu</span></a> , <span>Peiling Jiang</span> , <span>William Duan</span> , <span>Xiaoshuo Yao</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Videoconferencing</span><span class="ui brown basic label">Natural Language Interface</span><span class="ui brown basic label">Language Oriented Interaction</span><span class="ui brown basic label">Context Aware Computing</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="dis-2023-li"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2023-li.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">DIS 2023</span></p><p class="color" style="font-size:1.3em"><b>Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education</b></p><p><span>Jiatong Li</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Ken Nakagaki</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Physics Simulation</span><span class="ui brown basic label">Actuated Tangible Ui</span><span class="ui brown basic label">Swarm Ui</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-dhawka"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2023-dhawka.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>We are the Data: Challenges and Opportunities for Creating Demographically Diverse Anthropographics</b></p><p><a href="/people/priya-dhawka"><img src="/static/images/people/priya-dhawka.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Priya Dhawka</span></a> , <a href="/people/helen-ai-he"><img src="/static/images/people/helen-ai-he.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Helen Ai He</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-faridan"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2023-faridan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</b></p><p><a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mehrad Faridan</span></a> , <a href="/people/bheesha-kumari"><img src="/static/images/people/bheesha-kumari.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Bheesha Kumari</span></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Visual Cue</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Remote Guidance</span><span class="ui brown basic label">Human Surrogates</span><span class="ui brown basic label">Hands On Training</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2023-monteiro"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2023-monteiro.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2023</span></p><p class="color" style="font-size:1.3em"><b>Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</b></p><p><a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kyzyl Monteiro</span></a> , <a href="/people/ritik-vatsal"><img src="/static/images/people/ritik-vatsal.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ritik Vatsal</span></a> , <a href="/people/neil-chulpongsatorn"><img src="/static/images/people/neil-chulpongsatorn.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Neil Chulpongsatorn</span></a> , <span>Aman Parnami</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Tangible Interactions</span><span class="ui brown basic label">Everyday Objects</span><span class="ui brown basic label">Interactive Machine Teaching</span><span class="ui brown basic label">Human Centered Machine Learning</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-ea-2023-chulpongsatorn"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI EA 2023</span></p><p class="color" style="font-size:1.3em"><b>HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</b></p><p><a href="/people/neil-chulpongsatorn"><img src="/static/images/people/neil-chulpongsatorn.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Neil Chulpongsatorn</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-ea-2023-fang"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-ea-2023-fang.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI EA 2023</span></p><p class="color" style="font-size:1.3em"><b>VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences</b></p><p><span>Cathy Mengying Fang</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Daniel Leithinger</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Passive Haptics</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="tochi-2022-nittala"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tochi-2022-nittala.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">TOCHI 2022</span></p><p class="color" style="font-size:1.3em"><b>SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</b></p><p><span>Adwait Sharma</span> , <span>Christina Salchow-Hömmen</span> , <span>Vimal Suresh Mollyn</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <span>Michael A. Hedderich</span> , <span>Marion Koelle</span> , <span>Thomas Seel</span> , <span>Jürgen Steimle</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Hand Gestures</span><span class="ui brown basic label">Sensor Placement</span><span class="ui brown basic label">IMU</span><span class="ui brown basic label">Objects</span><span class="ui brown basic label">Design Tool</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-kaimoto"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-kaimoto.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</b></p><p><a href="/people/hiroki-kaimoto"><img src="/static/images/people/hiroki-kaimoto.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Hiroki Kaimoto</span></a> , <a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kyzyl Monteiro</span></a> , <a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mehrad Faridan</span></a> , <span>Jiatong Li</span> , <a href="/people/samin-farajian"><img src="/static/images/people/samin-farajian.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Samin Farajian</span></a> , <span>Yasuaki Kakehi</span> , <span>Ken Nakagaki</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Actuated Tangible Interfaces</span><span class="ui brown basic label">Swarm User Interfaces</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-liao"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-liao.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</b></p><p><a href="/people/jian-liao"><img src="/static/images/people/jian-liao.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Jian Liao</span></a> , <a href="/people/adnan-karim"><img src="/static/images/people/adnan-karim.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Adnan Karim</span></a> , <a href="/people/shivesh-jadon"><img src="/static/images/people/shivesh-jadon.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Shivesh Jadon</span></a> , <span>Rubaiat Habib Kazi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Augmented Presentation</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Gestural And Speech Input</span><span class="ui brown basic label">Video</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-nisser"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-nisser.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span></p><p class="color" style="font-size:1.3em"><b>Mixels: Fabricating Interfaces using Programmable Magnetic Pixels</b></p><p><span>Martin Nisser</span> , <span>Yashaswini Makaram</span> , <span>Lucian Covarrubias</span> , <span>Amadou Bah</span> , <span>Faraz Faruqi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Stefanie Mueller</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Programmable Materials</span><span class="ui brown basic label">Magnetic Interfaces</span><span class="ui brown basic label">Fabrication</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2022-nittala"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-nittala.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2022</span><span class="ui big basic pink label"><b><i class="fas fa-trophy"></i> Best Paper</b></span></p><p class="color" style="font-size:1.3em"><b>Prototyping Soft Devices with Interactive Bioplastics</b></p><p><span>Marion Koelle</span> , <span>Madalina Nicolae</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <span>Marc Teyssier</span> , <span>Jürgen Steimle</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Bioplastics</span><span class="ui brown basic label">Biomaterials</span><span class="ui brown basic label">Do It Yourself</span><span class="ui brown basic label">DIY</span><span class="ui brown basic label">Sustainability</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-sic-2022-faridan"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-sic-2022-faridan.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST SIC 2022</span><span class="ui big basic pink label"><b><i class="fas fa-award"></i> Honorable Mention</b></span></p><p class="color" style="font-size:1.3em"><b>UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers</b></p><p><a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Mehrad Faridan</span></a> , <a href="/people/marcus-friedel"><img src="/static/images/people/marcus-friedel.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Marcus Friedel</span></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Ultrasound Transducers</span><span class="ui brown basic label">Robotics</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="iros-2022-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/iros-2022-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">IROS 2022</span></p><p class="color" style="font-size:1.3em"><b>Selective Self-Assembly using Re-Programmable Magnetic Pixels</b></p><p><span>Martin Nisser</span> , <span>Yashaswini Makaram</span> , <span>Faraz Faruqi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Stefanie Mueller</span></p><p></p></div></div><div class="publication ui vertical segment stackable grid" data-id="gecco-2022-ivanov"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/gecco-2022-ivanov.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">GECCO 2022</span></p><p class="color" style="font-size:1.3em"><b>EvoIsland: Interactive Evolution via an Island-Inspired Spatial User Interface Framework</b></p><p><a href="/people/sasha-ivanov"><img src="/static/images/people/sasha-ivanov.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sasha Ivanov</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a> , <span>Christian Jacob</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Interactive Evolutionary Systems</span><span class="ui brown basic label">User Interfaces</span><span class="ui brown basic label">Visualization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="gi-2022-hull"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/gi-2022-hull.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">GI 2022</span></p><p class="color" style="font-size:1.3em"><b>Simultaneous Worlds: Supporting Fluid Exploration of Multiple Data Sets via Physical Models</b></p><p><a href="/people/carmen-hull"><img src="/static/images/people/carmen-hull.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Carmen Hull</span></a> , <a href="/people/soren-knudsen"><img src="/static/images/people/soren-knudsen.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Søren Knudsen</span></a> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sheelagh Carpendale</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Interactive Surfaces</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Architectural Models</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-bressa"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-bressa.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>Data Every Day: Designing and Living with Personal Situated Visualizations</b></p><p><a href="/people/nathalie-bressa"><img src="/static/images/people/nathalie-bressa.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nathalie Bressa</span></a> , <span>Jo Vermeulen</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Self Tracking</span><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Personal Data</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-ivanov"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-ivanov.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>One Week in the Future: Previs Design Futuring for HCI Research</b></p><p><a href="/people/sasha-ivanov"><img src="/static/images/people/sasha-ivanov.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sasha Ivanov</span></a> , <a href="/people/tim-au-yeung"><img src="/static/images/people/tim-au-yeung.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Tim Au Yeung</span></a> , <a href="/people/kathryn-blair"><img src="/static/images/people/kathryn-blair.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kathryn Blair</span></a> , <a href="/people/kurtis-danyluk"><img src="/static/images/people/kurtis-danyluk.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kurtis Danyluk</span></a> , <a href="/people/georgina-freeman"><img src="/static/images/people/georgina-freeman.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Georgina Freeman</span></a> , <a href="/people/marcus-friedel"><img src="/static/images/people/marcus-friedel.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Marcus Friedel</span></a> , <a href="/people/carmen-hull"><img src="/static/images/people/carmen-hull.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Carmen Hull</span></a> , <a href="/people/michael-hung"><img src="/static/images/people/michael-hung.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Michael Hung</span></a> , <a href="/people/sydney-pratte"><img src="/static/images/people/sydney-pratte.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Sydney Pratte</span></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Wesley Willett</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Design Futuring</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Previsualization</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-nittala"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-nittala.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices</b></p><p><a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Aditya Shekhar Nittala</span></a> , <span>Jürgen Steimle</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Wearable Devices</span><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Survey</span><span class="ui brown basic label">Soft Wearables</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-2022-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI 2022</span></p><p class="color" style="font-size:1.3em"><b>Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <a href="/people/adnan-karim"><img src="/static/images/people/adnan-karim.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Adnan Karim</span></a> , <a href="/people/tian-xia"><img src="/static/images/people/tian-xia.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Tian Xia</span></a> , <span>Hooman Hedayati</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Nicolai Marquardt</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Robotics</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Actuated Tangible UI</span><span class="ui brown basic label">Shape Changing UI</span><span class="ui brown basic label">AR HRI</span><span class="ui brown basic label">VAM HRI</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="chi-ea-2022-blair"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-ea-2022-blair.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">CHI EA 2022</span></p><p class="color" style="font-size:1.3em"><b>Art is Not Research. Research is not Art</b></p><p><a href="/people/kathryn-blair"><img src="/static/images/people/kathryn-blair.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Kathryn Blair</span></a> , <span>Miriam Sturdee</span> , <span>Lindsay Macdonald Vermeulen</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Lora Oehlberg</span></a></p><p><div class="ui large basic labels"><span class="ui brown basic label">Interdisciplinary Research</span><span class="ui brown basic label">Research Ethics</span><span class="ui brown basic label">Arts And Computing</span><span class="ui brown basic label">Research Methods</span><span class="ui brown basic label">Knowledge Creation</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="frobt-2022-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/frobt-2022-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">Frontiers 2022</span></p><p class="color" style="font-size:1.3em"><b>Designing Expandable-Structure Robots for Human-Robot Interaction</b></p><p><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Wyatt Rees1</span> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Deployable Robot</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Modular Robot</span><span class="ui brown basic label">Origami Robotics</span><span class="ui brown basic label">Deployable Structures</span><span class="ui brown basic label">Shape Changing Robots</span></div></p></div></div><div class="publication ui vertical segment stackable grid" data-id="uist-2021-suzuki"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2021-suzuki.jpg"/></div><div class="thirteen wide column"><p><span class="ui big inverted label label-color">UIST 2021</span></p><p class="color" style="font-size:1.3em"><b>HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots</b></p><p><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><span class="author-link">Ryo Suzuki</span></a> , <span>Eyal Ofek</span> , <span>Mike Sinclair</span> , <span>Daniel Leithinger</span> , <span>Mar Gonzalez-Franco</span></p><p><div class="ui large basic labels"><span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Encountered Type Haptics</span><span class="ui brown basic label">Tabletop Mobile Robots</span><span class="ui brown basic label">Swarm User Interfaces</span></div></p></div></div></div><div id="publications-modal"><div id="uist-2024-gunturu" class="ui large modal"><div class="header"><a href="/publications/uist-2024-gunturu" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2024-gunturu</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2024-gunturu.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2024-gunturu" target="_blank">Augmented Physics: Creating Interactive and Embedded Physics Simulations from Static Textbook Diagrams</a></h1><p class="meta"><a href="/people/aditya-gunturu"><img src="/static/images/people/aditya-gunturu.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Gunturu</strong></a> , <span>Yi Wen</span> , <a href="/people/nandi-zhang"><img src="/static/images/people/nandi-zhang.jpg" class="ui circular spaced image mini-profile"/><strong>Nandi Zhang</strong></a> , <a href="/people/jarin-thundathil"><img src="/static/images/people/jarin-thundathil.jpg" class="ui circular spaced image mini-profile"/><strong>Jarin Thundathil</strong></a> , <span>Rubaiat Habib Kazi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/uist-2024-gunturu.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2024-gunturu.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/MOdSeUp8YcE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/MOdSeUp8YcE?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/MOdSeUp8YcE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce Augmented Physics, a machine learning-integrated authoring tool designed for creating embedded interactive physics simulations from static textbook diagrams. Leveraging recent advancements in computer vision, such as Segment Anything and Multi-modal LLMs, our web-based system enables users to semi-automatically extract diagrams from physics textbooks and generate interactive simulations based on the extracted content. These interactive diagrams are seamlessly integrated into scanned textbook pages, facilitating interactive and personalized learning experiences across various physics concepts, such as optics, circuits, and kinematics. Drawing from an elicitation study with seven physics instructors, we explore four key augmentation strategies: 1) augmented experiments, 2) animated diagrams, 3) bi-directional binding, and 4) parameter visualization. We evaluate our system through technical evaluation, a usability study (N=12), and expert interviews (N=12). Study findings suggest that our system can facilitate more engaging and personalized learning experiences in physics education.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Physics Education</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Aditya Gunturu<!-- -->, <!-- -->Yi Wen<!-- -->, <!-- -->Nandi Zhang<!-- -->, <!-- -->Jarin Thundathil<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Augmented Physics: Creating Interactive and Embedded Physics Simulations from Static Textbook Diagrams</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;24)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3654777.3676392" target="_blank">https://doi.org/10.1145/3654777.3676392</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2024-danyluk" class="ui large modal"><div class="header"><a href="/publications/dis-2024-danyluk" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2024-danyluk</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2024-danyluk.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2024-danyluk" target="_blank">Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</a></h1><p class="meta"><a href="/people/kurtis-danyluk"><img src="/static/images/people/kurtis-danyluk.jpg" class="ui circular spaced image mini-profile"/><strong>Kurtis Danyluk</strong></a> , <span>Simon Klueber</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/dis-2024-danyluk.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>dis-2024-danyluk.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We explore the potential for subtle on-hand gesture and microgesture interactions for map navigation with augmented reality (AR) devices. We describe a design exercise and follow-up elicitation study in which we identified on-hand gestures for cartographic interaction primitives. Microgestures and on-hand interactions are a promising space for AR map navigation as they offers always-available, tactile, and memorable spaces for interaction. Our findings show a clear set of microgesture interaction patterns that are well suited for supporting map navigation and manipulation. In particular, we highlight how the properties of various microgestures align with particular cartographic interaction tasks. We also describe our experience creating an exploratory proof-of-concept AR map prototype which helped us identify new opportunities and practical challenges for microgesture control. Finally, we discuss how future AR map systems could benefit from on-hand and microgesture input schemes.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Gestural Input</span><span class="ui brown basic label">Microgestures</span><span class="ui brown basic label">AR</span><span class="ui brown basic label">Maps</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kurtis Danyluk<!-- -->, <!-- -->Simon Klueber<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Understanding Gesture and Microgesture Inputs for Augmented Reality Maps</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;24)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a href="https://doi.org/10.1145/3643834.3661630" target="_blank">https://doi.org/10.1145/3643834.3661630</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2024-bressa" class="ui large modal"><div class="header"><a href="/publications/chi-2024-bressa" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2024-bressa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2024-bressa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2024-bressa" target="_blank">Input Visualization: Collecting and Modifying Data with Visual Representations</a></h1><p class="meta"><a href="/people/nathalie-bressa"><img src="/static/images/people/nathalie-bressa.jpg" class="ui circular spaced image mini-profile"/><strong>Nathalie Bressa</strong></a> , <span>Jordan Louis</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <span>Samuel Huron</span></p><p><a href="/static/publications/chi-2024-bressa.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2024-bressa.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/RAfv2quE6nA" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/RAfv2quE6nA?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/RAfv2quE6nA/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We examine input visualizations, visual representations that are designed to collect (and represent) new data rather than encode preexisting datasets. Information visualization is commonly used to reveal insights and stories within existing data. As a result, most contemporary visualization approaches assume existing datasets as the starting point for design, through which that data is mapped to visual encodings. Meanwhile, the implications of visualizations as inputs and as data sources have received little attention—despite the existence of visual and physical examples stretching back centuries. In this paper, we present a design space of 50 input visualizations analyzing their visual representation, data, artifact, context, and input. Based on this, we identify input modalities, purposes of input visualizations, and a set of design considerations. Finally, we discuss the relationship between input visualization and traditional visualization design and suggest opportunities for future research to better understand these visual representations and their potential.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Input Visualization</span><span class="ui brown basic label">Data Physicalization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nathalie Bressa<!-- -->, <!-- -->Jordan Louis<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Samuel Huron<!-- -->. <b>Input Visualization: Collecting and Modifying Data with Visual Representations</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;24)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a href="https://doi.org/10.1145/3613904.3642808" target="_blank">https://doi.org/10.1145/3613904.3642808</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2024-dhawka" class="ui large modal"><div class="header"><a href="/publications/chi-2024-dhawka" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2024-dhawka</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2024</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2024-dhawka.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2024-dhawka" target="_blank">Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</a></h1><p class="meta"><a href="/people/priya-dhawka"><img src="/static/images/people/priya-dhawka.jpg" class="ui circular spaced image mini-profile"/><strong>Priya Dhawka</strong></a> , <span>Lauren Perera</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/chi-2024-dhawka.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2024-dhawka.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/dCEFvx4AqIo" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/dCEFvx4AqIo?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/dCEFvx4AqIo/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the potential of generative AI text-to-image models to help designers efficiently craft unique, representative, and demographically diverse anthropographics that visualize data about people. Currently, creating data-driven iconic images to represent individuals in a dataset often requires considerable design effort. Generative text-to-image models can streamline the process of creating these images, but risk perpetuating designer biases in addition to stereotypes latent in the models. In response, we outline a conceptual workflow for crafting anthropographic assets for visualizations, highlighting possible sources of risk and bias as well as opportunities for reflection and refinement by a human designer. Using an implementation of this workflow with Stable Diffusion and Google Colab, we illustrate a variety of new anthropographic designs that showcase the visual expressiveness and scalability of these generative approaches. Based on our experiments, we also identify challenges and research opportunities for new AI-enabled anthropographic visualization tools.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Priya Dhawka<!-- -->, <!-- -->Lauren Perera<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Better Little People Pictures: Generative Creation of Demographically Diverse Anthropographics</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;24)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->.  DOI: <a href="https://doi.org/10.1145/3613904.3641957" target="_blank">https://doi.org/10.1145/3613904.3641957</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-chulpongsatorn" class="ui large modal"><div class="header"><a href="/publications/uist-2023-chulpongsatorn" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2023-chulpongsatorn</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-chulpongsatorn" target="_blank">Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</a></h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img src="/static/images/people/neil-chulpongsatorn.jpg" class="ui circular spaced image mini-profile"/><strong>Neil Chulpongsatorn</strong></a> , <a href="/people/mille-skovhus-lunding"><img src="/static/images/people/mille-skovhus-lunding.jpg" class="ui circular spaced image mini-profile"/><strong>Mille Skovhus Lunding</strong></a> , <a href="/people/nishan-soni"><img src="/static/images/people/nishan-soni.jpg" class="ui circular spaced image mini-profile"/><strong>Nishan Soni</strong></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/uist-2023-chulpongsatorn.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2023-chulpongsatorn.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Zv6JQ5T-qn0" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Zv6JQ5T-qn0?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/Zv6JQ5T-qn0/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Explorable Explanations</span><span class="ui brown basic label">Interactive Paper</span><span class="ui brown basic label">Augmented Textbook</span><span class="ui brown basic label">Authoring Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Neil Chulpongsatorn<!-- -->, <!-- -->Mille Skovhus Lunding<!-- -->, <!-- -->Nishan Soni<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3586183.3606827" target="_blank">https://doi.org/10.1145/3586183.3606827</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-ihara" class="ui large modal"><div class="header"><a href="/publications/uist-2023-ihara" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2023-ihara</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-ihara.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-ihara" target="_blank">HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</a></h1><p class="meta"><a href="/people/keiichi-ihara"><img src="/static/images/people/keiichi-ihara.jpg" class="ui circular spaced image mini-profile"/><strong>Keiichi Ihara</strong></a> , <a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><strong>Mehrad Faridan</strong></a> , <span>Ayumi Ichikawa</span> , <span>Ikkaku Kawaguchi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/uist-2023-ihara.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2023-ihara.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/KSBPtiXy8Hg" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/KSBPtiXy8Hg?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/KSBPtiXy8Hg/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces HoloBots, a mixed reality remote collaboration system that augments holographic telepresence with synchronized mobile robots. Beyond existing mixed reality telepresence, HoloBots lets remote users not only be visually and spatially present, but also physically engage with local users and their environment. HoloBots allows the users to touch, grasp, manipulate, and interact with the remote physical environment as if they were co-located in the same shared space. We achieve this by synchronizing holographic user motion (Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio). Beyond the existing physical telepresence, HoloBots contributes to an exploration of broader design space, such as object actuation, virtual hand physicalization, world-in-miniature exploration, shared tangible interfaces, embodied guidance, and haptic communication. We evaluate our system with twelve participants by comparing it with hologram-only and robot-only conditions. Both quantitative and qualitative results confirm that our system significantly enhances the level of co-presence and shared experience, compared to the other conditions.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Physical Telepresence</span><span class="ui brown basic label">Mobile Robots</span><span class="ui brown basic label">Actuated Tangible Ui</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Keiichi Ihara<!-- -->, <!-- -->Mehrad Faridan<!-- -->, <!-- -->Ayumi Ichikawa<!-- -->, <!-- -->Ikkaku Kawaguchi<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3586183.3606727" target="_blank">https://doi.org/10.1145/3586183.3606727</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-xia" class="ui large modal"><div class="header"><a href="/publications/uist-2023-xia" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2023-xia</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-xia.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-xia" target="_blank">RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects</a></h1><p class="meta"><a href="/people/zhijie-xia"><img src="/static/images/people/zhijie-xia.jpg" class="ui circular spaced image mini-profile"/><strong>Zhijie Xia</strong></a> , <a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><strong>Kyzyl Monteiro</strong></a> , <a href="/people/kevin-van"><img src="/static/images/people/kevin-van.jpg" class="ui circular spaced image mini-profile"/><strong>Kevin Van</strong></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/uist-2023-xia.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2023-xia.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HVOgH1quDsc" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HVOgH1quDsc?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/HVOgH1quDsc/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce RealityCanvas, a mobile AR sketching tool that can easily augment real-world physical motion with responsive hand-drawn animation. Recent research in AR sketching tools has enabled users to not only embed static drawings into the real world but also dynamically animate them with physical motion. However, existing tools often lack the flexibility and expressiveness of possible animations, as they primarily support simple line-based geometry. To address this limitation, we explore both expressive and improvisational AR sketched animation by introducing a set of responsive scribble animation techniques that can be directly embedded through sketching interactions: 1) object binding, 2) flip-book animation, 3) action trigger, 4) particle effects, 5) motion trajectory, and 6) contour highlight. These six animation effects were derived from the analysis of 172 existing video-edited scribble animations. We showcase these techniques through various applications, such as video creation, augmented education, storytelling, and AR prototyping. The results of our user study and expert interviews confirm that our tool can lower the barrier to creating AR-based sketched animation, while allowing creative, expressive, and improvisational AR sketching experiences.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Sketching Interfaces</span><span class="ui brown basic label">Scribble Animation</span><span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Real Time Authoring</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Zhijie Xia<!-- -->, <!-- -->Kyzyl Monteiro<!-- -->, <!-- -->Kevin Van<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->14<!-- -->.  DOI: <a href="https://doi.org/10.1145/3586183.3606716" target="_blank">https://doi.org/10.1145/3586183.3606716</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2023-xia2" class="ui large modal"><div class="header"><a href="/publications/uist-2023-xia2" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2023-xia2</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2023-xia2.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2023-xia2" target="_blank">CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration</a></h1><p class="meta"><span>Haijun Xia</span> , <span>Tony Wang</span> , <a href="/people/aditya-gunturu"><img src="/static/images/people/aditya-gunturu.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Gunturu</strong></a> , <span>Peiling Jiang</span> , <span>William Duan</span> , <span>Xiaoshuo Yao</span></p><p><a href="/static/publications/uist-2023-xia2.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2023-xia2.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/8I1yXNRcm54" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/8I1yXNRcm54?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/8I1yXNRcm54/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Despite the advances and ubiquity of digital communication media such as videoconferencing and virtual reality, they remain oblivious to the rich intentions expressed by users. Beyond transmitting audio, videos, and messages, we envision digital communication media as proactive facilitators that can provide unobtrusive assistance to enhance communication and collaboration. Informed by the results of a formative study, we propose three key design concepts to explore the systematic integration of intelligence into communication and collaboration, including the panel substrate, language-based intent recognition, and lightweight interaction techniques. We developed CrossTalk, a videoconferencing system that instantiates these concepts, which was found to enable a more fluid and flexible communication and collaboration experience.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Videoconferencing</span><span class="ui brown basic label">Natural Language Interface</span><span class="ui brown basic label">Language Oriented Interaction</span><span class="ui brown basic label">Context Aware Computing</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Haijun Xia<!-- -->, <!-- -->Tony Wang<!-- -->, <!-- -->Aditya Gunturu<!-- -->, <!-- -->Peiling Jiang<!-- -->, <!-- -->William Duan<!-- -->, <!-- -->Xiaoshuo Yao<!-- -->. <b>CrossTalk: Intelligent Substrates for Language-Oriented Interaction in Video-Based Communication and Collaboration</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3586183.3606773" target="_blank">https://doi.org/10.1145/3586183.3606773</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="dis-2023-li" class="ui large modal"><div class="header"><a href="/publications/dis-2023-li" target="_blank"><i class="fas fa-link fa-fw"></i>dis-2023-li</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">DIS 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/dis-2023-li.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/dis-2023-li" target="_blank">Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education</a></h1><p class="meta"><span>Jiatong Li</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Ken Nakagaki</span></p><p><a href="/static/publications/dis-2023-li.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>dis-2023-li.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/7DKpq52282g" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/7DKpq52282g?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/7DKpq52282g/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we introduce Physica, a tangible physics simulation system and approach based on tabletop mobile robots. In Physica, each tabletop robot can physically represent distinct simulated objects that are controlled through an underlying physics simulation, such as gravitational force, molecular movement, and spring force. It aims to bring the benefits of tangible and haptic interaction into explorable physics learning, which was traditionally only available on screen-based interfaces. The system utilizes off-the-shelf mobile robots (Sony Toio) and an open-source physics simulation tool (Teilchen). Built on top of them, we implement the interaction software pipeline that consists of 1) an event detector to reflect tangible interaction by users, and 2) target speed control to minimize the gap between the robot motion and simulated moving objects. To present the potential for physics education, we demonstrate various application scenarios that illustrate different forms of learning using Physica. In our user study, we investigate the effect and the potential of our approach through a perception study and interviews with physics educators.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Physics Simulation</span><span class="ui brown basic label">Actuated Tangible Ui</span><span class="ui brown basic label">Swarm Ui</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jiatong Li<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Ken Nakagaki<!-- -->. <b>Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education</b>. <i>In Proceedings of the ACM on Designing Interactive Systems Conference (DIS &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->15<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526113.3545626" target="_blank">https://doi.org/10.1145/3526113.3545626</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-dhawka" class="ui large modal"><div class="header"><a href="/publications/chi-2023-dhawka" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2023-dhawka</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2023-dhawka.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-dhawka" target="_blank">We are the Data: Challenges and Opportunities for Creating Demographically Diverse Anthropographics</a></h1><p class="meta"><a href="/people/priya-dhawka"><img src="/static/images/people/priya-dhawka.jpg" class="ui circular spaced image mini-profile"/><strong>Priya Dhawka</strong></a> , <a href="/people/helen-ai-he"><img src="/static/images/people/helen-ai-he.jpg" class="ui circular spaced image mini-profile"/><strong>Helen Ai He</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/chi-2023-dhawka.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2023-dhawka.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/iBzv2jS3ECM" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/iBzv2jS3ECM?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/iBzv2jS3ECM/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Anthropographics are human-shaped visualizations that aim to emphasize the human importance of datasets and the people behind them. However, current anthropographics tend to employ homogeneous human shapes to encode data about diverse demographic groups. Such anthropographics can obscure important differences between groups and contemporary designs exemplify the lack of inclusive approaches for representing human diversity in visualizations. In response, we explore the creation of demographically diverse anthropographics that communicate the visible diversity of demographically distinct populations. Building on previous anthropographics research, we explore strategies for visualizing datasets about people in ways that explicitly encode diversity—illustrating these approaches with examples in a variety of visual styles. We also critically reflect on strategies for creating diverse anthropographics, identifying social and technical challenges that can result in harmful representations. Finally, we highlight a set of forward-looking research opportunities for advancing the design and understanding of diverse anthropographics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Anthropographics</span><span class="ui brown basic label">Demographic Data</span><span class="ui brown basic label">Diversity</span><span class="ui brown basic label">Marginalized Populations</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Priya Dhawka<!-- -->, <!-- -->Helen Ai He<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>We are the Data: Challenges and Opportunities for Creating Demographically Diverse Anthropographics</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->14<!-- -->.  DOI: <a href="https://doi.org/10.1145/3544548.3581086" target="_blank">https://doi.org/10.1145/3544548.3581086</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-faridan" class="ui large modal"><div class="header"><a href="/publications/chi-2023-faridan" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2023-faridan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2023-faridan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-faridan" target="_blank">ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</a></h1><p class="meta"><a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><strong>Mehrad Faridan</strong></a> , <a href="/people/bheesha-kumari"><img src="/static/images/people/bheesha-kumari.jpg" class="ui circular spaced image mini-profile"/><strong>Bheesha Kumari</strong></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/chi-2023-faridan.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2023-faridan.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/VOe3fETd3sk" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/VOe3fETd3sk?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/VOe3fETd3sk/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to the existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches (e.g. ChameleonMask), we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality (MR) hand gestural navigation and verbal communication. By overlaying the remote instructor&#x27;s virtual hands in the local user&#x27;s MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively ``teleoperating&#x27;&#x27; a real human. We evaluate our system through the in-the-wild deployment for physiotherapy classrooms, as well as lab-based experiments for other application domains such as mechanical assembly, sign language, and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Visual Cue</span><span class="ui brown basic label">Remote Collaboration</span><span class="ui brown basic label">Telepresence</span><span class="ui brown basic label">Remote Guidance</span><span class="ui brown basic label">Human Surrogates</span><span class="ui brown basic label">Hands On Training</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mehrad Faridan<!-- -->, <!-- -->Bheesha Kumari<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->13<!-- -->.  DOI: <a href="https://doi.org/10.1145/3544548.3581381" target="_blank">https://doi.org/10.1145/3544548.3581381</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2023-monteiro" class="ui large modal"><div class="header"><a href="/publications/chi-2023-monteiro" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2023-monteiro</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2023-monteiro.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2023-monteiro" target="_blank">Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</a></h1><p class="meta"><a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><strong>Kyzyl Monteiro</strong></a> , <a href="/people/ritik-vatsal"><img src="/static/images/people/ritik-vatsal.jpg" class="ui circular spaced image mini-profile"/><strong>Ritik Vatsal</strong></a> , <a href="/people/neil-chulpongsatorn"><img src="/static/images/people/neil-chulpongsatorn.jpg" class="ui circular spaced image mini-profile"/><strong>Neil Chulpongsatorn</strong></a> , <span>Aman Parnami</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/chi-2023-monteiro.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2023-monteiro.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/JssiyfrhIJw" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/JssiyfrhIJw?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/JssiyfrhIJw/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Prototyping Tools</span><span class="ui brown basic label">Tangible Interactions</span><span class="ui brown basic label">Everyday Objects</span><span class="ui brown basic label">Interactive Machine Teaching</span><span class="ui brown basic label">Human Centered Machine Learning</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kyzyl Monteiro<!-- -->, <!-- -->Ritik Vatsal<!-- -->, <!-- -->Neil Chulpongsatorn<!-- -->, <!-- -->Aman Parnami<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->15<!-- -->.  DOI: <a href="https://doi.org/10.1145/3544548.3581449" target="_blank">https://doi.org/10.1145/3544548.3581449</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-ea-2023-chulpongsatorn" class="ui large modal"><div class="header"><a href="/publications/chi-ea-2023-chulpongsatorn" target="_blank"><i class="fas fa-link fa-fw"></i>chi-ea-2023-chulpongsatorn</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI EA 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-ea-2023-chulpongsatorn.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-ea-2023-chulpongsatorn" target="_blank">HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</a></h1><p class="meta"><a href="/people/neil-chulpongsatorn"><img src="/static/images/people/neil-chulpongsatorn.jpg" class="ui circular spaced image mini-profile"/><strong>Neil Chulpongsatorn</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/chi-ea-2023-chulpongsatorn.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-ea-2023-chulpongsatorn.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We contribute interaction techniques for augmenting mixed reality (MR) visualizations with smartphone proxies. By combining head-mounted displays (HMDs) with mobile touchscreens, we can augment low-resolution holographic 3D charts with precise touch input, haptics feedback, high-resolution 2D graphics, and physical manipulation. Our approach aims to complement both MR and physical visualizations. Most current MR visualizations suffer from unreliable tracking, low visual resolution, and imprecise input. Data physicalizations on the other hand, although allowing for natural physical manipulation, are limited in dynamic and interactive modification. We demonstrate how mobile devices such as smartphones or tablets can serve as physical proxies for MR data interactions, creating dynamic visualizations that support precise manipulation and rich input and output. We describe 6 interaction techniques that leverage the combined physicality, sensing, and output capabilities of HMDs and smartphones, and demonstrate those interactions via a prototype system. Based on an evaluation, we outline opportunities for combining the advantages of both MR and physical charts.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Embedded Data Visualization</span><span class="ui brown basic label">Tangible Interaction</span><span class="ui brown basic label">Cross Device Interaction</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Neil Chulpongsatorn<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies</b>. <i>In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->8<!-- -->.  DOI: <a href="https://doi.org/10.1145/3544549.3585738" target="_blank">https://doi.org/10.1145/3544549.3585738</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-ea-2023-fang" class="ui large modal"><div class="header"><a href="/publications/chi-ea-2023-fang" target="_blank"><i class="fas fa-link fa-fw"></i>chi-ea-2023-fang</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI EA 2023</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-ea-2023-fang.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-ea-2023-fang" target="_blank">VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences</a></h1><p class="meta"><span>Cathy Mengying Fang</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Daniel Leithinger</span></p><p><a href="/static/publications/chi-ea-2023-fang.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-ea-2023-fang.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces VR Haptics at Home, a method of repurposing everyday objects in the home to provide casual and on-demand haptic experiences. Current VR haptic devices are often expensive, complex, and unreliable, which limits the opportunities for rich haptic experiences outside research labs. In contrast, we envision that, by repurposing everyday objects as passive haptics props, we can create engaging VR experiences for casual uses with minimal cost and setup. To explore and evaluate this idea, we conducted an in-the-wild study with eight participants, in which they used our proof-of-concept system to turn their surrounding objects such as chairs, tables, and pillows at their own homes into haptic props. The study results show that our method can be adapted to different homes and environments, enabling more engaging VR experiences without the need for complex setup process. Based on our findings, we propose a possible design space to showcase the potential for future investigation.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Interaction Techniques</span><span class="ui brown basic label">Passive Haptics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Cathy Mengying Fang<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Daniel Leithinger<!-- -->. <b>VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences</b>. <i>In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA &#x27;23)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->7<!-- -->.  DOI: <a href="https://doi.org/10.1145/3544549.3585871" target="_blank">https://doi.org/10.1145/3544549.3585871</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="tochi-2022-nittala" class="ui large modal"><div class="header"><a href="/publications/tochi-2022-nittala" target="_blank"><i class="fas fa-link fa-fw"></i>tochi-2022-nittala</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">TOCHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/tochi-2022-nittala.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/tochi-2022-nittala" target="_blank">SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</a></h1><p class="meta"><span>Adwait Sharma</span> , <span>Christina Salchow-Hömmen</span> , <span>Vimal Suresh Mollyn</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Michael A. Hedderich</span> , <span>Marion Koelle</span> , <span>Thomas Seel</span> , <span>Jürgen Steimle</span></p><p><a href="/static/publications/tochi-2022-nittala.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>tochi-2022-nittala.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>Gestural interaction with freehands and while grasping an everyday object enables always-available input. To sense such gestures, minimal instrumentation of the user’s hand is desirable. However, the choice of an effective but minimal IMU layout remains challenging, due to the complexity of the multi-factorial space that comprises diverse finger gestures, objects and grasps. We present SparseIMU, a rapid method for selecting minimal inertial sensor-based layouts for effective gesture recognition. Furthermore, we contribute a computational tool to guide designers with optimal sensor placement. Our approach builds on an extensive microgestures dataset that we collected with a dense network of 17 inertial measurement units (IMUs). We performed a series of analyses, including an evaluation of the entire combinatorial space for freehand and grasping microgestures (393K layouts), and quantified the performance across different layout choices, revealing new gesture detection opportunities with IMUs. Finally, we demonstrate the versatility of our method with four scenarios.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Gesture Recognition</span><span class="ui brown basic label">Hand Gestures</span><span class="ui brown basic label">Sensor Placement</span><span class="ui brown basic label">IMU</span><span class="ui brown basic label">Objects</span><span class="ui brown basic label">Design Tool</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Adwait Sharma<!-- -->, <!-- -->Christina Salchow-Hömmen<!-- -->, <!-- -->Vimal Suresh Mollyn<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Michael A. Hedderich<!-- -->, <!-- -->Marion Koelle<!-- -->, <!-- -->Thomas Seel<!-- -->, <!-- -->Jürgen Steimle<!-- -->. <b>SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures</b>. <i>In undefined (TOCHI &#x27;22)</i>. <!-- -->  Page: 1-<!-- -->40<!-- -->.  DOI: <a href="https://doi.org/10.1145/3569894" target="_blank">https://doi.org/10.1145/3569894</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-kaimoto" class="ui large modal"><div class="header"><a href="/publications/uist-2022-kaimoto" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2022-kaimoto</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-kaimoto.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-kaimoto" target="_blank">Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</a></h1><p class="meta"><a href="/people/hiroki-kaimoto"><img src="/static/images/people/hiroki-kaimoto.jpg" class="ui circular spaced image mini-profile"/><strong>Hiroki Kaimoto</strong></a> , <a href="/people/kyzyl-monteiro"><img src="/static/images/people/kyzyl-monteiro.jpg" class="ui circular spaced image mini-profile"/><strong>Kyzyl Monteiro</strong></a> , <a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><strong>Mehrad Faridan</strong></a> , <span>Jiatong Li</span> , <a href="/people/samin-farajian"><img src="/static/images/people/samin-farajian.jpg" class="ui circular spaced image mini-profile"/><strong>Samin Farajian</strong></a> , <span>Yasuaki Kakehi</span> , <span>Ken Nakagaki</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/uist-2022-kaimoto.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2022-kaimoto.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/xy-IeVgoEpY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/xy-IeVgoEpY?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/xy-IeVgoEpY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces Sketched Reality, an approach that com- bines AR sketching and actuated tangible user interfaces (TUI) for bi-directional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to affect each other through physical actuation and digital computation. In the existing AR sketching, the relationship between virtual and physical worlds is only one-directional --- while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketching interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Actuated Tangible Interfaces</span><span class="ui brown basic label">Swarm User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hiroki Kaimoto<!-- -->, <!-- -->Kyzyl Monteiro<!-- -->, <!-- -->Mehrad Faridan<!-- -->, <!-- -->Jiatong Li<!-- -->, <!-- -->Samin Farajian<!-- -->, <!-- -->Yasuaki Kakehi<!-- -->, <!-- -->Ken Nakagaki<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526113.3545626" target="_blank">https://doi.org/10.1145/3526113.3545626</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-liao" class="ui large modal"><div class="header"><a href="/publications/uist-2022-liao" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2022-liao</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-liao.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-liao" target="_blank">RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</a></h1><p class="meta"><a href="/people/jian-liao"><img src="/static/images/people/jian-liao.jpg" class="ui circular spaced image mini-profile"/><strong>Jian Liao</strong></a> , <a href="/people/adnan-karim"><img src="/static/images/people/adnan-karim.jpg" class="ui circular spaced image mini-profile"/><strong>Adnan Karim</strong></a> , <a href="/people/shivesh-jadon"><img src="/static/images/people/shivesh-jadon.jpg" class="ui circular spaced image mini-profile"/><strong>Shivesh Jadon</strong></a> , <span>Rubaiat Habib Kazi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/uist-2022-liao.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2022-liao.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/vfIMeICV-7c" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/vfIMeICV-7c?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/vfIMeICV-7c/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations, we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenter’s perspective to demonstrate the effectiveness of our system.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Augmented Presentation</span><span class="ui brown basic label">Natural Language Processing</span><span class="ui brown basic label">Gestural And Speech Input</span><span class="ui brown basic label">Video</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Jian Liao<!-- -->, <!-- -->Adnan Karim<!-- -->, <!-- -->Shivesh Jadon<!-- -->, <!-- -->Rubaiat Habib Kazi<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526113.3545702" target="_blank">https://doi.org/10.1145/3526113.3545702</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-nisser" class="ui large modal"><div class="header"><a href="/publications/uist-2022-nisser" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2022-nisser</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-nisser.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-nisser" target="_blank">Mixels: Fabricating Interfaces using Programmable Magnetic Pixels</a></h1><p class="meta"><span>Martin Nisser</span> , <span>Yashaswini Makaram</span> , <span>Lucian Covarrubias</span> , <span>Amadou Bah</span> , <span>Faraz Faruqi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Stefanie Mueller</span></p><p><a href="/static/publications/uist-2022-nisser.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2022-nisser.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/6SvFCQkVFtw" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/6SvFCQkVFtw?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/6SvFCQkVFtw/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we present Mixels, programmable magnetic pixels that can be rapidly fabricated using an electromagnetic printhead mounted on an off-the-shelve 3-axis CNC machine. The ability to program magnetic material pixel-wise with varying magnetic force enables Mixels to create new tangible, tactile, and haptic interfaces. To facilitate the creation of interactive objects with Mixels, we provide a user interface that lets users specify the high-level magnetic behavior and that then computes the underlying magnetic pixel assignments and fabrication instructions to program the magnetic surface. Our custom hardware add-on based on an electromagnetic printhead and hall effect sensor clips onto a standard 3-axis CNC machine and can both write and read magnetic pixel values from magnetic material. Our evaluation shows that our system can reliably program and read magnetic pixels of various strengths, that we can predict the behavior of two interacting magnetic surfaces before programming them, that our electromagnet is strong enough to create pixels that utilize the maximum magnetic strength of the material being programmed, and that this material remains magnetized when removed from the magnetic plotter.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Programmable Materials</span><span class="ui brown basic label">Magnetic Interfaces</span><span class="ui brown basic label">Fabrication</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Nisser<!-- -->, <!-- -->Yashaswini Makaram<!-- -->, <!-- -->Lucian Covarrubias<!-- -->, <!-- -->Amadou Bah<!-- -->, <!-- -->Faraz Faruqi<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Stefanie Mueller<!-- -->. <b>Mixels: Fabricating Interfaces using Programmable Magnetic Pixels</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->12<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526113.3545698" target="_blank">https://doi.org/10.1145/3526113.3545698</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2022-nittala" class="ui large modal"><div class="header"><a href="/publications/uist-2022-nittala" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2022-nittala</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2022-nittala.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2022-nittala" target="_blank">Prototyping Soft Devices with Interactive Bioplastics</a></h1><p class="meta"><span>Marion Koelle</span> , <span>Madalina Nicolae</span> , <a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Marc Teyssier</span> , <span>Jürgen Steimle</span></p><p><a href="/static/publications/uist-2022-nittala.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2022-nittala.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/8Paq3P3EsKQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/8Paq3P3EsKQ?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/8Paq3P3EsKQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Designers and makers are increasingly interested in leveraging bio-based and bio-degradable ‘do-it-yourself’ (DIY) materials for sustainable prototyping. Their self-produced bioplastics possess compelling properties such as self-adhesion but have so far not been functionalized to create soft interactive devices, due to a lack of DIY techniques for the fabrication of functional electronic circuits and sensors. In this paper, we contribute a DIY approach for creating Interactive Bioplastics that is accessible to a wide audience, making use of easy-to-obtain bio-based raw materials and familiar tools. We present three types of conductive bioplastic materials and their formulation: sheets, pastes and foams. Our materials enable additive and subtractive fabrication of soft circuits and sensors. Furthermore, we demonstrate how these materials can substitute conventional prototyping materials, be combined with off-the-shelf electronics, and be fed into a sustainable material ‘life-cycle’ including disassembly, re-use, and re-melting of materials. A formal characterization of our conductors highlights that they are even on-par with commercially available carbon-based conductive pastes.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Bioplastics</span><span class="ui brown basic label">Biomaterials</span><span class="ui brown basic label">Do It Yourself</span><span class="ui brown basic label">DIY</span><span class="ui brown basic label">Sustainability</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Marion Koelle<!-- -->, <!-- -->Madalina Nicolae<!-- -->, <!-- -->Aditya Shekhar Nittala<!-- -->, <!-- -->Marc Teyssier<!-- -->, <!-- -->Jürgen Steimle<!-- -->. <b>Prototyping Soft Devices with Interactive Bioplastics</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526113.3545623" target="_blank">https://doi.org/10.1145/3526113.3545623</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-sic-2022-faridan" class="ui large modal"><div class="header"><a href="/publications/uist-sic-2022-faridan" target="_blank"><i class="fas fa-link fa-fw"></i>uist-sic-2022-faridan</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST SIC 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-sic-2022-faridan.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-sic-2022-faridan" target="_blank">UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers</a></h1><p class="meta"><a href="/people/mehrad-faridan"><img src="/static/images/people/mehrad-faridan.jpg" class="ui circular spaced image mini-profile"/><strong>Mehrad Faridan</strong></a> , <a href="/people/marcus-friedel"><img src="/static/images/people/marcus-friedel.jpg" class="ui circular spaced image mini-profile"/><strong>Marcus Friedel</strong></a> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a></p><p><a href="/static/publications/uist-sic-2022-faridan.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-sic-2022-faridan.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/jMYAQzzQ_PI" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/jMYAQzzQ_PI?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/jMYAQzzQ_PI/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We introduce UltraBots, a system that combines ultrasound haptic feedback and robotic actuation for large-area mid-air haptics for VR. Ultrasound haptics can provide precise mid-air haptic feedback and versatile shape rendering, but the interaction area is often limited by the small size of the ultrasound devices, restricting the possible interactions for VR. To address this problem, this paper introduces a novel approach that combines robotic actuation with ultrasound haptics. More specifically, we will attach ultrasound transducer arrays to tabletop mobile robots or robotic arms for scalable, extendable, and translatable interaction areas. We plan to use Sony Toio robots for 2D translation and/or commercially available robotic arms for 3D translation. Using robotic actuation and hand tracking measured by a VR HMD (ex: Oculus Quest), our system can keep the ultrasound transducers underneath the user’s hands to provide on-demand haptics. We demonstrate applications with workspace environments, medical training, education and entertainment.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Haptics</span><span class="ui brown basic label">Ultrasound Transducers</span><span class="ui brown basic label">Robotics</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Mehrad Faridan<!-- -->, <!-- -->Marcus Friedel<!-- -->, <!-- -->Ryo Suzuki<!-- -->. <b>UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers</b>. <i>In Adjunct undefined (UIST SIC &#x27;22)</i>. <!-- -->  Page: 1-<!-- -->3<!-- -->.  DOI: <a href="https://doi.org/10.1145/3526114.3561350" target="_blank">https://doi.org/10.1145/3526114.3561350</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="iros-2022-suzuki" class="ui large modal"><div class="header"><a href="/publications/iros-2022-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>iros-2022-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">IROS 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/iros-2022-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/iros-2022-suzuki" target="_blank">Selective Self-Assembly using Re-Programmable Magnetic Pixels</a></h1><p class="meta"><span>Martin Nisser</span> , <span>Yashaswini Makaram</span> , <span>Faraz Faruqi</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Stefanie Mueller</span></p><p><a href="/static/publications/iros-2022-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>iros-2022-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HK9_ynH6A6w" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HK9_ynH6A6w?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/HK9_ynH6A6w/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper introduces a method to generate highly selective encodings that can be magnetically programmed onto physical modules to enable them to self-assemble in chosen configurations. We generate these encodings based on Hadamard matrices, and show how to design the faces of modules to be maximally attractive to their intended mate, while remaining maximally agnostic to other faces. We derive guarantees on these bounds, and verify their attraction and agnosticism experimentally. Using cubic modules whose faces have been covered in soft magnetic material, we show how inexpensive, passive modules with planar faces can be used to selectively self-assemble into target shapes without geometric guides. We show that these modules can be easily re-programmed for new target shapes using a CNC-based magnetic plotter, and demonstrate self-assembly of 8 cubes in a water tank.</p></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Martin Nisser<!-- -->, <!-- -->Yashaswini Makaram<!-- -->, <!-- -->Faraz Faruqi<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Stefanie Mueller<!-- -->. <b>Selective Self-Assembly using Re-Programmable Magnetic Pixels</b>. <i>In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS &#x27;22)</i>. <!-- -->IEEE, New York, NY, USA<!-- -->  Page: 1-<!-- -->8<!-- -->.  DOI: <a target="_blank"></a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="gecco-2022-ivanov" class="ui large modal"><div class="header"><a href="/publications/gecco-2022-ivanov" target="_blank"><i class="fas fa-link fa-fw"></i>gecco-2022-ivanov</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">GECCO 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/gecco-2022-ivanov.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/gecco-2022-ivanov" target="_blank">EvoIsland: Interactive Evolution via an Island-Inspired Spatial User Interface Framework</a></h1><p class="meta"><a href="/people/sasha-ivanov"><img src="/static/images/people/sasha-ivanov.jpg" class="ui circular spaced image mini-profile"/><strong>Sasha Ivanov</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a> , <span>Christian Jacob</span></p><p><a href="/static/publications/gecco-2022-ivanov.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>gecco-2022-ivanov.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We present EvoIsland, a scalable interactive evolutionary user interface framework inspired by the spatially isolated land masses seen on Earth. Our generalizable interaction system encourages creators to spatially explore a wide range of design possibilities through the combination, separation, and rearrangement of hexagonal tiles on a grid. As these tiles are grouped into islandlike clusters, localized populations of designs form through an underlying evolutionary system. The interactions that take place within EvoIsland provide content creators with new ways to shape, display and assess populations in evolutionary systems that produce a wide range of solutions with visual phenotype outputs.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Interactive Evolutionary Systems</span><span class="ui brown basic label">User Interfaces</span><span class="ui brown basic label">Visualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sasha Ivanov<!-- -->, <!-- -->Wesley Willett<!-- -->, <!-- -->Christian Jacob<!-- -->. <b>EvoIsland: Interactive Evolution via an Island-Inspired Spatial User Interface Framework</b>. <i>In undefined (GECCO &#x27;22)</i>. <!-- -->  Page: 1-<!-- -->8<!-- -->.  DOI: <a href="https://doi.org/10.1145/3512290.3528722" target="_blank">https://doi.org/10.1145/3512290.3528722</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="gi-2022-hull" class="ui large modal"><div class="header"><a href="/publications/gi-2022-hull" target="_blank"><i class="fas fa-link fa-fw"></i>gi-2022-hull</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">GI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/gi-2022-hull.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/gi-2022-hull" target="_blank">Simultaneous Worlds: Supporting Fluid Exploration of Multiple Data Sets via Physical Models</a></h1><p class="meta"><a href="/people/carmen-hull"><img src="/static/images/people/carmen-hull.jpg" class="ui circular spaced image mini-profile"/><strong>Carmen Hull</strong></a> , <a href="/people/soren-knudsen"><img src="/static/images/people/soren-knudsen.jpg" class="ui circular spaced image mini-profile"/><strong>Søren Knudsen</strong></a> , <a href="/people/sheelagh-carpendale"><img src="/static/images/people/sheelagh-carpendale.jpg" class="ui circular spaced image mini-profile"/><strong>Sheelagh Carpendale</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/gi-2022-hull.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>gi-2022-hull.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>We take the well-established use of physical scale models in architecture and identify new opportunities for using them to interactively visualize and examine multiple streams of geospatial data. Overlaying, comparing, or integrating visualizations of complementary data sets in the same physical space is often challenging given the constraints of various data types and the limited design space of possible visual encodings. Our vision of “simultaneous worlds” uses physical models as a substrate upon which visualizations of multiple data streams can be dynamically and concurrently integrated. To explore the potential of this concept, we created three design explorations that use an illuminated campus model to integrate visualizations about building energy use, climate, and movement paths on a university campus. We use a research through design approach, documenting how our interdisciplinary collaborations with domain experts, students, and architects informed our designs. Based on our observations, we characterize the benefits of models for 1) situating visualizations, 2) composing visualizations, and 3) manipulating and authoring visualizations. Our work highlights the potential of physical models to support embodied exploration of spatial and non-spatial visualizations through fluid interactions.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Information Visualization</span><span class="ui brown basic label">Interactive Surfaces</span><span class="ui brown basic label">Data Physicalization</span><span class="ui brown basic label">Architectural Models</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Carmen Hull<!-- -->, <!-- -->Søren Knudsen<!-- -->, <!-- -->Sheelagh Carpendale<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Simultaneous Worlds: Supporting Fluid Exploration of Multiple Data Sets via Physical Models</b>. <i>In undefined (GI &#x27;22)</i>. <!-- -->  Page: 1-<!-- -->10<!-- -->.  DOI: <a href="http://hdl.handle.net/1880/114742" target="_blank">http://hdl.handle.net/1880/114742</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-bressa" class="ui large modal"><div class="header"><a href="/publications/chi-2022-bressa" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2022-bressa</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-bressa.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-bressa" target="_blank">Data Every Day: Designing and Living with Personal Situated Visualizations</a></h1><p class="meta"><a href="/people/nathalie-bressa"><img src="/static/images/people/nathalie-bressa.jpg" class="ui circular spaced image mini-profile"/><strong>Nathalie Bressa</strong></a> , <span>Jo Vermeulen</span> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/chi-2022-bressa.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2022-bressa.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/B0bKMgDd1xY" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/B0bKMgDd1xY?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/B0bKMgDd1xY/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the design and utility of situated manual self-tracking visualizations on dedicated displays that integrate data tracking into existing practices and physical environments. Situating self-tracking tools in relevant locations is a promising approach to enable reflection on and awareness of data without needing to rely on sensorized tracking or personal devices. In both a long-term autobiographical design process and a co-design study with six participants, we rapidly prototyped and deployed 30 situated self-tracking applications over a ten month period. Grounded in the experience of designing and living with these trackers, we contribute findings on logging and data entry, the use of situated displays, and the visual design and customization of trackers. Our results demonstrate the potential of customizable dedicated self-tracking visualizations that are situated in relevant physical spaces, and suggest future research opportunities and new potential applications for situated visualizations.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Self Tracking</span><span class="ui brown basic label">Situated Visualization</span><span class="ui brown basic label">Personal Data</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Nathalie Bressa<!-- -->, <!-- -->Jo Vermeulen<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>Data Every Day: Designing and Living with Personal Situated Visualizations</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->18<!-- -->.  DOI: <a href="https://doi.org/10.1145/3491102.3517737" target="_blank">https://doi.org/10.1145/3491102.3517737</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-ivanov" class="ui large modal"><div class="header"><a href="/publications/chi-2022-ivanov" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2022-ivanov</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-ivanov.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-ivanov" target="_blank">One Week in the Future: Previs Design Futuring for HCI Research</a></h1><p class="meta"><a href="/people/sasha-ivanov"><img src="/static/images/people/sasha-ivanov.jpg" class="ui circular spaced image mini-profile"/><strong>Sasha Ivanov</strong></a> , <a href="/people/tim-au-yeung"><img src="/static/images/people/tim-au-yeung.jpg" class="ui circular spaced image mini-profile"/><strong>Tim Au Yeung</strong></a> , <a href="/people/kathryn-blair"><img src="/static/images/people/kathryn-blair.jpg" class="ui circular spaced image mini-profile"/><strong>Kathryn Blair</strong></a> , <a href="/people/kurtis-danyluk"><img src="/static/images/people/kurtis-danyluk.jpg" class="ui circular spaced image mini-profile"/><strong>Kurtis Danyluk</strong></a> , <a href="/people/georgina-freeman"><img src="/static/images/people/georgina-freeman.jpg" class="ui circular spaced image mini-profile"/><strong>Georgina Freeman</strong></a> , <a href="/people/marcus-friedel"><img src="/static/images/people/marcus-friedel.jpg" class="ui circular spaced image mini-profile"/><strong>Marcus Friedel</strong></a> , <a href="/people/carmen-hull"><img src="/static/images/people/carmen-hull.jpg" class="ui circular spaced image mini-profile"/><strong>Carmen Hull</strong></a> , <a href="/people/michael-hung"><img src="/static/images/people/michael-hung.jpg" class="ui circular spaced image mini-profile"/><strong>Michael Hung</strong></a> , <a href="/people/sydney-pratte"><img src="/static/images/people/sydney-pratte.jpg" class="ui circular spaced image mini-profile"/><strong>Sydney Pratte</strong></a> , <a href="/people/wesley-willett"><img src="/static/images/people/wesley-willett.jpg" class="ui circular spaced image mini-profile"/><strong>Wesley Willett</strong></a></p><p><a href="/static/publications/chi-2022-ivanov.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2022-ivanov.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/qoIwYW83iSU" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/qoIwYW83iSU?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/qoIwYW83iSU/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>We explore the use of cinematic “pre-visualization” (previs) techniques as a rapid ideation and design futuring method for human computer interaction (HCI) research. Previs approaches, which are widely used in animation and film production, use digital design tools to create medium-fidelity videos that capture richer interaction, motion, and context than sketches or static illustrations. When used as a design futuring method, previs can facilitate rapid, iterative discussions that reveal tensions, challenges, and opportunities for new research. We performed eight one-week design futuring sprints, in which individual HCI researchers collaborated with a lead designer to produce concept sketches, storyboards, and videos that examined future applications of their research. From these experiences, we identify recurring themes and challenges and present a One Week Futuring Workbook that other researchers can use to guide their own futuring sprints. We also highlight how variations of our approach could support other speculative design practices.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Design Futuring</span><span class="ui brown basic label">Prototyping</span><span class="ui brown basic label">Previsualization</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Sasha Ivanov<!-- -->, <!-- -->Tim Au Yeung<!-- -->, <!-- -->Kathryn Blair<!-- -->, <!-- -->Kurtis Danyluk<!-- -->, <!-- -->Georgina Freeman<!-- -->, <!-- -->Marcus Friedel<!-- -->, <!-- -->Carmen Hull<!-- -->, <!-- -->Michael Hung<!-- -->, <!-- -->Sydney Pratte<!-- -->, <!-- -->Wesley Willett<!-- -->. <b>One Week in the Future: Previs Design Futuring for HCI Research</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->15<!-- -->.  DOI: <a href="https://doi.org/10.1145/3491102.3517584" target="_blank">https://doi.org/10.1145/3491102.3517584</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-nittala" class="ui large modal"><div class="header"><a href="/publications/chi-2022-nittala" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2022-nittala</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-nittala.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-nittala" target="_blank">Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices</a></h1><p class="meta"><a href="/people/aditya-shekhar-nittala"><img src="/static/images/people/aditya-shekhar-nittala.jpg" class="ui circular spaced image mini-profile"/><strong>Aditya Shekhar Nittala</strong></a> , <span>Jürgen Steimle</span></p><p><a href="/static/publications/chi-2022-nittala.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2022-nittala.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/Lj9Yk5IQsok" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/Lj9Yk5IQsok?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/Lj9Yk5IQsok/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Skin is a promising interaction medium and has been widely explored for mobile, and expressive interaction. Recent research in HCI has seen the development of Epidermal Computing Devices: ultra-thin and non-invasive devices which reside on the user’s skin, offering intimate integration with the curved surfaces of the body, while having physical and mechanical properties that are akin to skin, expanding the horizon of on-body interaction. However, with rapid technological advancements in multiple disciplines, we see a need to synthesize the main open research questions and opportunities for the HCI community to advance future research in this area. By systematically analyzing Epidermal Devices contributed in the HCI community, physical sciences research and from our experiences in designing and building Epidermal Devices, we identify opportunities and challenges for advancing research across five themes. This multi-disciplinary synthesis enables multiple research communities to facilitate progression towards more coordinated endeavors for advancing Epidermal Computing.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Wearable Devices</span><span class="ui brown basic label">Epidermal Devices</span><span class="ui brown basic label">Survey</span><span class="ui brown basic label">Soft Wearables</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Aditya Shekhar Nittala<!-- -->, <!-- -->Jürgen Steimle<!-- -->. <b>Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->22<!-- -->.  DOI: <a href="https://doi.org/10.1145/3491102.3517668" target="_blank">https://doi.org/10.1145/3491102.3517668</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-2022-suzuki" class="ui large modal"><div class="header"><a href="/publications/chi-2022-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>chi-2022-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-2022-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-2022-suzuki" target="_blank">Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <a href="/people/adnan-karim"><img src="/static/images/people/adnan-karim.jpg" class="ui circular spaced image mini-profile"/><strong>Adnan Karim</strong></a> , <a href="/people/tian-xia"><img src="/static/images/people/tian-xia.jpg" class="ui circular spaced image mini-profile"/><strong>Tian Xia</strong></a> , <span>Hooman Hedayati</span> , <a href="/people/nicolai-marquardt"><img src="/static/images/people/nicolai-marquardt.jpg" class="ui circular spaced image mini-profile"/><strong>Nicolai Marquardt</strong></a></p><p><a href="/static/publications/chi-2022-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-2022-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/MvOWxQC_4uQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/MvOWxQC_4uQ?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/MvOWxQC_4uQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>This paper contributes to a taxonomy of augmented reality and robotics based on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have emerged as a new way to enhance human-robot interaction (HRI) and robotic interfaces (e.g., actuated and shape-changing interfaces). Recently, an increasing number of studies in HCI, HRI, and robotics have demonstrated how AR enables better interactions between people and robots. However, often research remains focused on individual explorations and key design strategies, and research questions are rarely analyzed systematically. In this paper, we synthesize and categorize this research field in the following dimensions: 1) approaches to augmenting reality; 2) characteristics of robots; 3) purposes and benefits; 4) classification of presented information; 5) design components and strategies for visual augmentation; 6) interaction techniques and modalities; 7) application domains; and 8) evaluation strategies. We formulate key challenges and opportunities to guide and inform future research in AR and robotics.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Augmented Reality</span><span class="ui brown basic label">Mixed Reality</span><span class="ui brown basic label">Robotics</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Actuated Tangible UI</span><span class="ui brown basic label">Shape Changing UI</span><span class="ui brown basic label">AR HRI</span><span class="ui brown basic label">VAM HRI</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Adnan Karim<!-- -->, <!-- -->Tian Xia<!-- -->, <!-- -->Hooman Hedayati<!-- -->, <!-- -->Nicolai Marquardt<!-- -->. <b>Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces</b>. <i>In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->33<!-- -->.  DOI: <a href="https://doi.org/10.1145/3491102.3517719" target="_blank">https://doi.org/10.1145/3491102.3517719</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="chi-ea-2022-blair" class="ui large modal"><div class="header"><a href="/publications/chi-ea-2022-blair" target="_blank"><i class="fas fa-link fa-fw"></i>chi-ea-2022-blair</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">CHI EA 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/chi-ea-2022-blair.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/chi-ea-2022-blair" target="_blank">Art is Not Research. Research is not Art</a></h1><p class="meta"><a href="/people/kathryn-blair"><img src="/static/images/people/kathryn-blair.jpg" class="ui circular spaced image mini-profile"/><strong>Kathryn Blair</strong></a> , <span>Miriam Sturdee</span> , <span>Lindsay Macdonald Vermeulen</span> , <a href="/people/lora-oehlberg"><img src="/static/images/people/lora-oehlberg.jpg" class="ui circular spaced image mini-profile"/><strong>Lora Oehlberg</strong></a></p><p><a href="/static/publications/chi-ea-2022-blair.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>chi-ea-2022-blair.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/G0CSYn_uhIE" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/G0CSYn_uhIE?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/G0CSYn_uhIE/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>Art is not Research. Research is not Art. is a multimedia, multi-site participatory installation by a collective of artists and researchers from Calgary, Toronto, and Lancaster; it is informed by these contexts. It reflects the tensions between how “participants” are treated in participatory art and interaction research. It offers a framework through which we can explore how epistemologies might evolve in a blending between Art and Research. Visitors download the paper to read, critically reflect on the relationship between art and research, and experientially engage with the material through a series of creative prompts. A performance variation of the piece will be performed in-person and online through the ACM SIGCHI Conference on Human Factors in Computing Systems alt.chi track.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Interdisciplinary Research</span><span class="ui brown basic label">Research Ethics</span><span class="ui brown basic label">Arts And Computing</span><span class="ui brown basic label">Research Methods</span><span class="ui brown basic label">Knowledge Creation</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Kathryn Blair<!-- -->, <!-- -->Miriam Sturdee<!-- -->, <!-- -->Lindsay Macdonald Vermeulen<!-- -->, <!-- -->Lora Oehlberg<!-- -->. <b>Art is Not Research. Research is not Art</b>. <i>In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA &#x27;22)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->9<!-- -->.  DOI: <a href="https://doi.org/10.1145/3491101.3516391" target="_blank">https://doi.org/10.1145/3491101.3516391</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="frobt-2022-suzuki" class="ui large modal"><div class="header"><a href="/publications/frobt-2022-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>frobt-2022-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">Frontiers 2022</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/frobt-2022-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/frobt-2022-suzuki" target="_blank">Designing Expandable-Structure Robots for Human-Robot Interaction</a></h1><p class="meta"><span>Hooman Hedayati</span> , <a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Wyatt Rees1</span> , <span>Daniel Leithinger</span> , <span>Daniel Szafir</span></p><p><a href="/static/publications/frobt-2022-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>frobt-2022-suzuki.pdf</a></p></div></div></div><div class="block"><h1>Abstract</h1><p>In this paper, we survey the emerging design space of expandable structures in robotics, with a focus on how such structures may improve human-robot interactions. We detail various implementation considerations for researchers seeking to integrate such structures in their own work and describe how expandable structures may lead to novel forms of interaction for a variety of different robots and applications, including structures that enable robots to alter their form to augment or gain entirely new capabilities, such as enhancing manipulation or navigation, structures that improve robot safety, structures that enable new forms of communication, and structures for robot swarms that enable the swarm to change shape both individually and collectively. To illustrate how these considerations may be operationalized, we also present three case studies from our own research in expandable structure robots, sharing our design process and our findings regarding how such structures enable robots to produce novel behaviors that may capture human attention, convey information, mimic emotion, and provide new types of dynamic affordances.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Deployable Robot</span><span class="ui brown basic label">Human Robot Interaction</span><span class="ui brown basic label">Modular Robot</span><span class="ui brown basic label">Origami Robotics</span><span class="ui brown basic label">Deployable Structures</span><span class="ui brown basic label">Shape Changing Robots</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Hooman Hedayati<!-- -->, <!-- -->Ryo Suzuki<!-- -->, <!-- -->Wyatt Rees1<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Daniel Szafir<!-- -->. <b>Designing Expandable-Structure Robots for Human-Robot Interaction</b>. <i>In undefined (Frontiers &#x27;22)</i>. <!-- -->  Page: 1-<!-- -->22<!-- -->.  DOI: <a href="https://doi.org/10.3389/frobt.2022.719639" target="_blank">https://doi.org/10.3389/frobt.2022.719639</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div><div id="uist-2021-suzuki" class="ui large modal"><div class="header"><a href="/publications/uist-2021-suzuki" target="_blank"><i class="fas fa-link fa-fw"></i>uist-2021-suzuki</a><div class="actions" style="float:right;cursor:pointer;color:grey"><i class="ui right cancel close icon"></i></div></div><div class="content"><div id="publication"><div class="block"><div id="breadcrumb" class="ui breadcrumb"><a class="section" href="/publications">Publications</a><i class="right angle icon divider"></i><a class="active section">UIST 2021</a></div><div class="ui stackable grid" style="margin-top:10px"><div class="three wide column" style="margin:auto"><img class="cover" src="/static/images/publications/cover/uist-2021-suzuki.jpg"/></div><div class="thirteen wide column"><h1><a href="/publications/uist-2021-suzuki" target="_blank">HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots</a></h1><p class="meta"><a href="/people/ryo-suzuki"><img src="/static/images/people/ryo-suzuki.jpg" class="ui circular spaced image mini-profile"/><strong>Ryo Suzuki</strong></a> , <span>Eyal Ofek</span> , <span>Mike Sinclair</span> , <span>Daniel Leithinger</span> , <span>Mar Gonzalez-Franco</span></p><p><a href="/static/publications/uist-2021-suzuki.pdf" target="_blank"><i class="far fa-file-pdf fa-fw"></i>uist-2021-suzuki.pdf</a></p></div></div></div><div class="block"><div class="video-container"><iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/HTiZgOESJyQ" srcDoc="&lt;style&gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&lt;/style&gt;&lt;a href=https://www.youtube.com/embed/HTiZgOESJyQ?autoplay=1&gt;&lt;img src=https://img.youtube.com/vi/HTiZgOESJyQ/maxresdefault.jpg&gt;&lt;span&gt;▶&lt;/span&gt;&lt;/a&gt;" frameBorder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="true" msallowfullscreen="true" oallowfullscreen="true" webkitallowfullscreen="true"></iframe></div></div><div class="block"><h1>Abstract</h1><p>HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic ap- proaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in- time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various ap- plications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.</p><div class="ui large basic labels">Keywords:  <span class="ui brown basic label">Virtual Reality</span><span class="ui brown basic label">Encountered Type Haptics</span><span class="ui brown basic label">Tabletop Mobile Robots</span><span class="ui brown basic label">Swarm User Interfaces</span></div></div><div class="block"><h1>Reference</h1><div class="ui segment"><p style="line-height:160%">Ryo Suzuki<!-- -->, <!-- -->Eyal Ofek<!-- -->, <!-- -->Mike Sinclair<!-- -->, <!-- -->Daniel Leithinger<!-- -->, <!-- -->Mar Gonzalez-Franco<!-- -->. <b>HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots</b>. <i>In Proceedings of the Annual ACM Symposium on User Interface Software and Technology (UIST &#x27;21)</i>. <!-- -->ACM, New York, NY, USA<!-- -->  Page: 1-<!-- -->16<!-- -->.  DOI: <a href="https://doi.org/10.1145/3472749.3474821" target="_blank">https://doi.org/10.1145/3472749.3474821</a></p></div></div></div></div><div class="actions"><div class="ui right cancel button">Close</div></div></div></div><div class="ui vertical segment stackable" style="text-align:center"><a class="ui button" href="/publications">+ 30 more publications</a></div></div><div id="people" class="category"><h1 class="ui horizontal divider header"><i class="child icon"></i>Students</h1><div class="people-category"><h2>PhD Students</h2><div class="ui grid"><a class="four wide column person" href="/people/ashratuz-zavin-asha"><img class="ui circular image medium-profile" src="/static/images/people/ashratuz-zavin-asha.jpg"/><p><b>Ashratuz Zavin Asha</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/christopher-smith"><img class="ui circular image medium-profile" src="/static/images/people/christopher-smith.jpg"/><p><b>Christopher Smith</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/georgina-freeman"><img class="ui circular image medium-profile" src="/static/images/people/georgina-freeman.jpg"/><p><b>Georgina Freeman</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/kathryn-blair"><img class="ui circular image medium-profile" src="/static/images/people/kathryn-blair.jpg"/><p><b>Kathryn Blair</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/kurtis-danyluk"><img class="ui circular image medium-profile" src="/static/images/people/kurtis-danyluk.jpg"/><p><b>Kurtis Danyluk</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/roberta-cabral-mota"><img class="ui circular image medium-profile" src="/static/images/people/roberta-cabral-mota.jpg"/><p><b>Roberta Cabral Mota</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/sydney-pratte"><img class="ui circular image medium-profile" src="/static/images/people/sydney-pratte.jpg"/><p><b>Sydney Pratte</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/terrance-mok"><img class="ui circular image medium-profile" src="/static/images/people/terrance-mok.jpg"/><p><b>Terrance Mok</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/tim-au-yeung"><img class="ui circular image medium-profile" src="/static/images/people/tim-au-yeung.jpg"/><p><b>Tim Au Yeung</b></p><p>PhD Student</p></a><a class="four wide column person" href="/people/zachary-mckendrick"><img class="ui circular image medium-profile" src="/static/images/people/zachary-mckendrick.jpg"/><p><b>Zachary McKendrick</b></p><p>PhD Student</p></a></div></div><div class="people-category"><h2>Masters Students</h2><div class="ui grid"><a class="four wide column person" href="/people/aditya-gunturu"><img class="ui circular image medium-profile" src="/static/images/people/aditya-gunturu.jpg"/><p><b>Aditya Gunturu</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/april-zhang"><img class="ui circular image medium-profile" src="/static/images/people/april-zhang.jpg"/><p><b>April Zhang</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/ben-pearman"><img class="ui circular image medium-profile" src="/static/images/people/ben-pearman.jpg"/><p><b>Ben Pearman</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/bonnie-wu"><img class="ui circular image medium-profile" src="/static/images/people/bonnie-wu.jpg"/><p><b>Bonnie Wu</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/clara-xi"><img class="ui circular image medium-profile" src="/static/images/people/clara-xi.jpg"/><p><b>Clara Xi</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/colin-auyeung"><img class="ui circular image medium-profile" src="/static/images/people/colin-auyeung.jpg"/><p><b>Colin Au Yeung</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/danissa-sandykbayeva"><img class="ui circular image medium-profile" src="/static/images/people/danissa-sandykbayeva.jpg"/><p><b>Danissa Sandykbayeva</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/desmond-larsen-rosner"><img class="ui circular image medium-profile" src="/static/images/people/desmond-larsen-rosner.jpg"/><p><b>Desmond Larsen-Rosner</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/dinmukhammed-mukashev"><img class="ui circular image medium-profile" src="/static/images/people/dinmukhammed-mukashev.jpg"/><p><b>Dinmukhammed Mukashev</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/huanjun-zhao"><img class="ui circular image medium-profile" src="/static/images/people/huanjun-zhao.jpg"/><p><b>Huanjun Zhao</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/iryna-luchak"><img class="ui circular image medium-profile" src="/static/images/people/iryna-luchak.jpg"/><p><b>Iryna Luchak</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/karly-ross"><img class="ui circular image medium-profile" src="/static/images/people/karly-ross.jpg"/><p><b>Karly Ross</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/nandi-zhang"><img class="ui circular image medium-profile" src="/static/images/people/nandi-zhang.jpg"/><p><b>Nandi Zhang</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/neil-chulpongsatorn"><img class="ui circular image medium-profile" src="/static/images/people/neil-chulpongsatorn.jpg"/><p><b>Neil Chulpongsatorn</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/shanna-hollingworth"><img class="ui circular image medium-profile" src="/static/images/people/shanna-hollingworth.jpg"/><p><b>Shanna Hollingworth</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/sutirtha-roy"><img class="ui circular image medium-profile" src="/static/images/people/sutirtha-roy.jpg"/><p><b>Sutirtha Roy</b></p><p>MSc Student</p></a><a class="four wide column person" href="/people/teale-masrani"><img class="ui circular image medium-profile" src="/static/images/people/no-profile.jpg"/><p><b>Teale Masrani</b></p><p>MSc Student</p></a></div></div><div class="ui vertical segment stackable" style="text-align:center"><a class="ui button" href="/people">+ see more members</a></div></div><div id="location" class="category"><h1 class="ui horizontal divider header"><i class="map outline icon"></i>Location</h1><div id="map" class="ui grid"><div class="ten wide column"><div class="feature map"><iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d2506.544200524445!2d-114.1279042!3d51.079963549999995!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x53716f0c07993c17%3A0xb8f1352e9e5dfa06!2sMath+Science%2C+Calgary%2C+AB+T2N+4V8%2C+Canada!5e0!3m2!1sen!2sus!4v1439359680603" frameBorder="0" style="border:0"></iframe></div></div><div class="six wide column"><div class="ui segment"><h1>Interactions Lab</h1><p>680 Math Science Building,<br/>University of Calgary<br/>Calgary, AB T2N 4V8, Canada</p></div></div></div><div class="ui vertical segment stackable" style="text-align:center"><a class="ui button" href="/location">+ learn more about our space</a></div></div></div></div><footer><div class="ui center aligned container"><div class="ui section divider"></div><img style="max-width:180px;margin:30px auto" src="/static/images/logo-6.png"/><div class="content"><img style="max-width:200px;margin:0px auto" src="/static/images/logo-4.png"/><div class="sub header">Department of Computer Science</div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{}},"page":"/","query":{},"buildId":"IH3t5oTZZKsOXvNbIke9X","dynamicBuildId":false,"nextExport":true}</script><script async="" id="__NEXT_PAGE__/" src="/_next/static/IH3t5oTZZKsOXvNbIke9X/pages/index.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/IH3t5oTZZKsOXvNbIke9X/pages/_app.js"></script><script src="/_next/static/runtime/webpack-8ed9452df514b4d17d80.js" async=""></script><script src="/_next/static/chunks/commons.2ccf7861fac39e850a30.js" async=""></script><script src="/_next/static/runtime/main-ceb4d64d798d2348aa74.js" async=""></script></body></html>